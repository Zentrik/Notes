\section{Fourier transforms}
\subsection{Fourier transforms}
In this section, we will write $L^p = L^p(\mathbb R^d)$ for the space of \underline{complex valued} Borel measurable fcns on $\mathbb{R}^d$, i.e. $f \colon \mathbb R^d \to \mathbb C$ s.t. $\norm{f}_p = \qty(\int_{\mathbb R^d} \abs{f(x)}^p \dd{\mu(x)})^{\frac 1p} < \infty$ for $1 \leq p < \infty$.

\begin{remark}
	For $g$ measurable s.t. $\int |g| < \infty$, define $\int g(x) \dd{\mu(x)} = \int \Re(g(x)) \dd{\mu(x)} + i \int \Im(g(x)) \dd{\mu(x)}$.

	% Note that for some $u + iv = \alpha \in \mathbb C$ with $\abs{\alpha} = 1$,
	% \[ \abs{\int_{\mathbb R^d} f(x) \dd{x}} = \int_{\mathbb R^d} \alpha f(x) \dd{x} = \int_{\mathbb R^d} u(x) \dd{x} + i \int_{\mathbb R^d} v(x) \dd{x} \]
	% But since the left hand side is real-valued, the $i \int_{\mathbb R^d} v(x) \dd{x}$ term vanishes.
	% So
	% \[ \abs{\int_{\mathbb R^d} f(x) \dd{x}} = \int_{\mathbb R^d} u(x) \dd{x} \leq \int_{\mathbb R^d} \abs{f(x)} \dd{x} \]
\end{remark}

For $f, g \in L^2$, $\inner{f, g} = \int f(x) \overline{g(x)} \dd{\mu(x)}$ is an inner product on $L^2(\mu)$.

For any $y \in \mathbb{R}^d$,
\begin{align*}
	\int f(x - y) \dd{x} &= \int f(y - x) \dd{x} - \int f(x) \dd{x} \\
	&= \int f(-x) \dd{x}.
\end{align*}
This is by the translation invariance and $x \mapsto -x$ symmetry of $\lambda$, proved in Sheet 3.
Also, for $a \in \mathbb{R}$ with $a \neq 0$, $\int f(ax) \dd{x} = \frac{1}{a^d} \int f(x) \dd{x}$.

\begin{definition}[Fourier Transform]
	Let $f \in L^1(\mathbb R^d)$.
	We define the \vocab{Fourier transform} $\hat f$ by
	\[ \hat f(u) = \int_{\mathbb R^d} f(x) e^{i\inner{u,x}} \dd{x} \]
	where $u \in \mathbb{R}^d$ and $\inner{u,x} = \sum_{i=1}^d u_i x_i$.
\end{definition}

\begin{remark}
	Note that $\abs{\hat f(u)} \leq \norm{f}_1 \quad \forall u \in \mathbb{R}^d$, i.e. $\hat{f} \in L^\infty$.

	Also, if $u_n \to u$, then $e^{i\inner{u_n,x}} \to e^{i\inner{u,x}}$ so $f(x) e^{i\inner{u_n,x}} \to f(x) e^{i\inner{u,x}}$; $|f(x) e^{i\inner{u_n,x}}| \leq |f(x)|$ and $f \in L^1$.
	By the DCT $\hat f(u_n) \to \hat f(u)$.
	Moreover, $\lim_{\norm{u} \to \infty} \hat{f}(u) = 0$ (Riemann-Lebesgue Lemma, Sheet 3).
	Thus $\hat f \in C_0(\mathbb{R}^d) = \qty{f \text{ bounded cts and vanishing at } \pm \infty}$.

	The map is $1 - 1$ (but not onto), its injective but not surjective.
\end{remark}

\begin{definition}[Fourier Transform]
	Let $\mu$ be a finite Borel measure on $\mathbb R^d$.
	We define the \vocab{Fourier transform} of the measure for $u \in \mathbb{R}^d$ by
	\[ \hat\mu(u) = \int_{\mathbb R^d} e^{i\inner{u,x}} \dd{\mu(x)} \]
\end{definition}

Note that $\abs{\hat \mu(u)} \leq \mu(\mathbb R^d)$, and $\hat \mu$ a bounded cts fcn on $\mathbb{R}^d$.
If $\mu$ has a density $f$ (wrt $\lambda$), $\hat\mu = \int_{\mathbb R^d} e^{i\inner{u,x}} f(x) \dd{x} = \hat f$.

\begin{definition}[Characteristic Function]
	Let $X$ be an $\mathbb R^d$-valued r.v..
	The \vocab{characteristic function (c.f.)} $\varphi_X$ of $X$ is the Fourier transform of its law $\mu_X = \mathbb{P} \circ X\inv$.
	So,
	\begin{align*}
		\varphi_X(u) = \hat \mu_X(u) = \int e^{i \inner{u, x}} \underbrace{\dd{\mu_X(x)}}_{\dd{\mathbb{P}} \circ X\inv(x)}\footnote{Note that $\nu \circ f\inv(g) = \nu(f \circ g)$.} = \int e^{i\inner{u, x}} \dd{\mathbb{P}} = \expect{e^{i\inner{u,X}}}.
	\end{align*}
\end{definition}

In particular if $X$ has pdf $f$, then $\phi_X(u) = \hat{f}(u)$.

\begin{definition}[Fourier Inversion Formula]
	Let $f \in L^1(\mathbb R^d)$ s.t. $\hat f \in L^1(\mathbb R^d)$.
	Then we say that the \vocab{Fourier inversion formula} holds for $f$ if
	\[ f(x) = \frac{1}{(2\pi)^d} \int_{\mathbb R^d} \hat f(u) e^{-i\inner{u,x}} \dd{u} \]
	a.e. in $\mathbb R^d$.
\end{definition}

\begin{remark}
	The RHS is cts by DCT, so for $f$ cts the equality is everywhere.
\end{remark}

\begin{remark}
	The map from $L^1 \to C_0$ by $f \mapsto \hat{f}$ is $1-1$ (for $f, g \in L^1$ with $\hat{f} = \hat{g}$, then $f - g \in L^1$ and $\widehat{f - g} = \hat{f} - \hat{g} = 0$. So by Fourier Inversion $f - g = 0$ a.e.)
\end{remark}

% \begin{definition}[Plancherel identity]
% 	Let $f \in L^1(\mathbb R^d) \cap L^2(\mathbb R^d)$.
% 	Then the \vocab{Plancherel identity} holds for $f$ if
% 	\[ \norm{\hat f}_2 = (2\pi)^{\frac d2} \norm{f}_2 \]
% \end{definition}

% We will show that the Fourier inversion formula holds whenever $\hat f \in L^1(\mathbb R^d)$, and the Plancherel identity holds for all $f \in L^1(\mathbb R^d) \cap L^2(\mathbb R^d)$.

% \begin{remark}
% 	Given the Plancherel identity, the Fourier transform is a linear isometry of $L^2(\mathbb R^d)$, by approximating any function in $L^2(\mathbb R^d)$ by integrable functions.
% \end{remark}

\subsection{Convolutions}

A key concept in Fourier analysis is convolutions.

\begin{definition}[Convolution]
	Let $f \in L^p(\mathbb R^d)$, $1 \leq p < \infty$ and $\nu$ be a probability measure on $\mathbb R^d$.
	We define their \vocab{convolution} $f \ast \nu$ by
	\[ (f \ast \nu)(x) = \begin{cases}
		\int_{\mathbb R^d} f(x-y) \dd{\nu(y)} & \text{if integral exists;} \\
		0 & \text{else.}
	\end{cases} \]
\end{definition}

\begin{remark}
	If $1 \leq p < \infty$, by Jensen's inequality,
	\begin{align*}
		\int_{\mathbb{R}^d} \abs{f \ast \nu(x)}^p \dd{x} &\leq \int_{\mathbb R^d} \qty( \int_{\mathbb R^d} \abs{f(x-y)} \dd{\nu(y)} )^p \dd{x} \\
		&\leq \int_{\mathbb R^d} \int_{\mathbb R^d} \abs{f(x-y)}^p \dd{\nu(y)} \dd{x} \quad \text{as $p \geq 1$}\\
		&= \int_{\mathbb R^d} \int_{\mathbb R^d} \abs{f(x-y)}^p \dd{x} \dd{\nu(y)} \\
		&= \int_{\mathbb R^d} \int_{\mathbb R^d} \abs{f(x)} \dd{\nu(y)} \dd{x} \\
		&= \int_{\mathbb R^d} \abs{f(x)} \dd{x} \\
		&= \norm{f}_p^p < \infty.
	\end{align*}
	Hence $f \ast v$ is defined a.e., and $\norm{f \ast v}_p \leq \norm{f}_p < \infty$.
	When $\nu$ has pdf $g \in L^1$\footnote{wrt Lebesgue measure}, $f \ast v(x) = \int f(x-y) g(y) \dd{y} = f \ast g(x)$.

	% For two probability measures $\mu, \nu$ on $\mathbb{R}^d$, the convolution $\mu \ast \nu$ is a new prob measure defined as
	% \begin{align*}
	% 	\mu \ast \nu(A) &= \int \int 1_A(x+y) \dd{\mu(x)} \dd{\nu(x)} = \mu \otimes \nu (x + y \in A) \\
	% 	&= \mathbb{P}(X + Y \in A)
	% \end{align*} where $X, Y$ independent and $X \sim \mu$ and $Y \sim \nu$.

	% If $\mu$ has pdf $f \in L^1$ then
	% \begin{align*}
	% 	\mu \ast \nu(A) &= \int \left(  \right)
	% \end{align*}


	% So $f \in L^p(\mathbb R^d)$, we have $(y \mapsto f(x-y)) \in L^p(\nu)$ almost everywhere, and again by Jensen's inequality,
	% \[ \norm{f \ast \nu}_p^p = \int_{\mathbb R^d} \abs{ \int_{\mathbb R^d} f(x-y)\dd{\nu(y)} }^p \dd{x} \leq \int_{\mathbb R^d} \qty( \int_{\mathbb R^d} \abs{f(x-y)} \dd{\nu(y)} )^p \dd{x} \leq \norm{f}_p^p \]
	% Hence $f \mapsto f \ast \nu$ is a contraction on $L^p(\mathbb R^d)$.
\end{remark}

In the case where $\nu$ has a density $g$ with respect to the Lebesgue measure, we write $f \ast g = f \ast \nu$.

\begin{definition}[Convolution]
	For probability measures $\mu, \nu$ on $\mathbb R^d$, their \vocab{convolution} $\mu \ast \nu$ is a probability measure on $\mathbb R^d$ given by the law of $X + Y$ where $X, Y$ are independent r.v.s with laws $\mu$ and $\nu$, so
	\begin{align*}
		(\mu \ast \nu)(A) &= \prob{X+Y \in A} \\
		&= \int_{\mathbb R^d \times \mathbb R^d} 1_A(x+y) \dd{(\mu \otimes \nu)(x, y)} \\
		&= \int_{\mathbb R^d} \int_{\mathbb R^d} 1_A(x+y) \dd{\nu(y)} \dd{\mu(x)}
	\end{align*}
\end{definition}

If $\mu$ has density $f \in L^1$ wrt the Lebesgue measure, $\mu \ast \nu$ has density $f \ast \nu$ wrt the Lebesgue measure.
Indeed,
\begin{align*}
	(\mu \ast \nu)(A) &= \int_{\mathbb R^d} \int_{\mathbb R^d} 1_A(x+y) f(x) \dd{x} \dd{\nu(y)} \\
	&= \int_{\mathbb R^d} \int_{\mathbb R^d} 1_A(x) f(x-y) \dd{x} \dd{\nu(y)} \\
	&= \int_{\mathbb R^d} 1_A(x) \int_{\mathbb R^d}f(x-y) \dd{\nu(y)} \dd{x} \\
	&= \int_{\mathbb R^d} 1_A(x) (f \ast \nu)(x) \dd{x}
\end{align*}

\begin{proposition}
	$\widehat{f \ast \nu}(u) = \hat f(u) \hat \nu(u)$ for all $f \in L^1$ and $\nu$ a prob measure.
\end{proposition}

\begin{proposition}
	$\widehat{\mu \ast \nu}(u) = \expect{e^{i\inner{u,X+Y}}} = \expect{e^{i\inner{u,X}}e^{i\inner{u,Y}}} = \hat \mu(u) \hat \nu(u)$ for all $\mu, \nu$ prob measures.
\end{proposition}

\subsection{Fourier transforms of Gaussians}

\begin{definition}[Normal Distribution]
	The \vocab{normal distribution} $N(0,t)$ is given by the probability density function
	\[ g_t(x) = \frac{1}{\sqrt{2\pi t}} e^{-\frac{x^2}{2t}} \]
\end{definition}

If $\varphi_Z$ is the characteristic function of $Z \sim N(0, 1)$, i.e. $\phi_Z(u) = \int \frac{1}{\sqrt{2 \pi}} e^{-z^2 / 2} e^{iuz} \dd{z}$ then by a previous thm, $\phi_z$ is differentiable and can use DUTIS.
\begin{align*}
	\dv{u} \varphi_Z(u) &= \frac{1}{\sqrt{2 \pi}} \int \dv{u} \left(e^{-z^2 / 2} e^{iuz}\right) \dd{z} \\
	&= \frac{1}{\sqrt{2 \pi}} \int iz e^{-z^2 / 2} e^{iuz} \dd{z} \\
	&= \frac{i}{\sqrt{2\pi}} \int \underbrace{e^{iuz}}_{v} \underbrace{z e^{-\frac{z^2}{2}}}_{w'} \dd{z} \\
	&= \frac{i}{\sqrt{2\pi}} \int iu e^{iux} e^{-\frac{z^2}{2}} \dd{x} \\
	&= -u \varphi_Z(u)
\end{align*}

Hence,
\[ \dv{u}\qty(e^{\frac{u^2}{2}} \varphi_Z(u)) = ue^{\frac{u^2}{2}} \varphi_Z(u) - e^{\frac{u^2}{2}} u \varphi_Z(u) = 0 \]
In particular, $\varphi_Z(u) = \varphi_Z(0) e^{-\frac{u^2}{2}} = e^{-\frac{u^2}{2}}$.
In other words, $\hat g_1(u) = \sqrt{2\pi} g_1(u)$.

In $\mathbb R^d$, consider a Gaussian r.v. $Z = (Z_1, \dots, Z_d)$ with iid entries $N(0,1)$.
Then, the joint pdf (wrt $\lambda^d$) of $\sqrt{t}Z$ is
\[ g_t(x) = \prod_{j=1}^d \frac{1}{\sqrt{2\pi t}} e^{-\frac{x_j^2}{2t}} = (2\pi t)^{-\frac{d}{2}} e^{-\frac{\norm{x}^2}{2t}} \]
The Fourier transform of $g_t$ is
\[ \hat g_t(u) = \expect{e^{i\inner{u,\sqrt{t}Z}}} = \expect{\prod_{j=1}^d e^{iu_j \sqrt{t} z_j}} = \prod_{j=1}^d \underbrace{\expect{e^{iu_j \sqrt{t} z_j}}}_{\phi_{Z_i}(\sqrt{t}u_i)} = \prod_{j=1}^d e^{-u_j^2 \frac{t}{2}} = e^{-\frac{\norm{u}^2 t}{2}} \]
which implies that in general, $\hat g_t(u) = \frac{(2\pi)^{\frac{d}{2}}}{t^{\frac{d}{2}}} \frac{t^{\frac{d}{2}}}{(2\pi)^{\frac{d}{2}}} e^{-\frac{\norm{u}^2 t}{2}} = (2\pi)^{\frac{d}{2}} t^{-\frac{d}{2}} g_{\frac{1}{t}}(u)$.
Taking the Fourier transform with respect to $u$, $\hhat g_t = (2\pi)^{\frac{d}{2}} t^{-\frac{d}{2}} \hat{g}_{\frac{1}{t}}(u) = (2\pi)^d g_t$.
Since $g_t(-x) = g_t(x)$ and the Lebesgue measure is translation invariant, we have
\begin{align*}
	g_t(x) = \frac{1}{(2\pi)^d} \hhat g_t(x) = \frac{1}{(2\pi)^d} \int_{\mathbb R^d} e^{-i\inner{u,x}} \hat g_t(u) \dd{u}
\end{align*}
so the Fourier inversion theorem holds for $g_t$.

\begin{definition}[Gaussian Convolution]
	We say that a function on $\mathbb R^d$ is a \vocab{Gaussian convolution} if it is of the form
	\[ f \ast g_t(x) = \int_{\mathbb R^d} f(x-y) g_t(y) \dd{y} \]
	where $x \in \mathbb R^d, t > 0, f \in L^1(\mathbb R^d)$.
\end{definition}

\begin{enumerate}
	\item $f \ast g_t \in L^1$ as $f \in L^1$ (proved earlier) and $\norm{f \ast g_t}_1 \leq \norm{f}_1 < \infty$.
	\item $f \ast g_t$ is continuous on $\mathbb R^d$ by noting $f \ast g_t(x) = \int_{\mathbb R^d} f(y) g_t(x-y) \dd{y}$ by translation invariance, then using DCT noting $g$ bounded as cts.
	\item $f \ast g_t$ is bdd.
	\item  $\widehat{f \ast g_t}(u) = \hat{f}(u) \hat{g}_t(u) = \hat{f}(u) e^{-\frac{\norm{u}^2 t}{2}}$.
	\item $\widehat{f \ast g_t}$ is bdd cts as $f \ast g_t \in L_1$.
	\item $\norm{\widehat{f \ast g_t}} \leq c_t \norm{\hat{f}}_\infty \leq c_t \norm{\hat{f}}_1$.
	\item For $\mu$ a prob measure and any $t > 0$, $\mu \ast g_t$ is a Gaussian convolution. As, $g_t = g_{t / 2} \ast g_{t / 2}$ as $g_t$ is the density of a $N(0, t)$ r.v.. Then $\mu \ast g_t = \underbracket{(\mu \ast g_{t / 2})}_{L_1} \ast g_{t / 2}$.
\end{enumerate}

\begin{lemma}
	The Fourier inversion theorem holds for all Gaussian convolutions.
\end{lemma}

\begin{proof}
	Let $f \in L_1$ and $t > 0$.
	We can use the Fourier inversion theorem for $g_t(y)$ to see that
	\begin{align*}
		(2\pi)^d f \ast g_t(x) &= (2\pi)^d \int_{\mathbb R^d} f(x-y) g_t(y) \dd{y} \\
		&= \int_{\mathbb R^d} f(x-y) \int_{\mathbb R^d} e^{-i\inner{u,y}} \hat g_t(u) \dd{u} \dd{y} \\
		&= \int_{\mathbb R^d} e^{-i\inner{u,x}} \int_{\mathbb R^d} f(x-y) e^{i\inner{u,x-y}} \dd{y} \hat g_t(u) \dd{u} \\
		&= \int_{\mathbb R^d} e^{-i\inner{u,x}} \int_{\mathbb R^d} f(z) e^{i\inner{u,z}} \dd{z} \hat g_t(u) \dd{u} \\
		&= \int_{\mathbb R^d} e^{-i\inner{u,x}} \hat f(u) \hat g_t(u) \dd{u} \\
		&= \int_{\mathbb R^d} e^{-i\inner{u,x}} \widehat{f \ast g_t}(u) \dd{u}
	\end{align*}
\end{proof}

% \begin{remark}
% 	If $\mu$ is a finite measure, then $\mu \ast g_t = \mu \ast g_{\frac{t}{2}} \ast g_{\frac{t}{2}}$ with $\mu \ast g_{\frac{t}{2}} \in L^1$, so is also a Gaussian convolution.
% \end{remark}

\begin{lemma}[Gaussian convolutions are dense in $L^p$]
	Let $f \in L^p(\mathbb{R}^d)$ where $1 \leq p < \infty$.
	Then $\norm{f \ast g_t - f}_p \to 0$ as $t \to 0$.
\end{lemma}

\begin{proof}
	One can easily show that the space $C_c(\mathbb R^d)$ of continuous functions of compact support is dense in $L^p$.
	Hence, given $\varepsilon > 0$, $\exists h \in C_c(\mathbb R^d)$ s.t. $\norm{f - h}_p < \frac{\varepsilon}{3}$.
	Then by linearity of convolution,
	\[ \norm{f \ast g_t - h \ast g_t}_p = \norm{(f - h) \ast g_t}_p \leq \norm{f - h}_p < \frac{\varepsilon}{3} \]
	So by Minkowski's inequality,
	\begin{align*}
		\norm{f \ast g_t - f}_p \leq \underbracket{\norm{f \ast g_t - h \ast g_t}_p}_{\leq \epsilon / 3} + \underbracket{\norm{h - f}_p}_{\leq \epsilon / 3} + \norm{h \ast g_t + h}_p
		\leq \frac{2\varepsilon}{3} + \norm{h \ast g_t - h}_p
	\end{align*}
	so it suffices to prove the result for $f = h \in C_c(\mathbb R^d)$.
	We define a new map
	\[ e(y) = \int_{\mathbb R^d} \abs{h(x-y) - h(x)}^p \dd{x} \]
	Since $h$ is bdd (cts on compact support) and supported on $[-M, M]$ say, for some $M > 0$.
	As $y \to 0$, $|h(x-y) - h(x)|^p \to 0$ as $h$ cts.
	Also for $|y| < 1$, $|h(x-y) - h(x)|^p \leq 2^p \norm{h(x)}^p_\infty 1_{|x| \leq M + 1}$, with the RHS being integrable.
	Hence by DCT, $e(y) \to 0$ as $y \to 0$.

	Hence, by Jensen's inequality,
	\begin{align*}
		\norm{h \ast g_t - h}_p^p &= \int_{\mathbb R^d} \abs{ \int_{\mathbb R^d} h(x-y) g_t(y) \dd{y} - h(x)}^p \dd{x} \\
		&= \int_{\mathbb R^d} \abs{ \int_{\mathbb R^d} (h(x-y) - h(x)) g_t(y) \dd{y} }^p \dd{x} \\
		&\leq \int_{\mathbb R^d} \int_{\mathbb R^d} \abs{h(x-y) - h(x)}^p \dd{x} g_t(y) \dd{y} \\
		&= \int_{\mathbb R^d} e(y) g_t(y) \dd{y} \\
		&= \int_{\mathbb R^d} e(y) \frac{1}{t^{d/2}} g_1 \qty(\frac{y}{\sqrt{t}})\footnote{Note that $g_t(u) = \frac{1}{t^{d/2}} g_1 \qty(\frac{u}{\sqrt{t}})$} \dd{y} \\
		&= \int_{\mathbb R^d} \underbracket{e(\sqrt{t} z)}_{\to e(0) = 0 \text{ as } t \to 0} g_1(z) \dd{z} \\
		&\to 0 \text{ by DCT.}
	\end{align*}
\end{proof}
% TODO: Convert random instances of g into g_t
\begin{theorem}[Fourier Inversion]
	Let $f \in L^1(\mathbb R^d)$ be s.t. $\hat f \in L^1(\mathbb R^d)$.
	Then a.e. in $\mathbb{R}^d$,
	\[ f(x) = \frac{1}{(2\pi)^d} \int_{\mathbb R^d} e^{-i\inner{u,x}} \hat f(u) \dd{u} \]
\end{theorem}

\begin{remark}
	This proves that the Fourier transform is injective; $\hat f = \hat g$ implies $\widehat{f - g} = 0$ so by Fourier inversion, $f = g$ almost everywhere.
	The identity holds everywhere on $\mathbb R^d$ for the (unique) continuous representative $f$ in its equivalence class.
\end{remark}

\begin{proof}
	Consider $f \ast g_t$ and
	\begin{align*}
		f_t(x) = \frac{1}{(2\pi)^d} \int_{\mathbb R^d} e^{-i\inner{u,x}} \hat f(u) \underbracket{e^{\frac{-\abs{u}^2 t}{2}}}_{\hat{g_t}(u)} \dd{u}
	\end{align*}
	As FI holds for $f \ast g_t$, $f \ast g_t = f_t$.

	So, $\norm{f_t - f}_1 \overset{t \to 0}{\longrightarrow} 0$ by density of Gaussian convolutions and as $f \in L^1$.
	So $f_t \to f$ in $\mathbb{P}$ and thus $\exists$ a subsequence s.t. $f_{t_n} \to f_t$ a.e.

	Also, $e^{-i\inner{u,x}} \hat f(u) e^{\frac{-\abs{u}^2 t}{2}}$ is bounded by $\abs{\hat f(u)}$, which is integrable, and $\to e^{-i\inner{u,x}} \hat f(u)$ as $t \to 0$.
	So by DCT, $f_t(x) \to \frac{1}{(2\pi)^d} \int e^{-i\inner{u,x}} \hat f(u) \dd{u}$ as $t \to 0$ a.e.

	Hence $f = \frac{1}{(2\pi)^d} \int e^{-i\inner{u,x}} \hat f(u) \dd{u}$ a.e. as $f_t$ converges to it a.e. and $f_{t_n} \to f$.
\end{proof}

\begin{theorem}[Plancherel]
	Let $f \in L^1(\mathbb R^d) \cap L^2(\mathbb R^d)$.
	Then $\norm{f}_2 = (2\pi)^{-\frac{d}{2}} \norm{\hat f}_2$.
\end{theorem}

\begin{remark}
	By the Pythagorean identity, $\inner{f, g} = (2\pi)^{-d} \inner{\hat f, \hat g}$.
\end{remark}

\begin{proof}
	Initially, we assume $f, \hat f \in L^1$.
	In this case, $f, \hat f \in L^\infty$, and $(x,u) \mapsto f(x)\hat f(u)$ is integrable for the product Lebesgue measure $\dd{x} \otimes \dd{u}$ on $\mathbb R^d \times \mathbb R^d$, so Fubini's theorem for bounded functions applies.
	\begin{align*}
		(2\pi)^d \norm{f}_2^2 &= (2\pi)^d \int_{\mathbb R^d} f(x) \overline{f(x)} \dd{x} \\
		&= \int_{\mathbb R^d} \qty(\int_{\mathbb R^d} e^{-i\inner{u,x}} \hat f(u) \dd{u}) \overline{f(x)} \dd{x} < \infty (\text{ and } f \in L^2) \\
		&= \int_{\mathbb R^d} \hat f(u) \overline{\int_{\mathbb R^d} e^{i\inner{u,x}} f(x) \dd{x}} \dd{u} \\
		&= \int_{\mathbb R^d} \hat f(u) \overline{\hat f(u)} \dd{u} \\
		&= \norm{\hat f}_2^2
	\end{align*}

	Now, let $f \in L^1 \cap L^2$.
	For $t > 0$, take $f_t = f \ast g_t \underset{t \to 0}{\longrightarrow} f$ in $L^2$ and so $\norm{f_t}_2 \underset{t \to 0}{\longrightarrow} \norm{f}_2$ continuity of the norm.
	Also, $\hat{f_t}(u) = \hat{f}(u) \hat{g_t}(u) = \hat f(u) e^{-\frac{\abs{u}^2 t}{2}}$.
	So $\abs{\hat{f_t}(u)} \uparrow \abs{\hat{f}(u)}$ as $t \to 0$.
	Thus $\norm{\hat{f_t}(u)}_2^2 = \int |\hat{f_t}(u)|^2 \dd{u} \underset{t \to 0}{\longrightarrow} \int |\hat{f}(u)|^2 \dd{u} = \norm{\hat{f}}_2^2$ by MCT.

	But, $f_t = f \ast g_t \in L^1$, and $\hat{f_t} \in L^1$.
	So by the first part of the proof, $(2\pi)^d \norm{f_t}_2^2 = \norm{\hat{f_t}}_2^2$.
	Letting $t \to 0$, we get $(2\pi)^d \norm{f}_2^2 = \norm{\hat{f}}_2^2$.
\end{proof}

\begin{remark}
	Since $L_1 \cap L_2$ is dense in $L^2$, we can extend the linear operator $F_0(f) = (2\pi)^{-\frac{d}{2}} \hat f$ to $L^2$ by continuity to a linear isometry $F \colon L^2 \to L^2$ known as the \emph{Fourier--Plancherel transform}.
	One can show that $F$ is surjective with inverse $F^{-1} \colon L^2 \to L^2$.
\end{remark}

\subsection{Characteristic fcns, Weak Convergence and the CLT}

\begin{definition}[Characteristic Function]
	For a r.v. $X$, its \vocab{characteristic function} is
	\begin{align*}
		\phi_X(t) = \mathbb{E}[e^{itx}] = \widehat{\mu_X} = \int e^{i \inner{t, x}} \dd{\mu_X(x)}
	\end{align*}
\end{definition}

\begin{example}
	Consider the Dirac measure $\delta_0$ on $\mathbb R$, so $\hat \delta_0(u) = \int_{\mathbb R} e^{iux} \dd{\delta_0(x)} = 1$.
	But the inverse Fourier transform would be $\frac{1}{2\pi} \int_{\mathbb R} e^{iux} \dd{u}$ which is not a Lebesgue integrable function.
\end{example}

To circumvent this, we test `$\mu$' on nice test fcns $f$.

\begin{remark}
	2 p.m.s $\mu, \nu$ on $\mathbb{R}^d$ coincide $\iff$ $\int f \dd{\mu} = \int f \dd{\nu} \quad \forall f : \mathbb{R}^d \to \mathbb{R}$ bdd cts\footnote{RHS implies LHS, as if true for $f = 1_{[a, b]}$ then true on a $\pi$-system so done. We can approximate such $f$ with bdd cts fcns and so done.}.
	In fact, enough to have condition holding $\forall f \in C_c^\infty$ (space of infinitely differentiable fcns with compact support).
\end{remark}

\begin{aside}{Aside}
	($\mu : f \mapsto \mu(f)$ mapping from $C_c^\infty \to \mathbb{R}$ is a linear, cts (on $L_f$ topology), hence $\mu$ is ``Schwartz distribution'').

\end{aside}

\begin{definition}[Weak Convergence]
	Let $(\mu_n), \mu$ be Borel prob measures on $\mathbb{R}^d$.
	Then \vocab{$\mu_n$ converges to $\mu$ weakly} if $\int f \dd{\mu_n} \to \int f \dd{\mu}$ as $n \to \infty$ for all $f : \mathbb{R}^d \to \mathbb{R}$ bdd cts.
\end{definition}

\begin{remark} \
	\begin{enumerate}
		\item For a sequence of r.v.s $X_n$ and $X$ another r.v., $X_n \to X$ weakly if $\mu_{X_n} \to \mu_X$ weakly.
		\item A sequence of prob measures, $\mu_n$, can have at most one weak limit by previous remark (the one about 2 p.m.s. coinciding).
		\item If $X_n \to X$ weakly, and $h : \mathbb{R}^d \to \mathbb{R}^k$ cts, then $h(X_n) \to h(X)$ weakly (as r.v. in $\mathbb{R}^k$). (Continuous Mapping Theorem) (from definition as $f \circ h$ bdd cts if $f$ bdd cts).
		\item Suffices to check condition in definition for all $f \in C_c^\infty$. (``tightness'' argument, i.e. $\exists K$ compact s.t. $\mu_n(K^c) < \epsilon \quad \forall n$ if $\mu_n \to \mu$ weakly. Sheet 4)
		\item When $d = 1$, this is equivalent to $X_n \to X$ in distribution (i.e., $F_{X_n}(x) \to F_X(x)$ at all points where $x \mapsto F_X(x)$ is cts). Sheet 4 Q1, approximate indictators.
	\end{enumerate}
\end{remark}

\begin{theorem}
	Let $X$ be a r.v. in $\mathbb R^d$ with law $\mu_X$.
	Then the characteristic function $\varphi_X = \hat \mu_X$ uniquely determines $\mu_X$.
	In addition, if $\varphi_X \in L^1$, then $\mu_X$ has a bdd cts pdf $f_X(x) = \frac{1}{(2\pi)^d} \int_{\mathbb R^d} e^{-i\inner{u,x}} \varphi_X(u) \dd{u}$ a.e..
\end{theorem}

\begin{proof}
	Let $Z = (Z_1, \dots, Z_d)$ be a vector of independent and identically distributed r.v.s, independent of $X$, with $Z_j \sim N(0,1)$.
	Then $\sqrt{t} Z$ has pdf $g_t$ and $X + \sqrt{t} Z$ has pdf $f_t = \mu_X \ast g_t$.
	Then, $\hat{f_t}(u) = \hat{\mu_X}(u) \hat{g_t}(u) = \phi_X(u) e^{-\frac{\abs{u}^2 t}{2}}$.
	So by F.I. of Gaussian convolutions,
	\[ f_t(x) = \frac{1}{(2\pi)^d} \int_{\mathbb R^d} e^{-i\inner{u,x}} \varphi_X(u) e^{-\frac{\abs{u}^2 t}{2}} \dd{u} \]
	which is uniquely determined by $\varphi_X$.

	We show on an example sheet that two Borel prob measures $\mu, \nu$ on $\mathbb R^d$ coincide iff $\mu(g) = \nu(g) \quad \forall g \colon \mathbb R^d \to \mathbb R$ bdd, cts with compact support.
	Now,
	\[ \int_{\mathbb R^d} g(x) f_t(x) \dd{x} = \expect{\underbrace{g(X + \sqrt{t} Z)}_{\to X \text{ a.s.}}} \]
	Since $\abs{g(X + \sqrt{t}Z)} \leq \norm{g}_\infty < \infty$, by BCT, this converges as $t \to 0$ to $\expect{g(X)} = \int_{\mathbb R^d} g(x) \dd{\mu_X(x)} (\ast)$.
	So by uniqueness of limits, $\varphi_X$ determines $\mu_X$.

	If $\varphi_X \in L^1$, then $ e^{-i\inner{u,x}} \varphi_X(u) e^{-\frac{\abs{u}^2 t}{2}} \underset{t \to 0}{\longrightarrow} e^{-i\inner{u,x}} \phi_X(u)$.
	By DCT, $f_t(x) \to f_X(x)$ as $t \to 0 \; \forall x$.

	In particular, since $\mu_X \ast g_t \geq 0$, $f_X \geq$ on $\mathbb R^d$.
	Then, for any bdd cts $g$ with compact support,

	\begin{align*}
		\int \underbracket{g(x) f_t(x)}_{\text{As } t \to 0, \to g(x) f_X(x)} \dd{x} &\to \int g(x) f_X(x) \dd{x} \text{ by DCT as } |f_t(x)| \leq \frac{1}{(2\pi)^d} \norm{\phi_X}_1
	\end{align*}
	By $(\ast)$, $\int g(x) \dd{\mu_X(x)} = \int g(x) f_X(x) \dd{x} \quad \forall g$ bdd cts with compact support.

	Thus $\mu_X$ has density $f_X$.
\end{proof}

\begin{theorem}[L\'evy's continuity theorem]
	Let $X_n, X$ be r.v.s on $\mathbb R^d$, s.t. $\varphi_{X_n}(u) \to \varphi_X(u) \; \forall u$, as $n \to \infty$.
	Then $X_n \to X$ weakly.
\end{theorem}

\begin{remark}
	\begin{enumerate}
		\item A stronger version of this theorem is that if $\phi_{X_n}(u) \to \phi(u) \; \forall u$ for some fcn $\phi$ that is cts in a nbd of $0$, then $\phi$ is the c.f. of some r.v. $X$ and $X_n \to X$ weakly.
		\item \underline{Cramer' Wold device}: Let $(X_n), X$ be r.v.s on $\mathbb{R}^d$, then $X_n \to X$ weakly iff $\inner{u, X_n} \to \inner{u, X} \; \forall u \in \mathbb{R}^d$ weakly or in distribution in $\mathbb{R}$. LHS $\implies RHS$ by continuous mapping theorem, the converse holds as $e^{i \inner{u, x}}$ is bdd cts so $\mathbb{E}[f(X_n)] = \phi_{X_n}(u) \to \mathbb{E}[f(x)] = \phi_X(u) \; \forall u$. So done by L\'evy's.
		\item The converse holds by definition of weak convergence, testing against the complex exponentials in the Fourier transform.
	\end{enumerate}
\end{remark}

\begin{proof}
	Let $g : \mathbb{R}^d \to \mathbb{R}$ be compactly suppored and Lipschitz cts, i.e. $|g(x) - g(y)| \leq C_g |x-y| \; \forall x, y \in \mathbb{R}^d$.
	This includes all $g \in C_c^\infty$ as any fcn with bounded derivative is Lipschitz.
	Enough to show, $\mathbb{E}[g(X_n)] \to \mathbb{E}[g(X)]$.

	Let $Z \sim N(0, I_d)$ indep of $(X_n), X$.
	Then for fixed $\epsilon > 0$, choose $t > 0$ small enough s.t. $C_g \sqrt t \expect{\abs{Z}} \leq \frac{\varepsilon}{3}$.
	Then,
	\begin{align*}
		\abs{\mu_{X_n}(g) - \mu_X(g)} &= \abs{\expect{g(X_n)} - \expect{g(X)}} \\
		&\leq \abs{\expect{g(X_n) - g(X_n + \sqrt t Z)}} + \abs{\expect{g(X) - g(X + \sqrt t Z)}} \\
		&+ \abs{\expect{g(X_n + \sqrt t Z) - g(X + \sqrt t Z)}} \\
		&\leq \expect{\abs{g(X_n) - g(X_n + \sqrt t Z)}} + \expect{\abs{g(X) - g(X + \sqrt t Z)}} \\
		&+ \abs{\expect{g(X_n + \sqrt t Z) - g(X + \sqrt t Z)}} \\
		&\leq 2 C_g \sqrt t \expect{\abs{Z}} + \abs{\expect{g(X_n + \sqrt t Z) - g(X + \sqrt t Z)}} \\
		&\leq \frac{2\varepsilon}{3} + \abs{\expect{g(X_n + \sqrt t Z) - g(X + \sqrt t Z)}}
	\end{align*}
	We show that the remaining term can be made less than $\frac{\varepsilon}{3}$ as $n \to \infty$.
	Let $f_{t,n}(x) = g_t \ast \mu_{X_n}$.
	Then, by Fourier inversion for Gaussian convolutions,
	\begin{align*}
		\expect{g(X_n + \sqrt t Z)} &= \int_{\mathbb R^d} g(x) f_{t,n}(x) \dd{x} \\
		&= \frac{1}{(2\pi)^d} \int_{\mathbb R^d} g(x) \int_{\mathbb R^d} e^{-i\inner{u,x}} \varphi_{X_n}(u) e^{-\frac{\abs{u}^2 t}{2}} \dd{u} \dd{x}
	\end{align*}
	Since characteristic functions are bounded by 1, we can use DCT with dominating function $\abs{g(x)} e^{-\frac{\abs{u}^2 t}{2}}$ to find
	\begin{align*}
		\expect{g(X_n + \sqrt t Z)} &\to \frac{1}{(2\pi)^d} \int_{\mathbb R^d} g(x) \int_{\mathbb R^d} e^{-i\inner{u,x}} \varphi_X(u) e^{-\frac{\abs{u}^2 t}{2}} \dd{u} \dd{x} \\
		&= \int_{\mathbb R^d} g(x) f_t(x) \dd{x} \\
		&= \expect{g(X + \sqrt t Z)}
	\end{align*}
	where $f_t = g_t \ast \mu_X$.
	So as $n \to \infty$, the difference between these two terms can be made less than $\frac{\varepsilon}{3}$ as required.
\end{proof}

\begin{note}
	We like adding Gaussians, as pdf of $X + Z$ exists due to $Z$ having a pdf. Also, pdf is a Gaussian convolution, which is nice.
\end{note}

\begin{theorem}[Central Limit Theorem]
	Let $X_1, \dots, X_n$ be iid r.v.s on $\mathbb{R}$ with $\expect{X_i} = 0$ and $\Var{X_i} = 1$.
	Let $S_n = \sum_{i=1}^n X_n$.
	Then
	\[ \frac{1}{\sqrt{n}} S_n \xrightarrow{\text{weakly}}\footnote{As $d = 1$ equiv to in distribution convergence} Z \sim N(0,1) \]
	In particular,
	\[ \prob{\frac{1}{\sqrt{n}} S_n \leq x} \to \prob{Z \leq x} \]
\end{theorem}

\begin{proof}
	Let $X = X_1$.
	The characteristic function $\varphi(u) = \varphi_X(u) = \expect{e^{iuX}}$ satisfies $\varphi(0) = 1$.
	As $\mathbb{E}[X^2] < \infty$ by DUTIS $\varphi'(u) = i \expect{X e^{iuX}}$,    $\varphi''(u) = i^2 \expect{X^2 e^{iuX}}$ (Sheet 3).
	We can find $\varphi'(0) = i\expect{X} = 0$ and $\varphi''(0) = -\expect{X^2} = -\Var X = -1$.
	By Taylor's theorem, $\varphi(v) = 1 - \frac{v^2}{2} + o(v^2)$ as $v \to 0$.
	Now, denoting $\varphi_n(u) = \varphi_{\frac{1}{\sqrt n} S_n}(u)$, we can write
	\begin{align*}
		\varphi_n(u) &= \expect{e^{i\frac{u}{\sqrt n} (X_1 + \dots + X_n)}} \\
		&= \prod_{j=1}^n \expect{e^{i\frac{u}{\sqrt n} X_j}} \\
		&= \qty[\varphi\qty(\frac{u}{\sqrt n})]^n \\
		&= \qty[1 - \frac{u^2}{2n} + o\qty(\frac{u^2}{n})]^n \\
		&= \qty[1 - \frac{u^2}{2n} + o\qty(\frac{1}{n})]^n \text{ fixing $u$ and letting $n \to \infty$}
	\end{align*}
	The complex logarithm satisfies $\log(1 + z) = z + o(z)$, so by taking logarithms, we find
	\[ \log \varphi_n(u) = n \log\qty(1 - \frac{u^2}{2n} + o\qty(\frac{1}{n})) = -\frac{u^2}{2} \]
	Hence, $\varphi_n(u) \to e^{-\frac{\abs{u}^2}{2}} = \varphi_Z(u)$.
	So by L\'evy's continuity theorem, the result follows.
\end{proof}

\begin{remark}
	The CLT in $\mathbb{R}^d$ can be proved similarly using the Cramer-Wold device and properties of multi-variate Guassians.

	% This theorem extends to $\mathbb R^d$ by using the next proposition, using the fact that $X_n \to X$ weakly in $\mathbb R^d$ if and only if $\inner{X_n, v} \to \inner{X, v}$ weakly in $\mathbb R$ for all $v \in \mathbb R^d$.
\end{remark}

\begin{definition}[Gaussian]
	A r.v. on $\mathbb{R}$ is \vocab{Gaussian} ($N(\mu, \sigma)$), if it has density
	\begin{align*}
		\frac{1}{\sqrt{2\pi} \sigma} e^{- \frac{(x - \mu)^2}{2\sigma^2}}
	\end{align*}
	for $\mu \in \mathbb{R}$, $\sigma > 0$.
\end{definition}

\begin{definition}[Gaussian]
	A r.v. $X$ in $\mathbb R^d$ is \vocab{Gaussian} if $\inner{X_n, v}$ are Gaussian for each $v \in \mathbb R^d$.
\end{definition}

\begin{example}
	If $X_1, \dots, X_n \overset{\text{iid}}{\sim} N(0, 1)$, then $X = (X_1, \dots, X_n)$ is Gaussian in $\mathbb{R}^n$.
	Check the c.f. of $\inner{X, v}$
\end{example}

\begin{proposition}
	Let $X$ be Gaussian in $\mathbb R^n$, $A$ is an $m \times n$ matrix and $b \in \mathbb R^m$.
	Then
	\begin{enumerate}
		\item $AX + b$ is Guassian in $\mathbb{R}^m$.
		\item $X \in L^2(\mathbb R^d)$, and $\mu = \expect{X}$ and $V = \Cov{(X_i, X_j)}$ exist and determine $\mu_X$.
		\item $\varphi_X(u) = e^{i\inner{\mu,u} - \frac{\inner{u,Vu}}{2}}$
		\item If $V$ is invertible, then $\mu_X$ has pdf
		\[ f_X(x) = (2\pi)^{-\frac{d}{2}} (\det V)^{-\frac{1}{2}} \exp{-\frac{1}{2} \inner{x-\mu, V^{-1}(x - \mu)}} \]
		\item Subvectors $X_{(1)}, X_{(2)}$\footnote{$X_{(1)}, X_{(2)}$ disjoint $X$} of $X$ are indep iff $\Cov{(X_{(1)}, X_{(2)})} = 0$.
	\end{enumerate}
\end{proposition}

\begin{proof}
	Proofs are easy, and in examples sheets and James Norris' notes.
\end{proof}

% \begin{proposition}
% 	Let $X_n \to X$ weakly in $\mathbb R^d$ as $n \to \infty$.
% 	Then,
% 	\begin{enumerate}
% 		\item if $h \colon \mathbb R^d \to \mathbb R^k$ is continuous, then $h(X_n) \to h(X)$ weakly;
% 		\item if $\abs{X_n - Y_n} \to 0$ in probability, then $Y_n \to X$ weakly;
% 		\item if $Y_n \to c$ in probability where $c$ is constant on $\Omega$, then $(X_n, Y_n) \to (X, c)$ weakly in $\mathbb R^d \times \mathbb R^d$.
% 	\end{enumerate}
% \end{proposition}

% \begin{remark}
% 	Combining parts (iii) and (i), $X_n + Y_n \to X + c$ weakly if $Y_n \to c$ in probability.
% 	If $d = 1$, then in addition $X_n Y_n \to c X$ weakly.
% \end{remark}

% \begin{proof}
% 	\emph{Part (i).}
% 	This follows from the fact that $gh$ is continuous for any test function $g$.

% 	\emph{Part (ii).}
% 	Let $g \colon \mathbb R^d \to \mathbb R$ be bounded and Lipschitz continuous.
% 	Then
% 	\[ \abs{\expect{g(Y_n)} - \expect{g(X)}} \leq \underbrace{\abs{\expect{g(X_n)} - \expect{g(X)}}}_{< \frac{\varepsilon}{3}} + \expect{\abs{g(X_n) - g(Y_n)}} \]
% 	where the bound on $\expect{g(X_n)} - \expect{g(X)}$ holds for sufficiently large $n$.
% 	Then the remaining term is upper bounded by
% 	\[ \expect{\abs{g(X_n) - g(Y_n)}} \qty(1_{\qty{\abs{X_n - Y_n} \leq \frac{\varepsilon}{3\norm{g}_{\mathrm{Lip}}}}} + 1_{\qty{\abs{X_n - Y_n} > \frac{\varepsilon}{3\norm{g}_{\mathrm{Lip}}}}} ) \]
% 	\[ \leq \norm{g}_{\mathrm{Lip}} \frac{\varepsilon}{3\norm{g}_{\mathrm{Lip}}} + 2\norm{g}_\infty \prob{\abs{X_n - Y_n} > \frac{\varepsilon}{3\norm{g}_{\mathrm{Lip}}}} < \frac{2\varepsilon}{3} \]
% 	for sufficiently large $n$.

% 	\emph{Part (iii).}
% 	$\abs{(X_n, c) - (X_n, Y_n)} = \abs{Y_n - c} \to 0$ in probability.
% 	Also, $\expect{g(X_n, c)} \to \expect{g(X, c)}$ for all bounded continuous maps $g \colon \mathbb R^d \times \mathbb R^d \to \mathbb R$, so $(X_n, c) \to (X, c)$ weakly.
% 	Hence, by (ii), $(X_n, Y_n) \to (X, c)$ weakly.
% \end{proof}
