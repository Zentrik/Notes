\section{Measurable Functions}
\subsection{Definition}
\begin{definition}[Measurable]
	Let $(E, \mathcal E), (G, \mathcal G)$ be measurable spaces.
	A function $f \colon E \to G$ is called \vocab{measurable} if $f^{-1}(A) \in \mathcal E \ \forall \; A \in \mathcal{G}$, where $f\inv(A)$ is the preimage of $A$ under $f$ i.e. $f\inv(A) = \qty{x \in E : f(x) \in A}$.
\end{definition}
% Informally, the preimage of a measurable set under a measurable function is measurable.

If $G = \mathbb R$ and $\mathcal G = \mathcal B$, we can just say that $f \colon (E, \mathcal E) \to G$ is measurable.
Moreover, if $E$ is a topological space and $\mathcal E = \mathcal B(E)$, we say $f$ is Borel measurable.

Note that preimages $f^{-1}$ commute with many set operations such as intersection, union, and complement.
This implies that $\qty{f^{-1}(A) : A \in \mathcal G}$ is a $\sigma$-algebra over $E$, and likewise, $\qty{A : f^{-1}(A) \in \mathcal E}$ is a $\sigma$-algebra over $G$.
Hence, if $\mathcal A$ is a collection of subsets s.t. $G \supset \sigma(\mathcal{A})$ then if $f^{-1}(A) \in \mathcal E$ for all $A \in \mathcal A$, the class $\qty{A : f^{-1} \in \mathcal E}$ is a $\sigma$-algebra that contains $\mathcal A$ and so $\sigma(\mathcal{A})$.
So $f$ is measurable.

If $f \colon (E, \mathcal E) \to \mathbb R$, the collection $\mathcal A = \qty{(-\infty,y] \colon y \in \mathbb R}$ generates $\mathcal B$ (Sheet 1).
Hence $f$ is Borel measurable iff $f^{-1}((-\infty,y]) = \qty{x \in E : f(x) \leq y} \in \mathcal E$ for all $y \in \mathbb R$.

If $E$ is a topological space and $\mathcal E = \mathcal B(E)$, then if $f \colon E \to \mathbb R$ is continuous, the preimages of open sets $B$ are open, and hence Borel sets.
The open sets in $\mathbb R$ generate the $\sigma$-algebra $\mathcal B$.
Hence, continuous functions to the real line are measurable.

\begin{example}
	Consider the indicator function $1_A$ of a set $A \subset E$. $1_A\inv(1) = A$ and $1_A\inv(0) = A^c$ hence measurable iff $A \in \mathcal E$.
\end{example}

\begin{example}
	The composition of measurable functions is measurable.
	Note that given a collection of maps $\qty{f_i \colon E \to (G,\mathcal G) : i \in I}$, we can make them all measurable by taking $\mathcal E$ to be a large enough $\sigma$-algebra, for instance $\sigma\qty(\qty{f_i^{-1}(A) : A \in \mathcal G, i \in I})$ called the $\sigma$-algebra generated by $\{f_i\}_{i \in I}$.
\end{example}

\begin{proposition}
	If $f_1, f_2, \dots$ are measurable $\mathbb{R}$-valued. Then $f_1 + f_2$, $f_1 f_2$, $\inf_n f_n$, $\sup_n f_n$, $\liminf f_n$, $\limsup f_n$ are all measurable.
\end{proposition}

\begin{proof}
	See Sheet 1.
\end{proof}

\subsection{Monotone class theorem}
\begin{theorem}[Monotone class theorem]
	Let $(E, \mathcal{E})$ be a measurable space and $\mathcal A$ be a $\pi$-system that generates the $\sigma$-algebra $\mathcal E$.
	Let $\mathcal V$ be a vector space of bounded maps from $E$ to $\mathbb R$ s.t.
	\begin{enumerate}
		\item $1_E \in \mathcal V$;
		\item $1_A \in \mathcal V$ for all $A \in \mathcal A$;
		\item if $f$ is bounded and $f_n \in \mathcal V$ are nonnegative functions that form an increasing sequence that converge pointwise to $f$ on $E$, then $f \in \mathcal V$.
	\end{enumerate}
	Then $\mathcal V$ contains all bounded measurable functions $f \colon E \to \mathbb R$.
\end{theorem}

\begin{proof}
	Define $\mathcal D = \qty{A \in \mathcal E : 1_A \in \mathcal V}$.
	Then $\mathcal{D}$ is a $d$-system as $1_E \in \mathcal{V}$ and for $A \subseteq B$, $1_{B \setminus A} = 1_B - 1_A \in \mathcal{V}$ as $\mathcal{V}$ a vector space so $B \setminus A \in \mathcal{D}$. \\
	If $A_n \in \mathcal D$ increases to $A$, we have $1_{A_n}$ increases pointwise to $1_A$, which lies in $\mathcal V$ by the (3.) so $A \in \mathcal{D}$.


	$\mathcal{D}$ contains $\mathcal A$ by (2.), as well as $E$ itself.
	So by Dynkin's lemma $\mathcal{D}$ contains $\sigma(\mathcal{A}) = \mathcal{E}$ so $\mathcal E = \mathcal D$ i.e. $1_A \in V \ \forall \; A \in \mathcal{E}$.

	Since $V$ a vector space it contains all finite linear combinations of indicators of measurable sets.
	Let $f \colon E \to \mathbb R$ be a bounded measurable function, which we will assume at first is nonnegative.
	We define
	\begin{align*}
		f_n(x) &= 2^{-n} \lfloor 2^n f(x) \rfloor \\
		&= 2^{-n} \sum_{j=0}^\infty 1_{A_{n, j}}(x) \\
		A_{n, j} &= \qty{2^n f(x) \in [j, j+1)} \\
		&= f\inv\left(\left[\frac{j}{2^n}, \frac{j+1}{2^n}\right)\right) \in \mathcal{E}.
	\end{align*}
	As $f$ is bounded we do not need an infinite sum but only a finite one.
	Then $f_n \leq f \leq f_n + 2^{-n}$.
	Hence $\abs{f_n - f} \leq 2^{-n} \to 0$ and $f_n \uparrow f$.

	So $0 \leq f_n \uparrow f, f_n \in \mathcal{V}$ and $f$ is bounded non-negative so $f \in \mathcal{V}$ by (3.).

	Finally, for any $f$ bounded and measurable, $f = f^+\footnote{$\max(f, 0)$} - f^-\footnote{$\max(-f, 0)$}$. $f^+, f^-$ are bounded, nonnegative and measurable, so in $\mathcal{V}$ and $\mathcal{V}$ a vector space thus $f \in \mathcal{V}$.
\end{proof}

\subsection{Image measures}

\begin{definition}[Image Measure]
	Let $f \colon (E,\mathcal E) \to (G,\mathcal G)$ be a measurable function and $\mu$ a measure on $(E, \mathcal E)$.
	Then the \vocab{image measure} $\nu = \mu \circ f^{-1}$ is obtained from assigning $\nu(A) = \mu(f^{-1}(A))$ for all $A \in \mathcal G$.
\end{definition}

\begin{remark}
	This is well defined as $f\inv(A) \in \mathcal{E}$ as $f$ measurable. $\nu$ is countably additive because the preimage satisfies set operations and $\mu$ countably additive (See Sheet 1).
\end{remark}

Starting from the Lebesgue measure, we can get all probability measures (in fact we can get all Radon measures) in this way.

% TODO: Define right-continuous
\begin{definition}[Right-Continuous]
	A function $f$ is \vocab{right-continuous} if $x_n \downarrow x \implies f(x_n) \to f(x)$.
\end{definition}

\begin{lemma}
	Let $g \colon \mathbb R \to \mathbb R$ be a non-constant, increasing, right-continuous function, and set $g(\pm\infty) = \lim_{z \to \pm \infty} g(z)$.
	On $I = (g(-\infty), g(+\infty))$ we define the \vocab{generalised inverse} $f : I \to \mathbb{R}$ by
	\[ f(x) = \inf \qty{y \in \mathbb R : g(y) \geq x}. \]
	Then $f$ is increasing, left-continuous, and $f(x) \leq y$ iff $x \leq g(y)$ for all $x \in I, y \in \mathbb R$.
\end{lemma}

\begin{remark}
	$f$ and $g$ form a Galois connection.
\end{remark}

\begin{proof}
	Fix $x \in I$. \\
	Let $J_x = \qty{y \in \mathbb R : g(y) \geq x}$.
	Since $x > g(-\infty)$, $J_x$ is nonempty and bounded below.
	Hence $f(x)$ is a well-defined real number. \\
	If $y \in J_x$, then $y' \geq y$ implies $y' \in J_x$ since $g$ is increasing.
	Since $g$ is right-continuous, if $y_n \downarrow y$, and all $y_n \in J_x$, then $g(y) = \lim_n g(y_n) \geq x$ so $y \in J_x$. \\
	So $J_x = [f(x), \infty)$.
	Hence $f(x) \leq y \iff x \leq g(y)$ as required.

	If $x \leq x'$, we have $J_x \supseteq J_{x'}$ (as $y \in J_x \Longleftarrow y \in J_x'$), i.e. $[f(x), \infty) \supseteq [f(x'), \infty)$ so $f(x) \leq f(x')$. \\
	Similarly, if $x_n \uparrow x$, we have $J_x = \bigcap_n J_{x_n}$\footnote{As $y \in \bigcap_n J_{x_n} \iff g(y) \geq x_n \ \forall \; n \iff g(y) \geq x \iff y \in J_x$.} so $[f(x), \infty) = \bigcap_n [f(x_n), \infty)$ so $f(x_n) \to f(x)$ as $x_n \to x$.
\end{proof}

\begin{theorem}
	Let $g \colon \mathbb R \to \mathbb R$ as in the previous lemma.
	Then $\exists$ a unique Radon measure $\mu_g$ on $\mathbb R$ such that $\mu_g((a,b]) = g(b) - g(a)$ for all $a < b$.
	Further, all Radon measures on $\mathbb{R}$ can be obtained in this way.
\end{theorem}

\begin{proof}
	Define $I, f$ as in the previous lemma and $\lambda$ the Lebesgue measure on $I$.

	$f$ is Borel measurable since $f^{-1}((-\infty,z]) = \qty{x \in I \colon f(x) \leq z} = \qty{x \in I \colon x \leq g(z)} = (-g(\infty),g(z)] \in \mathcal{B}$. As $\qty{(-\infty,z] : z \in \mathbb{R}}$ generate $\mathcal{B}$, $f$ measurable.

	Therefore, the image measure $\mu_g = \lambda \circ f^{-1}$ exists on $\mathcal{B}$.
	Then for any $-\infty < a < b < \infty$, we have
	\begin{align*}
		\mu_g((a,b]) &= \lambda \left( f^{-1}\left( (a,b] \right) \right) \\
		&= \lambda \left( \qty{x \colon a < f(x) \leq f(b)} \right) \\
		&= \lambda \left( \qty{x \colon g(a) < x \leq g(b)} \right) \\
		&= g(b) - g(a)
	\end{align*}
	By the \nameref{thm:uni} for $\sigma$-finite measures, $\mu_g$ is uniquely defined.
	% Since $g$ maps into $\mathbb R$, $g(b) - g(a) \in \mathbb R$ so any compact set has finite measure as it is a subset of a closed bounded interval.

	Conversely, let $\nu$ be a Radon measure on $\mathbb R$.
	Define $g : \mathbb{R} \to \mathbb{R}$ as
	\[ g(y) = \begin{cases}
		\nu((0,y]) & \text{if } y \geq 0 \\
		-\nu((y,0]) & \text{if } y < 0
	\end{cases} \]
	$\nu$ Radon tells us that $g$ is finite.
	Easy to check $g$ is right-continuous\footnote{For $y_n \downarrow y$ where $y \geq 0$, $(0, y_n] \downarrow (0, y]$ and then $\nu((0, y_n]) \downarrow \nu((0, y])$ by countably additivity. Similarly for $y < 0$.}.
	This is an increasing function in $y$, since $\nu$ is a measure.
	Finally, $\nu((a,b]) = g(b) - g(a)$ which can be seen by case analysis and additivity of the measure $\nu$.
	By uniqueness as before, this characterises $\nu$ in its entirety.
\end{proof}

\begin{remark}
	Such image measures $\mu_g$ are called \vocab{Lebesgue--Stieltjes measures} associated with $g$, where $g$ is the \vocab{Stieltjes distribution}.
\end{remark}

\begin{example}
	Fix $x \in \mathbb{R}$ and take $g = 1_{[x,\infty)}$.
	Then $\mu_g = \delta_x$ the \emph{dirac measure at $x$} defined for all $A \in \mathcal{B}$ by
	\[ \delta_x(A) = \begin{cases}
		1 & \text{if } x \in A \\
		0 & \text{otherwise}
	\end{cases} \]
\end{example}

\subsection{Random variables}

\begin{definition}[Random Variable]
	Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space, and $(E, \mathcal E)$ be a measurable space.
	If $X : \Omega \to E$ a measurable function then $X$ is a \vocab{random variable} in $E$.
\end{definition}
When $E = \mathbb R$ or $\mathbb R^d$ with the Borel $\sigma$-algebra, we simply call $X$ a random variable or random vector.

\begin{example}
	$X$ models a ``random'' outcome of an experiment, e.g. when tossing a coin $\Omega = \{H, T\}, X = \text{\# heads} : \Omega \to \{0, 1\}$.
\end{example}

\begin{definition}[Distribution]
	The \vocab{law} or \vocab{distribution} $\mu_X$ of a random variable $X$ is given by the image measure $\mu_X = \mathbb P \circ X^{-1}$.
	It is a measure on $(E, \mathcal{E})$.

	When $(E, \mathcal{E}) = (\mathbb{R}, \mathcal{B})$, $\mu_X$ is uniquely determined by its values on any $\pi$-system, we shall take $\qty{(-\infty, x] : x \in \mathbb{R}}$ and
	\begin{align*}
		F_X(z) = \mu_X((-\infty, z]) = \mathbb P(X^{-1}(-\infty,z]) = \prob{\qty{\omega \in \Omega : X(\omega) \leq z}} = \prob{X \leq z}
	\end{align*}
	The function $F_x$ is called the \vocab{distribution function} of $X$, because it uniquely determines the distribution of $X$.
\end{definition}

Using the properties of measures, we can show that any distribution function satisfies:

\begin{enumerate}
	\item $F_X$ is increasing;
	\item $F_X$ is right-continuous\footnote{$x_n \downarrow x \implies (-\infty, x_n] \downarrow (-\infty, x]$ hence by countable additivity of $\mathbb{P} \circ X\inv$.};
	\item $F_X(-\infty) = \lim_{z \to -\infty} F_X(z) = \mu_X(\varnothing) = 0$;
	\item $F_X(\infty) = \lim_{z \to \infty} F_X(z) = \mu_X(\mathbb R) = \prob{\Omega} = 1$.
\end{enumerate}

\begin{proposition}
	% Given any function $F_X : \mathbb{R} \to [0, 1]$ satisfying each property, we can obtain a random variable $X$ on $(\Omega, \mathcal F, \mathbb P) = ((0,1), \mathcal B((0,1)), \mu)$ by $X(\omega) = \inf\qty{x : \omega \leq f(x)}$, and then $F_X$ is the distribution function of $X$.
	Given any function $F$ satisfying the previous properties, $\exists \;$ a random variable $X$ s.t. $F = F_X$.
\end{proposition}

\begin{proof}
	Let $\Omega = (0, 1)$, $\mathcal{F} = \mathcal{B}(0, 1)$, $\mathbb{P}$ the Lebesgue measure $\eval{\lambda}_{(0, 1)}$. \\
	Let $F$ be any function satisfying the properties, then $F$ is increasing and right continuous so we can define the generalised inverse
	\begin{align*}
		X(\omega) = \inf\qty{x : \omega \leq F(x)} : (0, 1) \to \mathbb{R}
	\end{align*}
	Hence $X$ is a measurable function and thus a random variable.
	\begin{align*}
		F_X(x) &= \mathbb{P}(X \leq x) = \mathbb{P}(\qty{\omega \in \Omega : X(\omega) \leq x}) = \mathbb{P}(\qty{\omega \in \Omega : \omega \leq F(x)}) \\
		&= \mathbb{P}(\qty{\omega \in (0, 1) : \omega \leq F(x)}) \\
		&= \mathbb{P}((0, F(x)]) \\
		&= F(x) - 0
	\end{align*}
\end{proof}

\begin{remark}
	This is similar to what we saw in IB Probability, if we have $F$ then r.v. $F\inv(U)$ where $U \sim U(0, 1)$ has the distribution function $F$, where $F\inv$ is the generalised inverse.
\end{remark}

\begin{definition}[Independent]
	Consider a countable collection $(X_i \colon (\Omega, \mathcal F, \mathbb P) \to (E, \mathcal E))$ for $i \in I$.
	This collection of random variables is called \vocab{independent} if the $\sigma$-algebras $\sigma\qty(X_i)$ are independent, recall $\sigma(X_i) $ is generated by $\qty{X_i\inv(A) \colon A \in \mathcal E}$, the smallest $\sigma$-algebra s.t. $X_i$ measurable.
\end{definition}

For $(E, \mathcal E) = (\mathbb R, \mathcal B)$ we show on an Sheet 1 that this is equivalent to the condition
\[ \prob{X_1 \leq x_1, \dots, X_n \leq x_n} = \prob{X_1 \leq x_1} \dots \prob{X_n \leq x_n} \]
for all finite subsets $\qty{X_1, \dots, X_n}$ of the $X_i$.

\subsection{Constructing independent random variables}

\begin{question}
	Given a distribution function $F$, we know $\exists$ a r.v. $X$ corresponding to it.
	But given an infinite sequence of distribution functions $F_1, F_2, \dots$ does $\exists$ independent r.v. $(X_1, X_2, \dots)$ corresponding to them?
\end{question}

Let $(\Omega, \mathcal F, \mathbb P) = ((0,1), \mathcal B(0, 1), \eval{\lambda}_{(0,1)})$.
We start with Bernoulli random variables.

Any $\omega \in (0,1)$ has a binary representation given by $(\omega_i) \in \qty{0,1}^{\mathbb N}$ where $\omega = \sum_{i=1}^{\infty} 2^{-i} \omega_i$, which is unique if we exclude infinitely long tails of zeroes from the binary representation (same reasoning as $1.00000\ldots = 0.99999\dots$).

\begin{definition}[$n$th Rademacher function]
	The \vocab{$n$th Rademacher function} $R_n : \Omega \to {0, 1}$ is given by $R_n(\omega) = \omega_n$, it extracts the $n$th bit from the binary expansion.
\end{definition}

Observe that $R_1 = 1_{(1/2, 1]}$, $R_2 = 1_{(1/4, 1/2]} + 1_{(3/4, 1]}$ and so on.
Since each $R_n$ can be given as the sum of finite ($2^{n-1}$) indicator functions on measurable sets, they are measurable functions and are hence random variables.

\begin{claim}
	$R_i$ are iid $\operatorname{Ber}(\frac{1}{2})$.
\end{claim}

\begin{proof}
	$\prob{R_n = 1} = \frac{1}{2} = \prob{R_n = 0}$ can be checked by induction.

	We now show they are independent.
	For a finite set $(x_i)_{i=1}^n$, by considering the size of the intervals that $\omega$ can lie in,
	\begin{align*}
		\prob{R_1 = x_1, \dots, R_n = x_n} = 2^{-n} = \prob{R_1 = x_1} \dots \prob{R_n = x_n}
	\end{align*}
\end{proof}

Therefore, the $R_n$ are all independent, so countable sequences of independent random variables indeed exist. \\
The next step is to construct a sequence of iid r.v.s on $\operatorname{U}(0, 1)$.

Now, take a bijection $m \colon \mathbb N^2 \to \mathbb N$ and define $Y_{k,n} = R_{m(k, n)}$, the Rademacher functions.
We now define $Y_n = \sum_{k=1}^\infty 2^{-k} Y_{k,n}$\footnote{This converges for all $\omega \in \Omega$ since $\abs{Y_{k,n}} \leq 1$.}.

\begin{claim}
	$Y_n$ are iid $\operatorname{U}(0, 1)$, i.e. $\mu_{Y_n} = \eval{\lambda}_{(0, 1)}$ and $Y_n$ independent.
\end{claim}

\begin{lemma}
	Any measurable functions of independent random variables are independent.
\end{lemma}

\begin{proof}
	They are independent because the $Y_i$ are measurable functions of independent random variables, e.g. $Y_1$ is a measurable function of $Y_{1,1}, Y_{2, 1}, \dots$; $Y_2$ of $Y_{1, 2}, Y_{2, 2}, \dots$

	% The distribution of $Y_n$ is identified on the $\pi$-system of intervals $(\frac{i}{2^m}, \frac{i+1}{2^m}], i = 0, 1, \dots, 2^{m-1}$ for $m \in \mathbb{N}$.

	The $\pi$-system of intervals $\left( \frac{i}{2^m}, \frac{i+1}{2^m} \right]$ for $i = 0, \dots, 2^m - 1$ for $m \in \mathbb{N}$ generates $\mathcal B(0, 1)$ as $\mathbb{Q}$ dense in $\mathbb{R}$.
	So by \cref{thm:uni} the distribution of $Y_n$ is identified on the intervals.
	\begin{align*}
		\prob{Y_n \in \left( \frac{i}{2^m}, \frac{i+1}{2^m} \right]} &= \prob{\frac{i}{2^m} < \sum_{k=1}^\infty 2^{-k} Y_{k,n} \leq \frac{i+1}{2^n}}\footnote{This specifies the first $m$ digits in the binary expansion of $Y_n$.} \\
		&= \mathbb{P}(Y_{1,n} = y_1, \dots, Y_{m,n} = y_m) \text{ where } \frac{i}{2^m} = 0.y_1 y_2 \dots y_m \\
		&= \prod_{i=1}^m \mathbb{P}(Y_{m, n} = y_m) \text{ by independence.} \\
		&= 2^{-m} = \lambda\left( \frac{i}{2^m}, \frac{i+1}{2^m} \right]
	\end{align*}
	Hence $\mu_{Y_n} = \eval{\lambda}_{(0,1)}$ on the $\pi$-system and so on $\mathcal{B}(0, 1)$.
\end{proof}

As before, set $G_n(x) = F_n\inv(x)$ which is the generalised inverse.
Then $G_n$ are Borel functions, set $X_n = G_n(Y_n)$ for $n \in \mathbb{N}$, then as before $F_{X_n} = F_n$ and $X_n$ are independent as $Y_n$ are.

\subsection{Convergence of measurable functions}
Let $(E, \mathcal{E}, \mu)$ be a measure space. Let $A \in \mathcal{E}$ be defined by some property.

\begin{definition}[Almost everywhere]
	We say that a property defining a set $A \in \mathcal E$ holds \vocab{$\mu$-almost everywhere} if $\mu(A^c) = 0$.
\end{definition}

\begin{definition}[Almost surely]
	If $\mu$ is a $\mathbb P$- measure, we say a property holds \vocab{$\mathbb P$-almost surely} or \vocab{with probability one}, if $\mathbb{P}(A^c) = 0$, i.e. if $\mathbb P(A) = 1$.
\end{definition}

\begin{definition}[Convergence almost everywhere]
	If $f_n$ and $f$ are measurable functions on $(E,\mathcal E,\mu) \to (\mathbb{R}, \mathcal{B})$, we say \vocab{$f_n$ converges to $f$ $\mu$-almost everywhere} if $\mu(\qty{x \in E : f_n(x) \nrightarrow f(x)}) = 0$.

	For r.v.s, we say $X_n \to X$ \vocab{$\mathbb P$-almost surely} if $\mathbb{P}(\qty{\omega \in \Omega : X_n(\omega) \to X(\omega)}) = 1$.
\end{definition}

\begin{definition}[Convergence in Measure]
	We say \vocab{$f_n$ converges to $f$ in $\mu$-measure} if for all $\epsilon > 0$
	\begin{align*}
		\mu(\qty{x\in E : \abs{f_n(x) - f(x)} > \epsilon}) \to 0,
	\end{align*} as $n \to \infty$.

	We say $X_n \to X$ in $\mathbb{P}$-probability if $\forall \; \epsilon > 0$
	\begin{align*}
		\mathbb{P}(\abs{X_n - X} > \epsilon) \to 0
	\end{align*} as $n \to \infty$.
\end{definition}

\begin{theorem}
	Let $f_n \colon (E,\mathcal E,\mu) \to \mathbb R$ be measurable functions.
	\begin{enumerate}
		\item If $\mu(E) < \infty$, then $f_n \to 0$ a.e. $\implies f_n \to 0$ in measure;
		\item If $f_n \to 0$ in measure, $\exists$ subsequence $n_k$ s.t. $f_{n_k} \to 0$ a.e.
	\end{enumerate}
	% If $\mu(E) < \infty$, then $f_n \to 0$ a.e. $\implies$ $f_n \to 0$ in measure;
\end{theorem}

\begin{example}
	Let $f_n = 1_{(n, \infty)}$ and the Lebesgue measure, then $f_n \to 0$ a.e. but $\mu(|f_n| > \epsilon) = \infty \; \forall \; n$.
\end{example}

\begin{proof}
	Fix $\epsilon > 0$.
	Suppose $f_n \to 0$ a.e., then for every $n$,
	\begin{align*}
		\mu(E) \geq \mu(\abs{f_n} \leq \epsilon) \geq \mu\qty(\bigcap_{m \geq n} \qty{\abs{f_m} \leq \epsilon})
	\end{align*}
	Let $A_n = \bigcap_{m \geq n} \qty{\abs{f_m} \leq \epsilon}$ which is increasing to $\bigcup_n \bigcap_{m \geq n} \qty{\abs{f_m} \leq \epsilon}$.
	So by the countable additivity of $\mu$,
	\begin{align*}
		\mu\qty(\bigcap_{m \geq n} \qty{\abs{f_m} \leq \epsilon}) &\to \mu\qty(\bigcup_n \bigcap_{m \geq n} \qty{\abs{f_m} \leq \epsilon}) \\
		&= \mu\qty(\abs{f_n} \leq \epsilon \text{ eventually}) \\
		&\geq \mu(\abs{f_n} \to 0) \\
		&= \mu(E) \text{ as $f_n \to 0$ a.e. and $\mu$ finite.}
	\end{align*}
	Hence,
	\begin{align*}
		\liminf_{n \to \infty} \mu(\abs{f_n} \leq \epsilon) = \mu(E) \implies \limsup_{n \to \infty} \mu(\abs{f_n} > \epsilon) \leq 0 \implies \mu(\abs{f_n} > \epsilon) \to 0
	\end{align*}
\end{proof}

\begin{proof}
	Suppose $f_n \to 0$ in measure, choosing $\epsilon = \frac{1}{k}$ we have
	\begin{align*}
		\mu\qty(\abs{f_n} > \frac{1}{k}) \to 0.
	\end{align*}
	So we can choose $n_k$ s.t. $\mu\qty(\abs{f_n} > \frac{1}{k}) \leq \frac{1}{k^2}$.
	We can choose $n_{k+1}$ in the same way s.t. $n_{k+1} > n_k$.
	So we get a subsequence $n_k$ s.t. $\mu\qty(\abs{f_{n_k}} > \frac{1}{k}) < \frac{1}{k^2}$.
	Also $\sum_k \frac{1}{k^2} < \infty$, so $\sum_k \mu\qty(\abs{f_{n_k}} > \frac{1}{k}) < \infty$.
	So by the first Borel--Cantelli lemma, we have
	\begin{align*}
		\mu\qty(\underbracket{\abs{f_{n_k}} > \frac{1}{k} \text{ infinitely often}}_{f_{n_k} \not\to 0}) = 0
	\end{align*}
	so $f_{n_k} \to 0$ a.e.
\end{proof}

\begin{remark}
	The first statement is false if $\mu(E)$ is infinite: consider $f_n = 1_{(n,\infty)}$ on $(\mathbb R,\mathcal B,\mu)$, since $f_n \to 0$ almost everywhere but $\mu(f_n) = \infty$.

	The second statement is false if we do not restrict to subsequences: consider independent events $A_n$ such that $\prob{A_n} = \frac{1}{n}$, then $1_{A_n} \to 0$ in probability since $\prob{1_{A_n} > \epsilon} = \prob{A_n} = \frac{1}{n} \to 0$, but $\sum_n \prob{A_n} = \infty$, and by the second Borel--Cantelli lemma, $\prob{1_{A_n} > \epsilon \text{ infinitely often}} = 1$, so $1_{A_n} \nrightarrow 0$ almost surely.
\end{remark}

\begin{definition}[Convergence in Distribution]
	For $X$ and $X_n$ a sequence of r.v.s, we say $X_n \overset{d}{\to} X$\footnote{$X_n$ converges to $X$ in distribution} if $F_{X_n}(t) \to F_X(t)$ as $n\to\infty$ for all $t \in \mathbb{R}$ which are continuity points of $F_X$.
\end{definition}

\begin{remark}
	This definition does not require $X_n$ to be defined on the same probability space.
\end{remark}

\begin{remark}
	If $X_n \to X$ in probability, then $X_n \overset{d}{\to} X$, see Sheet 2 for proof.
\end{remark}

\begin{example}
	Let $(X_n)_{n \in \mathbb N}$ be iid $\operatorname{Exp}(1)$, i.e. $\prob{X_n > x} = e^{-x}$ for $x \geq 0$.

	\begin{question}
		Find a deterministic fcn $g : \mathbb{N} \to \mathbb{R}$ s.t. a.s. $\limsup \frac{X_n}{g(n)} = 1$.
	\end{question}

	Define $A_n = \qty{X_n \geq \alpha \log n}$ where $\alpha > 0$, so $\prob{A_n} = n^{-\alpha}$, and in particular, $\sum_n \prob{A_n} < \infty$ if and only if $\alpha > 1$.
	By the Borel--Cantelli lemmas, we have for all $\epsilon > 0$,
	\[ \prob{\frac{X_n}{\log n} \geq 1 \text{ infinitely often}} = 1;\quad \prob{\frac{X_n}{\log n} \geq 1 + \epsilon \text{ infinitely often}} = 0 \]
	In other words, $\mathbb{P}(\limsup_n \frac{X_n}{\log n} = 1) = 1$.
\end{example}

\subsection{Kolmogorov's zero-one law}
Let $(X_n)_{n \in \mathbb N}$ be a sequence of r.v.s.
We can define $\mathcal T_n = \sigma(X_{n+1}, X_{n+2}, \dots)\footnote{The smallest $\sigma$-algebra s.t. $X_{n+1}, \dots$ are measurable.}$.
Let $\mathcal T = \bigcap_{n \in \mathbb N} \mathcal T_n$ be the \vocab{tail $\sigma$-algebra}, which contains all events in $\mathcal F$ that depend only on the `limiting behaviour' of $(X_n)$.

\begin{theorem}[Kolmogorov 0-1 Law]
	Let $(X_n)_{n \in \mathbb N}$ be a sequence of independent r.v.s.
	Let $A \in \mathcal T$ be an event in the tail $\sigma$-algebra.
	Then $\prob{A} = 1$ or $\prob{A} = 0$. \\
	If $Y \colon (\Omega,\mathcal T) \to (\mathbb R,\mathcal B)$ is measurable, it is constant almost surely.
\end{theorem}

\begin{proof}
	Define $\mathcal F_n = \sigma(X_1, \dots, X_n)$ to be the $\sigma$-algebra generated by the first $n$ elements of $(X_n)$.
	This is also generated by the $\pi$-system of sets $A = \qty(X_1 \leq x_1, \dots, X_n \leq x_n)$ for any $x_i \in \mathbb R$.
	Note that the $\pi$-system of sets $B = \qty(X_{n+1} \leq x_{n+1}, \dots, X_{n+k} \leq x_{n+k})$, for arbitrary $k \in \mathbb N$ and $x_i \in \mathbb R$, generates $\mathcal T_n$.
	By independence of the sequence, we see that $\prob{A \cap B} = \prob{A} \prob{B}$ for all such sets $A, B$, and so the $\sigma$-algebras $\mathcal T_n, \mathcal F_n$ generated by these $\pi$-systems are independent.

	Let $\mathcal F_\infty = \sigma(X_1, X_2, \dots)$.
	Then, $\bigcup_n \mathcal F_n$ is a $\pi$-system that generates $\mathcal F_\infty$.
	If $A \in \bigcup_n \mathcal F_n$, we have $A \in \mathcal F_n$ for some $n$, so there exists $\overline n$ such that $B \in \mathcal T_{\overline n}$ is independent of $A$.
	In particular, $B \in \bigcap_n \mathcal T_n = \mathcal T$.
	By uniqueness, $\mathcal F_\infty$ is independent of $\mathcal T$.

	Since $\mathcal T \subseteq \mathcal F_\infty$, if $A \in \mathcal T$, $A$ is independent from $A$.
	So $\prob{A} = \prob{A \cap A} = \prob{A}\prob{A}$, so $\prob{A}^2 - \prob{A} = 0$ as required.

	Finally, if $Y \colon (\Omega,\mathcal T) \to (\mathbb R,\mathcal B)$, the preimages of $\qty{Y \leq y}$ lie in $\mathcal T$, which give probability one or zero.
	Let $c = \inf\qty{y : F_Y(y) = 1}$, so $Y = c$ almost surely.
\end{proof}
