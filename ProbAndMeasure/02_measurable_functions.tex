\section{Measurable Functions}
\subsection{Definition}
\begin{definition}[Measurable]
	Let $(E, \mathcal E), (G, \mathcal G)$ be measurable spaces.
	A function $f \colon E \to G$ is called \vocab{measurable} if $f^{-1}(A) \in \mathcal E \ \forall \; A \in \mathcal{G}$, where $f\inv(A)$ is the preimage of $A$ under $f$ i.e. $f\inv(A) = \qty{x \in E : f(x) \in A}$.
\end{definition}
% Informally, the preimage of a measurable set under a measurable function is measurable.

If $G = \mathbb R$ and $\mathcal G = \mathcal B$, we can just say that $f \colon (E, \mathcal E) \to G$ is measurable.
Moreover, if $E$ is a topological space and $\mathcal E = \mathcal B(E)$, we say $f$ is Borel measurable.

Note that preimages $f^{-1}$ commute with many set operations such as intersection, union, and complement.
This implies that $\qty{f^{-1}(A) \mid A \in \mathcal G}$ is a $\sigma$-algebra over $E$, and likewise, $\qty{A : f^{-1}(A) \in \mathcal E}$ is a $\sigma$-algebra over $G$.
Hence, if $\mathcal A$ is a collection of subsets s.t. $G \supset \sigma(\mathcal{A})$ then if $f^{-1}(A) \in \mathcal E$ for all $A \in \mathcal A$, the class $\qty{A : f^{-1} \in \mathcal E}$ is a $\sigma$-algebra that contains $\mathcal A$ and so $\sigma(\mathcal{A})$.
So $f$ is measurable.

If $f \colon (E, \mathcal E) \to \mathbb R$, the collection $\mathcal A = \qty{(-\infty,y] \colon y \in \mathbb R}$ generates $\mathcal B$ (Sheet 1).
Hence $f$ is Borel measurable iff $f^{-1}((-\infty,y]) = \qty{x \in E : f(x) \leq y} \in \mathcal E$ for all $y \in \mathbb R$.

If $E$ is a topological space and $\mathcal E = \mathcal B(E)$, then if $f \colon E \to \mathbb R$ is continuous, the preimages of open sets $B$ are open, and hence Borel sets.
The open sets in $\mathbb R$ generate the $\sigma$-algebra $\mathcal B$.
Hence, continuous functions to the real line are measurable.

\begin{example}
	Consider the indicator function $1_A$ of a set $A \subset E$. $1_A\inv(1) = A$ and $1_A\inv(0) = A^c$ hence measurable iff $A \in \mathcal E$.
\end{example}

\begin{example}
	The composition of measurable functions is measurable.
	Note that given a collection of maps $\qty{f_i \colon E \to (G,\mathcal G) \mid i \in I}$, we can make them all measurable by taking $\mathcal E$ to be a large enough $\sigma$-algebra, for instance $\sigma\qty(\qty{f_i^{-1}(A) : A \in \mathcal G, i \in I})$ called the $\sigma$-algebra generated by $\{f_i\}_{i \in I}$.
\end{example}

\begin{proposition}
	If $f_1, f_2, \dots$ are measurable $\mathbb{R}$-valued. Then $f_1 + f_2$, $f_1 f_2$, $\inf_n f_n$, $\sup_n f_n$, $\liminf f_n$, $\limsup f_n$ are all measurable.
\end{proposition}

\begin{proof}
	See Sheet 1.
\end{proof}

\subsection{Monotone class theorem}
\begin{theorem}[Monotone class theorem]
	Let $(E, \mathcal{E})$ be a measurable space and $\mathcal A$ be a $\pi$-system that generates the $\sigma$-algebra $\mathcal E$.
	Let $\mathcal V$ be a vector space of bounded maps from $E$ to $\mathbb R$ s.t.
	\begin{enumerate}
		\item $1_E \in \mathcal V$;
		\item $1_A \in \mathcal V$ for all $A \in \mathcal A$;
		\item if $f$ is bounded and $f_n \in \mathcal V$ are nonnegative functions that form an increasing sequence that converge pointwise to $f$ on $E$, then $f \in \mathcal V$.
	\end{enumerate}
	Then $\mathcal V$ contains all bounded measurable functions $f \colon E \to \mathbb R$.
\end{theorem}

\begin{proof}
	Define $\mathcal D = \qty{A \in \mathcal E : 1_A \in \mathcal V}$.
	Then $\mathcal{D}$ is a $d$-system as $1_E \in \mathcal{V}$ and for $A \subseteq B$, $1_{B \setminus A} = 1_B - 1_A \in \mathcal{V}$ as $\mathcal{V}$ a vector space so $B \setminus A \in \mathcal{D}$. \\
	If $A_n \in \mathcal D$ increases to $A$, we have $1_{A_n}$ increases pointwise to $1_A$, which lies in $\mathcal V$ by the (3.) so $A \in \mathcal{D}$.


	$\mathcal{D}$ contains $\mathcal A$ by (2.), as well as $E$ itself.
	So by Dynkin's lemma $\mathcal{D}$ contains $\sigma(\mathcal{A}) = \mathcal{E}$ so $\mathcal E = \mathcal D$ i.e. $1_A \in V \ \forall \; A \in \mathcal{E}$.

	Since $V$ a vector space it contains all finite linear combinations of indicators of measurable sets.
	Let $f \colon E \to \mathbb R$ be a bounded measurable function, which we will assume at first is nonnegative.
	We define
	\begin{align*}
		f_n(x) &= 2^{-n} \lfloor 2^n f(x) \rfloor \\
		&= 2^{-n} \sum_{j=0}^\infty 1_{A_{n, j}}(x) \\
		A_{n, j} &= \qty{2^n f(x) \in [j, j+1)} \\
		&= f\inv\left(\left[\frac{j}{2^n}, \frac{j+1}{2^n}\right)\right) \in \mathcal{E}.
	\end{align*}
	As $f$ is bounded we do not need an infinite sum but only a finite one.
	Then $f_n \leq f \leq f_n + 2^{-n}$.
	Hence $\abs{f_n - f} \leq 2^{-n} \to 0$ and $f_n \uparrow f$.

	So $0 \leq f_n \uparrow f, f_n \in \mathcal{V}$ and $f$ is bounded non-negative so $f \in \mathcal{V}$ by (3.).

	Finally, for any $f$ bounded and measurable, $f = f^+\footnote{$\max(f, 0)$} - f^-\footnote{$\max(-f, 0)$}$. $f^+, f^-$ are bounded, nonnegative and measurable, so in $\mathcal{V}$ and $\mathcal{V}$ a vector space thus $f \in \mathcal{V}$.
\end{proof}

\subsection{Image measures}

\begin{definition}[Image Measure]
	Let $f \colon (E,\mathcal E) \to (G,\mathcal G)$ be a measurable function and $\mu$ a measure on $(E, \mathcal E)$.
	Then the \vocab{image measure} $\nu = \mu \circ f^{-1}$ is obtained from assigning $\nu(A) = \mu(f^{-1}(A))$ for all $A \in \mathcal G$.
\end{definition}

\begin{remark}
	This is well defined as $f\inv(A) \in \mathcal{E}$ as $f$ measurable. $\nu$ is countably additive because the preimage satisfies set operations and $\mu$ countably additive (See Sheet 1).
\end{remark}

Starting from the Lebesgue measure, we can get all probability measures (in fact we can get all Radon measures) in this way.

% TODO: Define right-continuous
\begin{definition}[Right-Continuous]
	A function $f$ is \vocab{right-continuous} if $x_n \downarrow x \implies f(x_n) \to f(x)$.
\end{definition}

\begin{lemma}
	Let $g \colon \mathbb R \to \mathbb R$ be a non-constant, increasing, right-continuous function, and set $g(\pm\infty) = \lim_{z \to \pm \infty} g(z)$.
	On $I = (g(-\infty), g(+\infty))$ we define the \vocab{generalised inverse} $f : I \to \mathbb{R}$ by
	\[ f(x) = \inf \qty{y \in \mathbb R : g(y) \geq x}. \]
	Then $f$ is increasing, left-continuous, and $f(x) \leq y$ iff $x \leq g(y)$ for all $x \in I, y \in \mathbb R$.
\end{lemma}

\begin{remark}
	$f$ and $g$ form a Galois connection.
\end{remark}

\begin{proof}
	Fix $x \in I$. \\
	Let $J_x = \qty{y \in \mathbb R : g(y) \geq x}$.
	Since $x > g(-\infty)$, $J_x$ is nonempty and bounded below.
	Hence $f(x)$ is a well-defined real number. \\
	If $y \in J_x$, then $y' \geq y$ implies $y' \in J_x$ since $g$ is increasing.
	Since $g$ is right-continuous, if $y_n \downarrow y$, and all $y_n \in J_x$, then $g(y) = \lim_n g(y_n) \geq x$ so $y \in J_x$. \\
	So $J_x = [f(x), \infty)$.
	Hence $f(x) \leq y \iff x \leq g(y)$ as required.

	If $x \leq x'$, we have $J_x \supseteq J_{x'}$ (as $y \in J_x \Longleftarrow y \in J_x'$), i.e. $[f(x), \infty) \supseteq [f(x'), \infty)$ so $f(x) \leq f(x')$. \\
	Similarly, if $x_n \uparrow x$, we have $J_x = \bigcap_n J_{x_n}$\footnote{As $y \in \bigcap_n J_{x_n} \iff g(y) \geq x_n \ \forall \; n \iff g(y) \geq x \iff y \in J_x$.} so $[f(x), \infty) = \bigcap_n [f(x_n), \infty)$ so $f(x_n) \to f(x)$ as $x_n \to x$.
\end{proof}

\begin{theorem}
	Let $g \colon \mathbb R \to \mathbb R$ as in the previous lemma.
	Then $\exists$ a unique Radon measure $\mu_g$ on $\mathbb R$ such that $\mu_g((a,b]) = g(b) - g(a)$ for all $a < b$.
	Further, all Radon measures on $\mathbb{R}$ can be obtained in this way.
\end{theorem}

\begin{proof}
	Define $I, f$ as in the previous lemma and $\lambda$ the Lebesgue measure on $I$.

	$f$ is Borel measurable since $f^{-1}((-\infty,z]) = \qty{x \in I \colon f(x) \leq z} = \qty{x \in I \colon x \leq g(z)} = (-g(\infty),g(z)] \in \mathcal{B}$. As $\qty{(-\infty,z] : z \in \mathbb{R}}$ generate $\mathcal{B}$, $f$ measurable.

	Therefore, the image measure $\mu_g = \lambda \circ f^{-1}$ exists on $\mathcal{B}$.
	Then for any $-\infty < a < b < \infty$, we have
	\begin{align*}
		\mu_g((a,b]) &= \lambda \left( f^{-1}\left( (a,b] \right) \right) \\
		&= \lambda \left( \qty{x \colon a < f(x) \leq f(b)} \right) \\
		&= \lambda \left( \qty{x \colon g(a) < x \leq g(b)} \right) \\
		&= g(b) - g(a)
	\end{align*}
	By the \nameref{thm:uni} for $\sigma$-finite measures, $\mu_g$ is uniquely defined.
	% Since $g$ maps into $\mathbb R$, $g(b) - g(a) \in \mathbb R$ so any compact set has finite measure as it is a subset of a closed bounded interval.

	Conversely, let $\nu$ be a Radon measure on $\mathbb R$.
	Define $g : \mathbb{R} \to \mathbb{R}$ as
	\[ g(y) = \begin{cases}
		\nu((0,y]) & \text{if } y \geq 0 \\
		-\nu((y,0]) & \text{if } y < 0
	\end{cases} \]
	$\nu$ Radon tells us that $g$ is finite.
	Easy to check $g$ is right-continuous\footnote{For $y_n \downarrow y$ where $y \geq 0$, $(0, y_n] \downarrow (0, y]$ and then $\nu((0, y_n]) \downarrow \nu((0, y])$ by countably additivity. Similarly for $y < 0$.}.
	This is an increasing function in $y$, since $\nu$ is a measure.
	Finally, $\nu((a,b]) = g(b) - g(a)$ which can be seen by case analysis and additivity of the measure $\nu$.
	By uniqueness as before, this characterises $\nu$ in its entirety.
\end{proof}

\begin{remark}
	Such image measures $\mu_g$ are called \vocab{Lebesgue--Stieltjes measures} associated with $g$, where $g$ is the \vocab{Stieltjes distribution}.
\end{remark}

\begin{example}
	Fix $x \in \mathbb{R}$ and take $g = 1_{[x,\infty)}$.
	Then $\mu_g = \delta_x$ the \emph{dirac measure at $x$} defined for all $A \in \mathcal{B}$ by
	\[ \delta_x(A) = \begin{cases}
		1 & \text{if } x \in A \\
		0 & \text{otherwise}
	\end{cases} \]
\end{example}

\subsection{Random variables}

\begin{definition}[Random Variable]
	Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space, and $(E, \mathcal E)$ be a measurable space.
	If $X : \Omega \to E$ a measurable function then $X$ is a \vocab{random variable} in $E$.
\end{definition}
When $E = \mathbb R$ or $\mathbb R^d$ with the Borel $\sigma$-algebra, we simply call $X$ a random variable or random vector.

\begin{example}
	$X$ models a ``random'' outcome of an experiment, e.g. when tossing a coin $\Omega = \{H, T\}, X = \text{\# heads} : \Omega \to \{0, 1\}$.
\end{example}

\begin{definition}[Distribution]
	The \vocab{law} or \vocab{distribution} $\mu_X$ of a random variable $X$ is given by the image measure $\mu_X = \mathbb P \circ X^{-1}$.
	It is a measure on $(E, \mathcal{E})$.

	When $(E, \mathcal{E}) = (\mathbb{R}, \mathcal{B})$, $\mu_X$ is uniquely determined by its values on any $\pi$-system, we shall take $\qty{(-\infty, x] : x \in \mathbb{R}}$ and
	\begin{align*}
		F_X(z) = \mu_X((-\infty, z]) = \mathbb P(X^{-1}(-\infty,z]) = \prob{\qty{\omega \in \Omega : X(\omega) \leq z}} = \prob{X \leq z}
	\end{align*}
	The function $F_x$ is called the \vocab{distribution function} of $X$, because it uniquely determines the distribution of $X$.
\end{definition}

Using the properties of measures, we can show that any distribution function satisfies:

\begin{enumerate}
	\item $F_X$ is increasing;
	\item $F_X$ is right-continuous\footnote{$x_n \downarrow x \implies (-\infty, x_n] \downarrow (-\infty, x]$ hence by countable additivity of $\mathbb{P} \circ X\inv$.};
	\item $F_X(-\infty) = \lim_{z \to -\infty} F_X(z) = \mu_X(\varnothing) = 0$;
	\item $F_X(\infty) = \lim_{z \to \infty} F_X(z) = \mu_X(\mathbb R) = \prob{\Omega} = 1$.
\end{enumerate}

Given any function $F_X : \mathbb{R} \to [0, 1]$ satisfying each property, we can obtain a random variable $X$ on $(\Omega, \mathcal F, \mathbb P) = ((0,1), \mathcal B((0,1)), \mu)$ by $X(\omega) = \inf\qty{x \mid \omega \leq f(x)}$, and then $F_X$ is the distribution function of $X$.

\begin{definition}
	Consider a countable collection $(X_i \colon (\Omega, \mathcal F, \mathbb P) \to (E, \mathcal E))$ for $i \in I$.
	This collection of random variables is called \emph{independent} if the $\sigma$-algebras $\sigma\qty(\qty{X_i^{-1}(A) \colon A \in \mathcal E})$ are independent.
\end{definition}
For $(E, \mathcal E) = (\mathbb R, \mathcal B)$ we show on an example sheet that this is equivalent to the condition
\[ \prob{X_1 \leq x_1, \dots, X_n \leq x_n} = \prob{X_1 \leq x_1} \dots \prob{X_n \leq x_n} \]
for all finite subsets $\qty{X_1, \dots, X_n}$ of the $X_i$.

\subsection{Constructing independent random variables}
We now construct an infinite sequence of independent random variables with prescribed distribution functions on $(\Omega, \mathcal F, \mathbb P) = ((0,1), \mathcal B, \mu)$ with $\mu$ the Lebesgue measure on $(0,1)$.
We start with Bernoulli random variables.

Any $\omega \in (0,1)$ has a binary representation given by $(\omega_i) \in \qty{0,1}^{\mathbb N}$, which is unique if we exclude infinitely long tails of zeroes from the binary representation.
We can then define the \emph{$n$th Rademacher function} $R_n(\omega) = \omega_n$ which extracts the $n$th bit from the binary expansion.
Since each $R_n$ can be given as the sum of $2^{n-1}$ indicator functions on measurable sets, they are measurable functions and are hence random variables.
Their distribution is given by $\prob{R_n = 1} = \frac{1}{2} = \prob{R_n = 0}$, so we have constructed Bernoulli random variables with parameter $\frac 12$.
We show they are independent.
For a finite set $(x_i)_{i=1}^n$,
\[ \prob{R_1 = x_1, \dots, R_n = x_n} = 2^{-n} = \prob{R_1 = x_1} \dots \prob{R_n = x_n} \]
Therefore, the $R_n$ are all independent, so countable sequences of independent random variables indeed exist.
Now, take a bijection $m \colon \mathbb N^2 \to \mathbb N$ and define $Y_{nk} = R_{m(n,k)}$, which are independent random variables.
We can now define $Y_n = \sum_k 2^{-k} Y_{nk}$.
This converges for all $\omega \in \Omega$ since $\abs{Y_{nk}} \leq 1$, and these are still independent.
We show the $Y_n$ are uniform random variables, by showing the distribution coincides with the uniform distribution on the $\pi$-system of intervals $\left( \frac{i}{2^m}, \frac{i+1}{2^{m+1}} \right]$ for $i = 0, \dots, 2^m - 1$, which generates $\mathcal B$.
\[ \prob{Y_n \in \left( \frac{i}{2^m}, \frac{i+1}{2^m} \right]} = \prob{\frac{i}{2^m} < \sum_k 2^{-k} Y_{nk} \leq \frac{i+1}{2^n}} = 2^{-m} = \mu\left( \frac{i}{2^m}, \frac{i+1}{2^{m+1}} \right] \]
Hence $\mu_{Y_n} = \eval{\mu}_{(0,1)}$ by the uniqueness theorem, and so we have constructed an infinite sequence of independent uniform random variables $Y_n$.
If $F_n$ are probability distribution functions, taking the generalised inverse, we see that the $F_n^{-1}(Y_n)$ are independent and have distribution function $F_n$.

\subsection{Convergence of measurable functions}
\begin{definition}
	We say that a property defining a set $A \in \mathcal E$ holds \emph{$\mu$-almost everywhere} if $\mu(A^c) = 0$ for a measure $\mu$ on $\mathcal E$.
	If $\mu = \mathbb P$, we say a property holds \emph{$\mathbb P$-almost surely} or \emph{with probability one}, if $\mathbb P(A) = 1$.
\end{definition}
\begin{definition}
	If $f_n$ and $f$ are measurable functions on $(E,\mathcal E,\mu)$, we say \emph{$f_n$ converges to $f$ $\mu$-almost everywhere} if $\mu(\qty{x \in E \mid f_n(x) \nrightarrow f(x)}) = 0$.
	We say \emph{$f_n$ converges to $f$ in $\mu$-measure} if for all $\varepsilon > 0$, $\mu(\qty{x\in E \mid \abs{f_n(x) - f(x)} > \varepsilon}) \to 0$ as $n \to \infty$.
	For random variables, we say $X_n \to X$ \emph{$\mathbb P$-almost surely} or \emph{in $\mathbb P$-probability}, written $X_n \to^p X$, respectively.
	If $X_n, X$ take values in $\mathbb R$, we say $X_n \to X$ \emph{in distribution}, written $X_n \to^d X$ if $\prob{X_n \leq x} \to \prob{X \leq x}$ at all points $x$ for which the limit $x \mapsto \prob{X \leq x}$ is continuous.
\end{definition}
We can show that $X_n \to^p X \implies X^n \to^d X$.
\begin{theorem}
	Let $f_n \colon (E,\mathcal E,\mu) \to \mathbb R$ be measurable functions.
	Then,
	\begin{enumerate}
		\item if $\mu(E) < \infty$, then $f_n \to 0$ almost everywhere implies that $f_n \to 0$ in measure;
		\item if $f_n \to 0$ in measure, $f_{n_k} \to 0$ almost everywhere on some subsequence.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let $\varepsilon > 0$.
	\[ \mu(\abs{f_n} < \varepsilon) \geq \mu\qty(\bigcap_{m \geq n} \qty{\abs{f_m} \leq \varepsilon}) \]
	The sequence $\qty(\bigcap_{m \geq n} \qty{\abs{f_m} \leq \varepsilon})_n$ increases to $\bigcup_n \bigcap_{m \geq n} \qty{\abs{f_m} \leq \varepsilon}$.
	So by countable additivity,
	\begin{align*}
		\mu\qty(\bigcap_{m \geq n} \qty{\abs{f_m} \leq \varepsilon}) &\to \mu\qty(\bigcup_n \bigcap_{m \geq n} \qty{\abs{f_m} \leq \varepsilon}) \\
		&= \mu\qty(\abs{f_n} \leq \varepsilon \text{ eventually}) \\
		&\geq \mu(\abs{f_n} \to 0) = \mu(E)
	\end{align*}
	Hence,
	\[ \liminf_n \mu(\abs{f_n} \leq \varepsilon) \geq \mu(E) \implies \limsup_n \mu(\abs{f_n} > \varepsilon) \leq 0 \implies \mu(\abs{f_n} > \varepsilon) \to 0 \]
	For the second part, by hypothesis, we have
	\[ \mu\qty(\abs{f_n} > \frac{1}{k}) < \varepsilon \]
	for sufficiently large $n$.
	So choosing $\varepsilon = \frac{1}{k^2}$, we see that along some subsequence $n_k$ we have
	\[ \mu\qty(\abs{f_{n_k}} > \frac{1}{k}) \leq \frac{1}{k^2} \]
	Hence,
	\[ \sum_k \mu\qty(\abs{f_{n_k}} > \frac{1}{n}) < \infty \]
	So by the first Borel--Cantelli lemma, we have
	\[ \mu\qty(\abs{f_{n_k}} > \frac{1}{k} \text{ infinitely often}) = 0 \]
	so $f_{n_k} \to 0$ almost everywhere.
\end{proof}
\begin{remark}
	Condition (i) is false if $\mu(E)$ is infinite: consider $f_n = 1_{(n,\infty)}$ on $(\mathbb R,\mathcal B,\mu)$, since $f_n \to 0$ almost everywhere but $\mu(f_n) = \infty$.
	Condition (ii) is false if we do not restrict to subsequences: consider independent events $A_n$ such that $\prob{A_n} = \frac{1}{n}$, then $1_{A_n} \to 0$ in probability since $\prob{1_{A_n} > \varepsilon} = \prob{A_n} = \frac{1}{n} \to 0$, but $\sum_n \prob{A_n} = \infty$, and by the second Borel--Cantelli lemma, $\prob{1_{A_n} > \varepsilon \text{ infinitely often}} = 1$, so $1_{A_n} \nrightarrow 0$ almost surely.
\end{remark}
\begin{example}
	Let $(X_n)_{n \in \mathbb N}$ be a sequence of independent exponential random variables distributed by $\prob{X_1 \leq x} = 1 - e^{-x}$ for $x \geq 0$.
	Define $A_n = \qty{X_n \geq \alpha \log n}$ where $\alpha > 0$, so $\prob{A_n} = n^{-\alpha}$, and in particular, $\sum_n \prob{A_n} < \infty$ if and only if $\alpha > 1$.
	By the Borel--Cantelli lemmas, we have for all $\varepsilon > 0$,
	\[ \prob{\frac{X_n}{\log n} \geq 1 \text{ infinitely often}} = 1;\quad \prob{\frac{X_n}{\log n} \geq 1 + \varepsilon \text{ infinitely often}} = 0 \]
	In other words, $\limsup_n \frac{X_n}{\log n} = 1$ almost surely.
\end{example}

\subsection{Kolmogorov's zero-one law}
Let $(X_n)_{n \in \mathbb N}$ be a sequence of random variables.
We can define $\mathcal T_n = \sigma(X_{n+1}, X_{n+2}, \dots)$.
Let $\mathcal T = \bigcap_{n \in \mathbb N} \mathcal T_n$ be the \emph{tail $\sigma$-algebra}, which contains all events in $\mathcal F$ that depend only on the limiting behaviour of $(X_n)$.
\begin{theorem}
	Let $(X_n)_{n \in \mathbb N}$ be a sequence of independent random variables.
	Let $A \in \mathcal T$ be an event in the tail $\sigma$-algebra.
	Then $\prob{A} = 1$ or $\prob{A} = 0$.
	If $Y \colon (\Omega,\mathcal T) \to (\mathbb R,\mathcal B)$ is measurable, it is constant almost surely.
\end{theorem}
\begin{proof}
	Define $\mathcal F_n = \sigma(X_1, \dots, X_n)$ to be the $\sigma$-algebra generated by the first $n$ elements of $(X_n)$.
	This is also generated by the $\pi$-system of sets $A = \qty(X_1 \leq x_1, \dots, X_n \leq x_n)$ for any $x_i \in \mathbb R$.
	Note that the $\pi$-system of sets $B = \qty(X_{n+1} \leq x_{n+1}, \dots, X_{n+k} \leq x_{n+k})$, for arbitrary $k \in \mathbb N$ and $x_i \in \mathbb R$, generates $\mathcal T_n$.
	By independence of the sequence, we see that $\prob{A \cap B} = \prob{A} \prob{B}$ for all such sets $A, B$, and so the $\sigma$-algebras $\mathcal T_n, \mathcal F_n$ generated by these $\pi$-systems are independent.

	Let $\mathcal F_\infty = \sigma(X_1, X_2, \dots)$.
	Then, $\bigcup_n \mathcal F_n$ is a $\pi$-system that generates $\mathcal F_\infty$.
	If $A \in \bigcup_n \mathcal F_n$, we have $A \in \mathcal F_n$ for some $n$, so there exists $\overline n$ such that $B \in \mathcal T_{\overline n}$ is independent of $A$.
	In particular, $B \in \bigcap_n \mathcal T_n = \mathcal T$.
	By uniqueness, $\mathcal F_\infty$ is independent of $\mathcal T$.

	Since $\mathcal T \subseteq \mathcal F_\infty$, if $A \in \mathcal T$, $A$ is independent from $A$.
	So $\prob{A} = \prob{A \cap A} = \prob{A}\prob{A}$, so $\prob{A}^2 - \prob{A} = 0$ as required.

	Finally, if $Y \colon (\Omega,\mathcal T) \to (\mathbb R,\mathcal B)$, the preimages of $\qty{Y \leq y}$ lie in $\mathcal T$, which give probability one or zero.
	Let $c = \inf\qty{y \mid F_Y(y) = 1}$, so $Y = c$ almost surely.
\end{proof}
