\section{Estimation}

\subsection{Estimators}
Suppose $X_1, \dots, X_n$ are i.i.d.\ observations with a p.d.f.\ (or p.m.f.) $f_X(x \mid \theta)$, where $\theta$ is an unknown parameter in some parameter space $\Theta$.
Let $X = (X_1, \dots, X_n)$.

\begin{definition}[Estimator]
	An \vocab{estimator} is a statistic, or a function of the data, written $T(X) = \hat\theta$, which is used to approximate the true value of $\theta$.
	This does not depend (explicitly) on $\theta$.
	The distribution of $T(X)$ is called its \vocab{sampling distribution}.
\end{definition}

\begin{example}
	Let $X_1, \dots, X_n \sim N(0,1)$ be i.i.d.
	Let $\hat \mu = T(X) = \overline X_n$.
	The sampling distribution is $T(X) \sim N\qty(\mu, \frac{1}{n})$.
	Note that this sampling distribution in general depends on the true parameter $\mu$.
\end{example}

\begin{definition}[Bias]
	The \vocab{bias} of $\hat \theta$ is
	\begin{align*}
		\mathrm{bias}\qty(\hat \theta) = \esub{\theta}{\hat \theta} - \theta
	\end{align*}
	Note that $\hat \theta$ is a function only of $X_1, \dots, X_n$, and the expectation operator $\mathbb E_\theta$ assumes that the true value of the parameter is $\theta$.
\end{definition}

\begin{remark}
	In general, the bias is a function of the true parameter $\theta$, even though it is not explicit in the notation.
\end{remark}

\begin{definition}[Unbiased Estimator]
	An estimator with zero bias for all $\theta$ is called an \vocab{unbiased estimator}.
\end{definition}

\begin{example}
	The estimator $\hat \mu$ in the above example is unbiased, since
	\begin{align*}
		\esub{\mu}{\hat \mu} = \esub{\mu}{\overline X_n} = \mu
	\end{align*}
	for all $\mu \in \mathbb R$.
\end{example}

\begin{definition}[Mean Squared Error]
	The \vocab{mean squared error} of $\theta$ is defined as
	\begin{align*}
		\mathrm{mse}\qty(\hat \theta) = \esub{\theta}{\qty(\hat \theta - \theta)^2}
	\end{align*}
\end{definition}

\begin{remark}
	Like the bias, the mean squared error is, in general, a function of the true parameter $\theta$.
\end{remark}

\subsection{Bias-variance decomposition}
The mean squared error can be written as
\begin{align*}
	\mathrm{mse}\qty(\hat \theta) = \esub{\theta}{\qty(\hat \theta - \esub{\theta}{\hat\theta} + \esub{\theta}{\hat\theta} - \theta)^2} = \Varsub{\theta}{\hat \theta} + \mathrm{bias}^2\qty(\hat\theta)
\end{align*}
Note that both the variance and bias squared terms are positive.
This implies a tradeoff between bias and variance when minimising error.
\begin{example}
	Let $X \sim \mathrm{Bin}(n, \theta)$ where $n$ is known and $\theta$ is an unknown probability.
	Let $T_U = X / n$.
	This is the proportion of successes observed.
	This is an unbiased estimator, since $\esub{\theta}{T_U} = \esub{\theta}{X}/n = \theta$.
	The mean squared error for the estimator is then
	\begin{align*}
		\Varsub{\theta}{T_n} = \Varsub{\theta}{\frac{X}{n}} = \frac{\Varsub{\theta}{X}}{n^2} = \frac{\theta(1-\theta)}{n}
	\end{align*}
	Now, consider an alternative estimator which has some bias:
	\begin{align*}
		T_B = \frac{X+1}{n+2} = w \underbrace{\frac{X}{n}}_{T_U} + (1-w)\frac{1}{2};\quad w = \frac{n}{n+2}
	\end{align*}
	This interpolates between the estimator $T_U$ and the fixed estimator $\frac{1}{2}$.
	Here,
	\begin{align*}
		\mathrm{bias}(T_B) = \esub{\theta}{T_B} - \theta = \frac{n}{n+2}\theta - \frac{1}{n+2}\theta
	\end{align*}
	The bias is nonzero for all but one value of $\theta$.
	Further,
	\begin{align*}
		\Varsub{\theta}{T_B} = \frac{\Varsub{\theta}{X+1}}{(n+2)^2} = \frac{n\theta(1-\theta)}{(n+2)^2}
	\end{align*}
	We can calculate
	\begin{align*}
		\mathrm{mse}(T_B) = (1-w)^2 \qty(\frac{1}{2} - \theta)^2 + w^2\underbrace{\frac{\theta(1-\theta)}{n}}_{\mathrm{mse}(T_U)}
	\end{align*}
	There exists a range of $\theta$ such that $T_B$ has a lower mean squared error, and similarly there exists a range such that $T_U$ has a lower error.
	This indicates that prior judgement of the true value of $\theta$ can be used to determine which estimator is better.
\end{example}
It is not necessarily desirable that an estimator is unbiased.
\begin{example}
	Suppose $X \sim \mathrm{Poisson}(\lambda)$ and we wish to estimate $\theta = \prob{X = 0}^2 = e^{-2\lambda}$.
	For some estimator $T(X)$ of $\theta$ to be unbiased, we need that
	\begin{align*}
		\esub{\lambda}{T(X)} = \sum_{x=0}^\infty T(x) \frac{\lambda^x e^{-\lambda}}{x!} = e^{-2\lambda}
	\end{align*}
	Hence,
	\begin{align*}
		\sum_{x=0}^\infty T(x) \frac{\lambda^x}{x!} = e^{-\lambda}
	\end{align*}
	But $e^{-\lambda}$ has a known power series expansion, giving $T(X) \equiv (-1)^X$ for all $X$.
	This is not a good estimator, for example because it often predicts negative numbers for a positive quantity.
\end{example}

\subsection{Sufficiency}
\begin{definition}[Sufficiency]
	A statistic $T(X)$ is \vocab{sufficient} for $\theta$ if the conditional distribution of $X$ given $T(X)$ does not depend on $\theta$.
	Note that $\theta$ and $T(X)$ may be vector-valued, and need not have the same dimension.
\end{definition}

\begin{example}
	Let $X_1, \dots, X_n$ be i.i.d.\ Bernoulli random variables with parameter $\theta$ where $\theta \in [0,1]$.
	The mass function is
	\begin{align*}
		f_X(x \mid \theta) = \prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i} = \theta^{\sum x_i} (1-\theta)^{n - \sum x_i}
	\end{align*}
	Note that this dependent only on $x$ via the statistic $T(X) = \sum_{n=1}^n x_i$.
	Here,
	\begin{align*}
		f_{X \mid T = t}(x \mid \theta) = \frac{\psub{\theta}{X = x, T(X) = t}}{\psub{\theta}{T(x) = t}}
	\end{align*}
	If $\sum x_i = t$, we have
	\begin{align*}
		f_{X \mid T = t}(x \mid \theta) = \frac{\theta^{\sum x_i} (1-\theta)^{n-\sum x_i}}{\binom{n}{t} \theta^t (1-\theta)^{n-\sum x_i}} = \frac{1}{\binom{n}{t}}
	\end{align*}
	Hence $T(X)$ is sufficient for $\theta$.
\end{example}

\subsection{Factorisation criterion}
\begin{theorem}
	$T$ is sufficient for $\theta$ if and only if
	\begin{align*}
		f_X(x \mid \theta) = g(T(x), \theta) h(x)
	\end{align*}
	for suitable functions $g,h$.
\end{theorem}
\begin{proof}
	This will be proven in the discrete case; the continuous case can be handled analogously.
	Suppose that the factorisation criterion holds.
	Then, if $T(x) = t$,
	\begin{align*}
		f_{X \mid T = t}(x \mid T = t) & = \frac{\psub{\theta}{X = x, T(x) = t}}{\psub{\theta}{T(x) = t}}             \\
		                               & = \frac{g(T(x),\theta)h(x)}{\sum_{x' \colon T(x') = t} g(T(x'),\theta)h(x')} \\
		                               & = \frac{h(x)}{\sum_{x' \colon T(x') = t} h(x')}
	\end{align*}
	which does not depend on $\theta$.
	By definition, $T(X)$ is sufficient.

	Conversely, suppose that $T(X)$ is sufficient.
	\begin{align*}
		f_X(x \mid \theta) & = \psub{\theta}{X = x}                                                                                                \\
		                   & = \psub{\theta}{X = x, T(X) = T(x)}                                                                                   \\
		                   & = \underbrace{\psub{\theta}{X = x \mid T(X) = T(x)}}_{h(x)} \underbrace{\psub{\theta}{T(X) = T(x)}}_{g(T(X), \theta)}
	\end{align*}
\end{proof}
\begin{example}
	Consider the above example with $n$ Bernoulli random variables with mass function
	\begin{align*}
		f_X(x \mid \theta) = \theta^{\sum x_i} (1-\theta)^{n - \sum x_i}
	\end{align*}
	Let $T(X) = \sum x_i$, and then the above mass function is in the form of $g(T(X), \theta)$ and we can set $h(x) \equiv 1$.
	Hence $T(X)$ is sufficient.
\end{example}
\begin{example}
	Let $X_1, \dots, X_n$ be i.i.d.\ from a uniform distribution on the interval $[0,\theta]$ for some $\theta > 0$.
	The mass function is
	\begin{align*}
		f_X(x \mid \theta) = \prod_{i=1}^n \frac{1}{\theta} \mathbbm 1\qty{x_i \in [0,\theta]} = \qty(\frac{1}{\theta})^{n} \mathbbm 1\qty{\min_i x_i \geq 0} \mathbbm 1\qty{\max_i x_i \leq \theta}
	\end{align*}
	Let $T(X) = \max_i X_i$.
	Then
	\begin{align*}
		g(T(X), \theta) = \qty(\frac{1}{\theta})^n \mathbbm 1\qty{\max_i x_i \leq \theta};\quad h(x) \equiv \mathbbm 1\qty{\min_i x_i \geq 0}
	\end{align*}
	We can then conclude that $T(X)$ is sufficient for $\theta$.
\end{example}

\subsection{Minimal sufficiency}
Sufficient statistics are not unique.
For instance, any bijection applied to a sufficient statistic is also sufficient.
Further, $T(X) = X$ is always sufficient.
We instead seek statistics that maximally compress and summarise the relevant data in $X$ and that discard extraneous data.

\begin{definition}[Minimal Sufficiency]
	A sufficient statistic $T(X)$ for $\theta$ is \vocab{minimal} if it is a function of every other sufficient statistic for $\theta$.
	More precisely, if $T'(X)$ is sufficient, $T'(x) = T'(y) \implies T(x) = T(y)$.
\end{definition}

\begin{remark}
	Any two minimal statistics $S, T$ for the same $\theta$ are bijections of each other.
	That is, $T(x) = T(y)$ if and only if $S(x) = S(y)$.
\end{remark}

\begin{theorem}
	Suppose that $f_X(x \mid \theta)/f_X(y \mid \theta)$ is constant in $\theta$ if and only if $T(x) = T(y)$.
	Then $T$ is minimal sufficient.
\end{theorem}

\begin{remark}
	This theorem essentially states the following.
	Let $x \overset{1}{\sim} y$ if the above ratio of probability density or mass functions is constant in $\theta$.
	This is an equivalence relation.
	Similarly, we can define $x \overset{2}{\sim} y$ if $T(x) = T(y)$.
	This is also an equivalence relation.
	The hypothesis in the theorem is that the equivalence classes of $\overset{1}{\sim}$ and $\overset{2}{\sim}$ are equal.
	Further, we may always construct a minimal sufficient statistic for any parameter since we can use the construction $\overset{1}{\sim}$ to create equivalence classes, and set $T$ to be constant for all such equivalence classes.
\end{remark}
\begin{proof}
	Let $t \in \Im T$.
	Then let $z_t$ be a representative of the equivalence class $\qty{x \colon T(x) = t}$.
	Then
	\begin{align*}
		f_X(x \mid \theta) = f_X(z_{T(x)} \mid \theta) \frac{f_X(x \mid \theta)}{f_X(z_{T(x)} \mid \theta)}
	\end{align*}
	By the hypothesis, the ratio on the right hand side does not depend on $\theta$, so let this ratio be $h(x)$.
	Further, the other term depends only on $T(x)$, so it may be $g(T(x), \theta)$.
	Hence $T$ is sufficient by the factorisation criterion.

	To prove minimality, let $S$ be any other sufficient statistic, and then by the factorisation criterion there exist $g_S$ and $h_S$ such that $f_X(x \mid \theta) = g_S(S(x), \theta) h_S(x)$.
	Now, suppose $S(x) = S(y)$ for some $x, y$.
	Then,
	\begin{align*}
		\frac{f_X(x \mid \theta)}{f_X(y \mid \theta)} = \frac{g_S(S(x), \theta) h_S(x)}{g_S(S(y), \theta) h_S(y)} = \frac{h_S(x)}{h_S(y)}
	\end{align*}
	which is constant in $\theta$.
	Hence, $x \overset{1}{\sim} y$.
	By the hypothesis, we have $x \overset{2}{\sim} y$, so $T(x) = T(y)$, which is the requirement for minimality.
\end{proof}

\begin{remark}
	Sometimes the range of $X$ depends on $\theta$ (e.g. $X_1, \dots, X_n \overset{iid}{\sim}\operatorname{Unif}([0, \theta])$).
	In this case we can interpret ``$\frac{f_X(x \mid \theta)}{f_X(y \mid \theta)}$ constant in $\theta$'' to mean that $f_X(x \mid \theta) = c(x, y) f_X(y \mid \theta)$ for some function $c$ which does not depend on $\theta$.
\end{remark}

\begin{example}
	Let $X_1, \dots, X_n$ be normal with unknown $\mu, \sigma^2$.
	\begin{align*}
		\frac{f_X(x \mid \mu, \sigma^2)}{f_X(y \mid \mu, \sigma^2)} & = \frac{(2 \pi \sigma^2)^{-n/2} \exp{-\frac{1}{2\sigma^2} \sum_i (x_i - \mu)^2}}{(2 \pi \sigma^2)^{-n/2} \exp{-\frac{1}{2\sigma^2} \sum_i (y_i - \mu)^2}} \\
		& = \exp{-\frac{1}{2\sigma^2} \qty(\sum_i x_i^2 - \sum_i y_i^2) + \frac{\mu}{\sigma^2} \qty(\sum_i x_i - \sum_i y_i)}
	\end{align*}
	Hence, for minimality, this is constant in the parameters $\mu, \sigma^2$ if and only if $\sum_i x_i^2 = \sum_i y_i^2$ and $\sum_i x_i = \sum_i y_i$.
	Thus, a minimal sufficient statistic is $\qty(\sum_i x_i^2, \sum_i x_i)$ is a minimal sufficient statistic.
	A more common way of expressing the minimal sufficient statistic is
	\begin{align*}
		S(x) = \qty(\overline X_n, S_{xx});\quad \overline X_n = \frac{1}{n} \sum_i x_i;\quad S_{xx} = \sum_i \qty(X_i - \overline X_n)^2
	\end{align*}
	which is a bijection of the above minimal sufficient statistic so is also minimal sufficient.
\end{example}

\begin{remark}
	$\theta$ and a minimal statistic $T$ need not have the same dimension.
\end{remark} 

\begin{example}
	Consider $X_1, \dots, X_n \overset{\text{iid}}{\sim} N(\mu, \mu^2)$. 
	Here, there is a single parameter $\mu$ but the minimal sufficient statistic is still $S(x)$ as defined above.
\end{example}

\subsection{Rao-Blackwell theorem}
Previously, the notation $\mathbb E_\theta$ and $\mathbb P_\theta$ have been used to denote expectations and probabilities under the model where the observations are i.i.d.\ with p.d.f.\ or p.m.f.\ $f_X$.
From now, we omit this subscript, as it will be implied for much of the remainder of the course.
\begin{theorem}
	Let $T$ be a sufficient statistic for $\theta$, and define an estimator $\widetilde \theta$ with $\expect{{\widetilde \theta}^2} < \infty$ for all $\theta$.
	Now we define another estimator
	\begin{align*}
		\hat \theta = \expect{\widetilde \theta \mid T(x)}
	\end{align*}
	Then, for all values of $\theta$, we have
	\begin{align*}
		\expect{\qty(\hat \theta - \theta)^2} \leq \expect{\qty(\widetilde \theta - \theta)^2}
	\end{align*}
	In other words, the mean squared error of $\hat \theta$ is not greater than the mean squared error of $\widetilde \theta$.
	Further, the inequality is strict unless $\widetilde \theta$ is a function of $T$.
\end{theorem}

\begin{remark}
	Starting from any estimator $\widetilde \theta$, if we condition on the sufficient statistic $T$ we obtain a `better' statistic $\hat \theta$.
	Note that $T$ must be sufficient, otherwise $\hat \theta$ may be a function of $\theta$ and thus not an estimator:
	\begin{align*}
		\hat \theta(X) = \hat \theta(T) = \int \hat \theta(x) \underbrace{f_{X \mid T}(x \mid T)}_{\mathclap{\text{does not depend on } \theta \text{ as } T \text{ is sufficient}}} \dd{x}
	\end{align*}
\end{remark}

The message to take away from this theorem is that we can improve the mse of any estimate $\tilde \theta$ by taking a conditional expectation given $T(x)$.

\begin{proof}
	By the tower property of the expectation, we can find
	\begin{align*}
		\expect{\hat \theta} = \expect{\expect{\widetilde \theta \mid T(x)}} = \expect{\widetilde \theta}
	\end{align*}
	Hence, $\mathrm{bias}\qty(\hat\theta) = \mathrm{bias}\qty(\widetilde\theta)$.
	By the conditional variance formula,
	\begin{align*}
		\Var{\widetilde \theta} = \expect{\underbrace{\Var{\widetilde \theta \mid T}}_{\geq 0}} + \underbrace{\Var{\expect{\widetilde \theta \mid T}}}_{\Var{\hat\theta}} \geq \Var{\hat \theta} \quad \forall \; \theta.
	\end{align*}
	By the bias-variance decomposition, we know that $\mathrm{mse}\qty(\widetilde \theta) \geq \mathrm{mse}\qty(\hat \theta)$.
	The inequality is strict unless $\Var{\widetilde \theta \mid T} = 0$ almost surely.
	This requires that $\widetilde \theta$ is a function of $T$.
\end{proof}
\begin{example}
	Let $X_1, \dots, X_n$ be i.i.d.\ Poisson random variables with parameter $\lambda$.
	Then let $\theta = \prob{X_1 = 0} = e^{-\lambda}$.
	Here,
	\begin{align*}
		f_X(x \mid \lambda) = \frac{e^{-n \lambda} \lambda^{\sum x_i}}{\prod x_i!} \implies f_X(x \mid \theta) = \frac{\theta^n (-\log \theta)^{\sum x_i}}{\prod x_i!}
	\end{align*}
	Using the factorisation criterion, we find
	\begin{align*}
		g(T(x), \theta) = g\qty(\sum x_i, \theta) = \theta^n (-\log\theta)^{\sum x_i};\quad h(x) = \frac{1}{\prod x_i!}
	\end{align*}
	so $T(x) = \sum x_i$ is sufficient.

	Note that $\sum X_i$ has a Poisson distribution with parameter $n \lambda$.

	Consider the estimator $\widetilde \theta = \mathbbm 1\qty{X_1 = 0}$.
	This depends only on $X_1$, hence it is a weak estimator.
	However, it is unbiased, so when we apply the Rao-Blackwell theorem we will construct an unbiased $\hat \theta$, which is precisely
	\begin{align*}
		\hat \theta = \expect{\widetilde \theta \mid \sum X_i = t} & = \prob{X_1 = 0 \mid \sum X_i = t} \\
		& = \frac{\prob{X_1 = 0, \sum X_i = t}}{\prob{\sum X_i = t}} \\
		& = \frac{\prob{X_1 = 0}\prob{\sum_{i=2}^n X_i = t}}{\prob{\sum_{i=1}^n X_i = t}}\footnote{We know the distribution of $\sum X_i$, so simply sub this pmf in.} \\
		&\vdots \\
		& = \qty(\frac{n-1}{n})^t
\end{align*}
	This may also be written
	\begin{align*}
		\hat \theta = \qty(1 - \frac{1}{n})^{\sum x_i}
	\end{align*}
	which is an estimator with strictly lower mean squared error than $\widetilde \theta$ for all $\theta$ by Rao-Blackwell and as $\widetilde \theta$ doesn't depend solely upon $T$.

	Note that $\hat \theta = \qty(1 - \frac{1}{n})^{n \overline X_n}$ converges in the limit to $e^{-\overline X_n}$.
	By the strong law of large numbers, $\overline X_n \to \expect{X_1} = \lambda$ almost surely, so we arrive at $\hat \theta \to e^{-\lambda} = \theta$ almost surely.
\end{example}
\begin{example}
	Let $X_1, \dots, X_n$ be i.i.d.\ uniform random variables in an interval $[0, \theta]$.
	We wish to estimate $\theta \geq 0$.
	We observed that $T = \max X_i$ is sufficient for $\theta$. \\
	Let $\widetilde \theta = 2 X_1$.
	This is an unbiased estimator of $\theta$.
	Then the Rao-Blackwellised estimator $\hat \theta$ is
	\begin{align*}
		\hat \theta & = \expect{\widetilde \theta \mid T = t} \\
		& = 2 \expect{X_1 \mid \max X_i = t} \\
		& = 2 \expect{X_1 \mid \max X_i = t, X_1 = \max X_i} \prob{X_1 = \max X_i \mid \max X_i = t} \\
		& + 2 \expect{X_1 \mid \max X_i = t, X_1 \neq \max X_i} \prob{X_1 \neq \max X_i \mid \max X_i = t}
	\end{align*}
	Since $X_1, \dots, X_n$ are i.i.d., the conditional probability $\prob{X_1 = \max X_i \mid \max X_i = t}$ can be reduced to $\prob{X_1 = \max X_i} = \frac{1}{n}$.
	The complementary event may be reduced in an analogous way.
	The expectation $\expect{X_1 \mid \max X_i = t, X_1 = \max X_i}$ can be reduced to $t$.
	\begin{align*}
		\hat \theta & = \frac{2t}{n} + \frac{2(n-1)}{n} \expect{X_1 \mid X_1 < t, \max_{i=2}^n X_i = t} \\
		& = \frac{2t}{n} + \frac{2(n-1)}{n} \expect{X_1 \mid X_1 < t}\footnote{By independence} \\
		& = \frac{2t}{n} + \frac{2(n-1)}{n} \frac{t}{2} \\
		& = \frac{2t}{n} + \frac{t(n-1)}{n} = \frac{n+1}{n} \max_i X_i
	\end{align*}
	By the Rao-Blackwell theorem, the mean squared error of $\hat \theta$ is strictly better than the mean squared error of $\widetilde \theta$.
	This is also an unbiased estimator.
\end{example}

\subsection{Maximum likelihood estimation}
Let $X_1, \dots, X_n$ be i.i.d.\ random variables with mass or density function $f_X(x \mid \theta)$.
\begin{definition}[Likelihood Function]
	For fixed observations $x$, the \vocab{likelihood function} $L \colon \Theta \to \mathbb R$ is given by
	\begin{align*}
		L(\theta) = f_X(x \mid \theta) = \prod_{i=1}^n f_{X_i} (x_i \mid \theta)
	\end{align*}
\end{definition}

\begin{definition}[Log-Likelihood Function]
	We will denote the \vocab{log-likelihood} by
	\begin{align*}
		\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log f_{X_i} (x_i \mid \theta)
	\end{align*}
\end{definition} 
\begin{definition}
	A \vocab{maximum likelihood estimator} is an estimator that maximises the likelihood function $L$ over $\Theta$.
	Equivalently, the estimator maximises $\ell$.
\end{definition}

\begin{example}
	Let $X_1, \dots, X_n$ be i.i.d.\ Bernoulli random variables with parameter $p$.
	The log-likelihood function is
	\begin{align*}
		\ell(p) = \sum_{i=1}^n [X_i \log p + (1-X_i) \log (1-p)] = \log p \sum X_i + \log(1-p) \qty(n - \sum X_i)
	\end{align*}
	The derivative is
	\begin{align*}
		\ell'(p) = \frac{\sum X_i}{p} + \frac{n - \sum X_i}{1-p}
	\end{align*}
	which has a single stationary point at $p = \frac{1}{n} \sum X_i = \overline X$.
	We have $\expect{\hat p} = p$, so the maximum likelihood estimator in this case is unbiased.
\end{example}

\begin{example}
	Let $X_1, \dots, X_n$ be i.i.d.\ normal random variables with unknown mean $\mu$ and variance $\sigma^2$.
	\begin{align*}
		\ell(\mu, \sigma^2) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log \sigma^2 - \frac{1}{2\sigma^2} \sum (X_i - \mu)^2
	\end{align*}
	This function is concave in $\mu$ and $\sigma^2$, so there exists a unique maximiser.
	In particular, $\ell$ is maximised when $\pdv{\ell}{\mu} = \pdv{\ell}{\sigma^2} = 0$.
	\begin{align*}
		\pdv{\ell}{\mu} = \frac{1}{\sigma^2} \sum (X_i - \mu)
	\end{align*}
	This is zero if $\mu = \overline X$. \\
	Further,
	\begin{align*}
		\pdv{\ell}{\sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum (X_i - \mu)^2 = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum (X_i - \overline X)^2
	\end{align*}
	This is zero iff
	\begin{align*}
		\sigma^2 = \frac{1}{n} \sum (X_i - \overline X)^2 = \frac{S_{xx}}{n}
	\end{align*}
	Hence, the maximum likelihood estimator is $\qty(\hat \mu, \hat \sigma^2) = \qty(\overline X_n, \frac{1}{n} S_{xx})$.

	We can show that $\hat \mu = \overline{X}$ is unbiased.

	We will later prove that
	\begin{align*}
		\frac{S_{xx}}{\sigma^2} = \frac{n\hat \sigma^2}{\sigma^2} \sim \chi_{n-1}^2
	\end{align*}
	Hence
	\begin{align*}
		\expect{\hat \sigma^2} = \frac{\sigma^2}{n} \expect{\chi_{n-1}^2} = \sigma^2 \frac{n-1}{n} \neq \sigma^2
	\end{align*}
	This is therefore a biased estimator, but the bias converges to zero as $n \to \infty \; \forall \; \sigma^2$ : $\hat \sigma^2$ is \vocab{asymptotically unbiased}.
\end{example}

\begin{example}
	Let $X_1, \dots, X_n$ be i.i.d.\ uniform random variables on $[0,\theta]$.
	Here, we derived the unbiased estimator $\hat \theta = \frac{n+1}{n} \max X_i$.

	The likelihood is given by
	\begin{align*}
		L(\theta) = \frac{1}{\theta^n} \mathbbm 1\qty{\max X_i \leq \theta}
	\end{align*}
	This function is maximised at $\hat \theta_{\mathrm{mle}} = \max X_i$.

	By comparison to the $\hat \theta$ derived from the Rao-Blackwell process, $\hat \theta_{\mathrm{mle}}$ is biased but asymptotically unbiased.
	In particular,
	\begin{align*}
		\expect{\hat \theta_{\mathrm{mle}}} = \frac{n}{n+1} \expect{\hat \theta} = \frac{n}{n+1} \theta
	\end{align*}
\end{example}

\begin{remark}
	\begin{enumerate}
		\item If $T$ is a sufficient statistic for $\theta$, then the maximum likelihood estimator is a function of $T(X)$.
		Indeed, since $X$ and $T(X)$ are fixed, the maximiser of $L(\theta) = g(T(X), \theta) h(X)$ depends on $X$ only through $T$.
		This is good as otherwise we could use Rao-Blackwell to get a better estimator in terms of the mse.
		\item If $\varphi = H(\theta)$ for a bijection $H$, then if $\hat \theta$ is the maximum likelihood estimator for $\theta$, we have that $H(\hat \theta)$ is the maximum likelihood estimator for $\varphi$.
		\item Asymptotic Normality: Under some regularity conditions, as $n \to \infty$ the statistic $\sqrt{n} (\hat \theta - \theta)$ is approximately normal with mean zero and covariance matrix $\Sigma$.
		More precisely, for `nice' sets $A$ and `regular' values of $\theta$, we have
		\begin{align*}
			\prob{\sqrt{n} \qty(\hat \theta(n) - \theta) \in A} \overset{n \to \infty}\to \prob{Z \in A};\quad Z \sim N(0, \Sigma)
		\end{align*}
		We say that the maximum likelihood estimator is \textit{asymptotically normal}.
		The limiting covariance matrix $\Sigma$ is a known function of $\ell$, which will not be defined in this course.
		There is a theorem (Cramer-Rao) which says in some sense, $\Sigma$ is the smallest variance that any estimator can achieve asymptotically.
		\item For practical purposes, this estimator can often be found numerically by maximising $\ell$ or $L$.
	\end{enumerate} 
\end{remark}
