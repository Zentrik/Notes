\section{Bayesian analysis}

\subsection{Introduction}
Frequentist analysis considers the value $\theta$ to be fixed, and then we can make inferential statements about $\theta$ in the context of repeated experiments on a random variable $X$.
Bayesian analysis is an alternative to frequentist analysis, where $\theta$ is itself treated as a random variable taking values in the parameter space $\Theta$.
We say that the \textit{prior} distribution $\pi(\theta)$ is a distribution representing the beliefs of the investigator about $\theta$ before observing data.
The data $X$ has a p.d.f.\ or p.m.f.\ conditional on $\theta$ given by $f_X(\wildcard \mid \theta)$.
Having observed $X$, we can combine this information with the prior distribution to form the \textit{posterior} distribution $\pi(\theta \mid X)$, which is the conditional distribution of $\theta$ given $X$.
This contains updated information about the value of $\theta$.

By Bayes' rule,
\begin{align*}
	\pi(\theta \mid x) = \frac{\pi(\theta) f_X(x \mid \theta)}{f_X(x)}
\end{align*}
where $f_X(x)$ is the marginal distribution of $X$, defined by
\begin{align*}
	f_X(x) = \begin{cases}
		\int_\Theta f_X(x\mid\theta) \pi(\theta) \dd{\theta} & \theta \text{ continuous} \\
		\sum_\Theta f_X(x\mid\theta) \pi(\theta)             & \theta \text{ discrete}
	\end{cases}
\end{align*}
More simply,
\begin{align*}
	\pi(\theta \mid X) \propto \pi(\theta) \cdot f_X(X \mid \theta)
\end{align*}
The proportionality here is with respect to $\theta$.
So the posterior is proportional to the prior multiplied by the likelihood.
It is often easy to recognise that the right hand side of this expression is in some family of distributions, such as $N$ or $\Gamma$, up to some normalising constant.
\begin{remark}
	By the factorisation criterion, if $T$ is a sufficient statistic for $\theta$, the posterior $\pi(\theta \mid x)$ depends on $X$ only through $T$.
	More precisely,
	\begin{align*}
		\pi(\theta \mid X) \propto \pi(\theta) g(T(X),\theta) h(X) \propto \pi(\theta) g(T(C),\theta)
	\end{align*}
\end{remark}

\begin{example}[Clear Prior]
	Consider a patient who we will test for the presence of a disease, where we have no information about the health or lifestyle of the patient.
	Let $\theta$ take the value 1 if the patient is infected and 0 otherwise.
	We have a random variable $X$ which takes the value 1 if a given test returns a positive result and 0 if the test is negative.
	We know the \textit{sensitivity} of the test $f_X(X=1\mid \theta=1)$, and the \textit{specificity} of the test $f_X(X=0\mid \theta=0)$.
	This fully specifies the likelihood function.

	We now must choose a prior distribution.
	For example, let $\pi(\theta = 1)$ be the estimated proportion of the general population that have the given disease.
	The posterior is the probability of an infection given the test result.
	\begin{align*}
		\pi(\theta = 1 \mid X = 1) = \frac{\pi(\theta = 1) f_X(X = 1 \mid \theta = 1)}{\pi(\theta = 1) f_X(X = 1 \mid \theta = 1) + \pi(\theta = 0) f_X(X = 1 \mid \theta = 0)}
	\end{align*}
	Even with a positive test result, the posterior distribution may still yield a low probability for $\theta$, which may happen if $\pi(\theta = 1) \ll \pi(\theta = 0)$.
\end{example}

\begin{example}
	Let $\theta$ be the mortality rate of a particular surgery, which will take values in $[0,1]$.
	In the first ten operations, we observed that none of the patients died.
	We will model $X_i \sim B(10,\theta)$ and observe $X_i = 0$.

	We must choose a prior.
	Suppose that we have data from other hospitals that suggests that the mortality for the surgery ranges from 3\% to 20\%, with an average of 10\%.
	We can choose the prior to be the beta distribution, $\pi(\theta) \sim \mathrm{Beta}(a,b)$, since the value of $\theta$ should range between zero and one.
	Let $a = 3$ and $b = 27$, which will give $\expect{\theta} = 0.1$ and $\prob{0.03 < \theta < 0.2} \approx 0.9$.
	In this case, the posterior is
	\begin{align*}
		\pi(\theta \mid X) &\propto \pi(\theta) f_X(X\mid \theta) \\
		&\propto \theta^{a-1} (1-\theta)^{b-1} \theta^{\sum x_i} (1-\theta)^{10 - \sum x_i} \\
		&= \theta^{\sum x_i +a-1} (1-\theta)^{b - 10 - \sum x_i -1}
	\end{align*}
	This is again a beta distribution with parameters $\sum x_i + a$ and $10 - \sum x_i + b$.
	The normalising constant does not need to be explicitly calculated since the form of the distribution can be recognised.

	With the above data, we obtain $\pi(\theta \mid \sum x_i = 0) \sim \mathrm{Beta}(3,37)$.
	This posterior has a smaller variance than the prior, and a smaller expectation due to observing no deaths.
\end{example}

\begin{note}
	Here the prior and posterior are in the same family of distributions.
	This is know as \vocab{conjugacy}.
\end{note} 

\subsection{Inference from the posterior}
The posterior distribution $\pi(\theta \mid x)$ represents information about $\theta$ after having observed some data $X$.
This can be used to make decisions under uncertainty.
\begin{enumerate}
	\item We first choose some decision $\delta \in \Delta$.
	      For instance, in the first example, a decision could be to ask the patient to isolate from others to reduce transmission.
	\item We define a \textit{loss function} $L(\theta,\delta)$, which defines what loss is incurred by making decision $\delta$ given the true value of $\theta$.
	      In the above example, $L(\theta = 1, \delta = 1)$ is the loss incurred by asking the patient to isolate given that they have the disease.
	\item We can now choose the decision $\delta$ that minimises
	      \begin{align*}
		      \int_\Theta L(\theta, \delta) \pi(\theta \mid x) \dd{\theta}
	      \end{align*}
	      which is the posterior expectation of the loss.
\end{enumerate}
This method maximises over your preferences if they can be represented using a utility function, see Von Neumann - Morgenstern utility theorem.
\subsection{Point estimation}
We can use Bayesian analysis to represent an estimate for the value of $\theta$ as a decision, where the decision is a ``best guess'' for the true parameter.

\begin{definition}[Bayes estimator]
	The \vocab{Bayes estimator} $\hat \theta^{(B)}$ minimises
	\begin{align*}
		h(\delta) = \int_\Theta L(\theta, \delta) \pi(\theta \mid x) \dd{\theta}
	\end{align*}
\end{definition}

\begin{example}
	Suppose the loss function is quadratic, given by $L(\theta, \delta) = (\theta-\delta)^2$.
	Here,
	\begin{align*}
		h(\delta) = \int_\Theta (\theta - \delta)^2 \pi(\theta \mid x) \dd{\theta}
	\end{align*}
	Thus, $h'(\delta) = 0$ if
	\begin{align*}
		\int_\Theta (\theta - \delta) \pi(\theta \mid x) \dd{\theta} = 0 \iff \delta = \int_\Theta \theta \pi(\theta \mid x) \dd{x}
	\end{align*}
	Under the quadratic loss function, $\hat \theta^{(B)}$ can be described as the expectation of $\theta$ under the posterior distribution.
\end{example}

\begin{example}
	Consider the absolute error loss, given by $L(\theta, \delta) = \abs{\theta - \delta}$.
	In this case we have
	\begin{align*}
		h(\delta) = \int_\Theta \abs{\theta - \delta} \pi(\theta \mid x) \dd{\theta} = \int_{-\infty}^\delta -(\theta-\delta)\pi(\theta \mid x)\dd{\theta} + \int_\delta^\infty (\theta-\delta)\pi(\theta \mid x)\dd{\theta}
	\end{align*}
	We can differentiate, using the fundamental theorem of calculus, to find
	\begin{align*}
		h'(\delta) = \int_{-\infty}^\delta \pi(\theta\mid x) \dd{\theta} - \int_\delta^\infty \pi(\theta \mid x)\dd{\theta}
	\end{align*}
	This is zero if and only if
	\begin{align*}
		\int_{-\infty}^\delta \pi(\theta\mid x) \dd{\theta} = \int_\delta^{\infty} \pi(\theta \mid x) \dd{\theta}
	\end{align*}
	This yields the median of the posterior distribution.
\end{example}

\subsection{Credible intervals}
\begin{definition}[Credible Interval]
	A $100\gamma$\% \vocab{credible interval} $(A(x), B(x))$ satisfies
	\begin{align*}
		\pi(A(x) \leq \theta \leq B(x) \mid x) = \gamma
	\end{align*}
\end{definition}
Here $x$ is fixed at the observed value.

\begin{remark}
	Unlike confidence intervals, credible intervals can be interpreted conditionally on the data.
	For example, we could say that given a specific observation $x$, we are $100 \gamma$\% certain that $\theta$ lies within $(A(x), B(x))$.
	This credible interval is also dependent on the choice of prior distribution.
\end{remark}

\begin{note}
	If $T$ is a sufficient statistic, $\pi(\theta \mid x)$ only depends on $x$ through $T(x)$.
	\begin{align*}
		\pi(\theta \mid x) &\propto \pi(\theta) f_X(x \mid \theta) \\
		&= \pi(\theta) g(T(x), \theta) h(x) \\
		&\propto \pi(\theta) g(T(x), \theta).
	\end{align*} 
	I.e. any two data sets with the same sufficient statistic will have the same posterior.
\end{note} 

\begin{example}
	Let $X_1, \dots, X_n \sim \mathcal{N}(\mu, 1)$ iid.
	Our prior is $\pi(\mu) \sim \mathcal{N}(0, \frac{1}{\tau^2})$.
	\begin{align*}
		\pi(\mu \mid x) &\propto f_X(x \mid \mu) \pi(\mu) \\
		&\propto \exp\qty[-\frac{1}{2} \sum_{i=1}^{n} (x_i - \mu)^2] \exp \qty[- \frac{\mu^2 \tau^2}{2}]\footnote{We have omitted constants independent of $\mu$.} \\
		&\propto \exp\qty[-\frac{1}{2} (n + \tau^2) \qty(\mu - \frac{\sum x_i}{n + \tau^2})].
	\end{align*} 
	We recognise this as a $\mathcal{N}\qty(\frac{\sum x_i}{n + \tau^2}, \frac{1}{n + \tau^2})$ distribution.
	The Bayes estimator $\hat{\mu}^{(B)} = \frac{\sum x_i}{n + \tau^2}$ for both quadratic and absolute error loss.
	Whereas $\hat{\mu}^{(mle)} = \frac{\sum x_i}{n}$.

	A $95\%$ credible interval is 
	\begin{align*}
		(\hat{\mu}^{(B)} - \frac{1.96}{\sqrt{n + \tau^2}}, \hat{\mu}^{(B)} + \frac{1.96}{\sqrt{n + \tau^2}}).
	\end{align*}
	This is close to a $95\%$ confidence interval when $n >> \tau^2$.
\end{example} 

\begin{example}
	Let $X_1, \dots, X_n \sim \operatorname{Poi}(\lambda)$ iid.
	Our prior is $\pi(\lambda) \sim \operatorname{Exp}(1)$, i.e. $\pi(\lambda) = e^{-\lambda}$ where $\lambda > 0$.
	So for $\lambda > 0$,
	\begin{align*}
		\pi(\lambda \mid x) &\propto f_X(x \mid \lambda) \pi(\lambda) \\
		&\propto \frac{e^{-n \lambda} \lambda^{\sum x_i}}{\prod_i x_i!} e^{-\lambda} \\
		&\propto e^{-(n+1) \lambda} \lambda^{\sum x_i}.
	\end{align*} 
	This is a $\operatorname{Gamma}(\sum x_i + 1, n + 1)$ distribution.

	The Bayes estimator for quadratic loss is the posterior mean $\hat{\lambda}^{(B)} = \frac{\sum x_i + 1}{n + 1} \to \frac{\sum x_i}{n} = \hat{\lambda}^{(mle)}$ as $n \to \infty$.
	Under the absolute error loss the Bayes estimator $\hat{\lambda}^{(B)}$ is 
	\begin{align*}
		\int_{0}^{\hat{\lambda}^{(B)}} \frac{(n+1)^{\sum x_i - 1}}{\qty(\sum x_i)!} \lambda^{\sum x_i} e^{-(n+1) \lambda} \dd{\lambda} = \frac{1}{2}.
	\end{align*} 
\end{example} 