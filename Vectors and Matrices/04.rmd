# Matrices and Linear Maps

## Introduction

### Definitions

::: {.definition #lmp}
A *linear map* or *linear transformation* is a function
\begin{align*}
    T : V \to W
\end{align*} between vector spaces $V\ (\dim n)$ and $W\ (\dim m)$ such that 
\begin{align*}
    T(\lambda \underline{x} + \mu \underline{y}) &= \lambda T(\underline{x}) + \mu T(\underline{y}) \\
    \forall \; \underline{x}, \underline{y} \in V \\
    \forall \; \lambda, \mu \in \mathbb{R} \text{ or } \mathbb{C} \\
\end{align*} 
for $V, W$ both real or complex vector spaces.^[Mostly concerned with $V = \mathbb{R}^n,\ W = \mathbb{R}^m$ or $V = \mathbb{C}^n,\ W = \mathbb{C}^m$]
:::

*Note*: a linear map is completely determined by its action on a basis $\{ \underline{e}_1, \ldots, \underline{e}_n \}$ for $V$, since \begin{align*}
    T\left( \sum_i x_i \underline{e}_i \right) = \sum_i x_i T(\underline{e}_i)
\end{align*} 

$\underline{x}' = T(\underline{x}) \in W$ is the *image* of $\underline{x} \in V$ under T.\
$\operatorname{Im}(T) = \{ \underline{x}' \in W: \underline{x}' = T(\underline{x}) \text{ for some } \underline{x} \in V \}$ is the *image* of $T$.\
$\ker(T) = \{ \underline{x} \in V: \underline{x}' = T(\underline{x}) = \underline{0} \}$ is the *kernel* of $T$.\

::: {.lemma}
$\ker(T)$ is a subspace of $V$ and $\operatorname{Im}(T)$ is a subspace of $W$.
:::

::: {.proof}
$\underline{x}, \underline{y} \in \ker(T) \implies T(\lambda \underline{x} + \mu \underline{y}) = \lambda T(\underline{x}) + \mu T(\underline{y}) = \underline{0}$ and $\underline{0} \in \ker(T)$, so results follows.

Also $\underline{0} \in \operatorname{Im}(T)$ and $\underline{x}', \underline{y}' \in \operatorname{Im}(T)$ then $T(\lambda \underline{x} + \mu \underline{y}) = \lambda T(\underline{x}) + \mu T(\underline{y}) = \lambda \underline{x}' + \mu \underline{y}' \in \operatorname{Im}(T)$ for some $\underline{x}, \underline{y} \in V$.
:::

::: {.example #f-one}
Zero linear map $T : V \to W$ is given by $T(\underline{x}) = \underline{0} \; \forall \; \underline{x} \in V$.
$\operatorname{Im}(T) = \{ \underline{0} \}$ and $\ker(T) = V$
:::

::: {.example #f-two}
For $V = W$, the identity linear map $T: V \to V$ is given by $T(\underline{x}) = \underline{x} \; \forall \; x \in V$.
$\operatorname{Im}(T) = V$ and $\ker(T) = \{ \underline{0} \}$
:::

::: {.example #f-three}
$V = W = \mathbb{R}^3$, $\underline{x}' = T(\underline{x})$ given by
\begin{align*}
    x_1' &= 3 x_1 + x_2 + 5 x_3 \\
    x_2' &= - x_1 - 2 x_3 \\
    x_3' &= 2 x_1 + x_2 + 3 x_3 \\
    \ker(T) &= \left\{ \lambda \begin{pmatrix}2 \\-1 \\-1\end{pmatrix} \right\} \hspace{0.5cm} (\dim 1) \\
    \operatorname{Im}(T) &= \left\{ \lambda \begin{pmatrix}3 \\-1 \\2\end{pmatrix} + \mu \begin{pmatrix}1 \\0 \\1\end{pmatrix} \right\} \hspace{0.5cm} (\dim 2)
\end{align*} 
:::

### Rank and Nullity
$\dim \operatorname{Im}(T)$ is the *rank* of $T$ ($\leq n$).\
$\dim \ker(T)$ is the *nullity* of $T$ ($\leq n$).\

::: {.theorem name="rank-nullity"}
For $T : V \to W$ a linear map, \@ref(def:lmp)
\begin{align*}
    \operatorname{rank}(T) + \operatorname{null}(T) &= n = \dim V
\end{align*} 
:::


::: {.example #f-four}
same as those in [Definitions]

i. $\operatorname{rank}(T) + \operatorname{null}(T) = 0 + n = n$

ii. $\operatorname{rank}(T) + \operatorname{null}(T) = n + 0 = n$

iii. $\operatorname{rank}(T) + \operatorname{null}(T) = 2 + 1 = 3$
:::

Non-examinable

::: {.proof}
Let $\underline{e}_1, \ldots, \underline{e}_k$ be a basis for $\ker(T)$ so $T(\underline{e}_i) = 0$ for $i = 1, \ldots, k$.\
Extend by $\underline{e}_{k + 1}, \ldots, \underline{e}_n$ to get a basis for $V$. 
Claim 
\begin{align*}
    \mathcal{B} &= \{ T(\underline{e}_{k + 1}), \ldots, T(\underline{e}_n) \}
\end{align*} is the basis for $\operatorname{Im}(T)$.
The result then follows since $\operatorname{null}(T) = k$ and $\operatorname{rank}(T) = n - k$, implying $\operatorname{null}(T) + \operatorname{rank}(T) = n$.

To check claim:\
$\mathcal{B}$ spans $\operatorname{Im}(T)$ since $\underline{x} = \sum_{i=1}^{n} x_i \underline{e}_i$
\begin{align*}
    \implies T(\underline{x}) = \sum_{i=1}^{n} x_i T(\underline{e}_i) = \sum_{i = k + 1}^{n} x_i T(\underline{e}_i)
\end{align*} 
$\mathcal{B}$ is linearly independent since 
\begin{align*}
    \sum_{i=k+1}^{n} \lambda_i T(\underline{e}_i) &= \underline{0} \\
    \implies T(\sum_{i=k+1}^{n} \lambda_i \underline{e}_i) &= \underline{0} \\
    \implies \sum_{i=k+1}^{n} \lambda_i \underline{e}_i &\in \ker(T) \\
    \implies \sum_{i=k+1}^{n} \lambda_i \underline{e}_i &= \sum_{i=1}^{k} \mu_i \underline{e}_i \\
\end{align*} 
But $\underline{e}_1, \ldots, \underline{e}_n$ are linearly independent in $V$
\begin{align*}
    \implies \lambda_i &= 0 \\
    \mu_i &= 0
\end{align*} 
:::

## Geometrical Examples

### Rotations

In $\mathbb{R}^2$, a rotation about $\underline{0}$ through angle $\theta$ is defined by
\begin{align*}
    \underline{e}_1 &\mapsto \underline{e}_1' = \ \; (\cos \theta) \underline{e}_1 + (\sin \theta) \underline{e}_2 \\
    \underline{e}_2 &\mapsto \underline{e}_2' = -(\sin \theta) \underline{e}_1 + (\cos \theta) \underline{e}_2
\end{align*} 

```{r 04-rotations, echo = FALSE, fig.cap = "", fig.align="center", out.height = "300cm"} 
knitr::include_graphics("figures/04-rotations.png") 
```

In $\mathbb{R}^3$, rotation about axis given by $\underline{e}_3$ is defined as above, with
\begin{align*}
    \underline{e}_3 \mapsto \underline{e}_3' = \underline{e}_3
\end{align*} 

Now consider rotation about any axis $\underline{n}$ (a unit vector).
Given $\underline{x}$, resolve $\parallel$ and $\bot$ $\underline{n}$:
\begin{align*}
    \underline{x} &= \underline{x}_\parallel + \underline{x}_\bot \hspace{0.5cm} \text{with } \underline{x}_\parallel = (\underline{x} \cdot \underline{n}) \underline{n} \ (\implies \underline{n} \cdot \underline{x}_\bot = \underline{0}) \\
    \text{Under rotation} \\
    \underline{x}_\parallel &\mapsto \underline{x}_\parallel' = \underline{x}_\parallel \\
    \underline{x}_\bot &\mapsto \underline{x}_\bot' = (\cos \theta) \underline{x}_\bot + (\sin \theta) \underline{n} \wedge \underline{x},
\end{align*} by considering plane $\bot \underline{n}$, comparing to rotation in $\mathbb{R}^2$ and noting that $|\underline{x}_\bot| = | \underline{n} \wedge \underline{x} |$.

```{r 04-3d-rotation, echo = FALSE, fig.cap = "", fig.align="center", out.height = "300cm"} 
knitr::include_graphics("figures/04-3d-rotation.png") 
```

\begin{align*}
    \underline{x} \mapsto \underline{x}' &= \underline{x}_\parallel' + \underline{x}_\bot' \\
    &= (\cos \theta) \underline{x} + (1 - \cos \theta) (\underline{n} \cdot \underline{x}) \underline{n} + \sin \theta \underline{n} \wedge \underline{x}.
\end{align*} 

### Reflections

Consider a *reflection* in a plane in $\mathbb{R}^3$ (or line in $\mathbb{R}^2$) through $\underline{0}$ with unit normal $\underline{n}$.\
Given $\underline{x}$, resolve  resolve $\parallel$ and $\bot$ $\underline{n}$:
\begin{align*}
    \underline{x}_\parallel &\mapsto \underline{x}_\parallel' = -\underline{x}_\parallel \\
    \underline{x}_\bot &\mapsto \underline{x}_\bot' = \underline{x}_\bot
\end{align*}

```{r 04-reflection-3d, echo = FALSE, fig.cap = "", fig.align="center", out.height = "300cm"} 
knitr::include_graphics("figures/04-reflection-3d.png") 
```

\begin{align*}
    \underline{x} &\mapsto \underline{x}' = \underline{x} - 2 (\underline{x} \cdot \underline{n}) \underline{n} \\
\end{align*} 

### Dilations

A dilation by scale factors $\alpha, \beta, \gamma$ (real, $> 0$) along axes $\underline{e}_1, \underline{e}_2, \underline{e}_3$ in $\mathbb{R}^3$ is defined by
\begin{align*}
    \underline{x} &= x_1 \underline{e}_1 + x_2 \underline{e}_2 + x_3 \underline{e}_3 \\
    \mapsto \underline{x}' &= \alpha x_1 \underline{e}_1 + \beta x_2 \underline{e}_2 + \gamma x_3 \underline{e}_3.
\end{align*}

A dilation maps a unit cube to a cuboid.

### Shears

Given $\underline{a}, \underline{b}$ orthogonal unit vectors define a shear with parameter $\lambda$ by 
\begin{align*}
    \underline{x} &\mapsto \underline{x}' = \underline{x} + \lambda (\underline{x} \cdot \underline{b}) \underline{a}
\end{align*} 

```{r 04-shear, echo = FALSE, fig.cap = "", fig.align="center", out.height = "300cm"} 
knitr::include_graphics("figures/04-shear.png") 
```

Definition applies in $\mathbb{R}^n$ and $\underline{u}' = u$ for any vector $\underline{u} \bot \underline{b}$.

## Matrices as Linear Maps $\mathbb{R}^n \to \mathbb{R}^m$
### Definitions
Consider a linear map $T : \mathbb{R}^n \to \mathbb{R}^m$ and standard bases $\{ \underline{e}_i \}$ and $\{ \underline{f}_a \}$ respectively.\
Let $\underline{x}' = T(\underline{x})$ with 
\begin{align*}
    \underline{x} = \sum_i x_i \underline{e}_i = \begin{pmatrix}x_1 \\ \vdots \\ x_n\end{pmatrix},\ \underline{x}' = \sum_a x_a' \underline{f}_a = \begin{pmatrix}x_1' \\ \vdots \\ x_m'\end{pmatrix}
\end{align*} 
Linearity implies $T$ is determined by $T(\underline{e}_i) = \underline{e}_i' = \underline{C}_i \in \mathbb{R}^m \ (i = 1, \ldots, n)$; take these ase *columns* of a $m \times n$ *array* or *matrix* $M$ with *rows*
\begin{align*}
    \underline{R}_a \in \mathbb{R}^n \ (a = 1, \ldots, m).
\end{align*} 
$M$ has entries $M_{ai} \in \mathbb{R}$ where $a$ labels rows and $i$ labels columns.

\begin{align*}
    \begin{pmatrix}
    \uparrow &  & \uparrow \\
    \underline{C}_1 & \ldots & \underline{C}_n \\
    \downarrow &  & \downarrow
    \end{pmatrix} = M = 
    \begin{pmatrix}
    \leftarrow & \underline{R}_1 & \rightarrow \\
     & \vdots &  \\
    \leftarrow & \underline{R}_m & \rightarrow
    \end{pmatrix}
\end{align*} 

$(\underline{C}_i)_a = M_{ai} = (\underline{R}_a)_i$.

Action of $T$ is given by matrix $M$ multiplying vector $\underline{x}$,

::: {.bluebox}
$\underline{x}' = M \underline{x}$ defined by $x_a' = M_ai x_i$ ($\sum$ convention).
:::

This follows from definitions above since 
\begin{align*}
    \underline{x}' &= T\left( \sum_i x_i \underline{e}_i \right) = \sum_i x_i \underline{C}_i \\
    \implies (\underline{x}')_a &= \sum_i x_i (\underline{C}_i)_a \\
    &= \sum_i M_{ai} x_i \\
    &= \sum_i (\underline{R}_a) x_i \\
    &= \underline{R}_a \cdot x
\end{align*} 

Now regard properties of $T$ as properties of $M$.\
$\operatorname{Im}(T) = \operatorname{Im}(M) = \operatorname{span} \{ \underline{C}_1, \ldots, \underline{C}_n \}$, the *image of M* (or T) is the span of the columns.\
$\ker(T) = \ker(M) = \{\underline{x} : \underline{R}_a \cdot \underline{x} = 0 \; \forall \; a \}$, *kernel of M* is subspace $\bot$ all rows.

### Examples
Refer to Examples \@ref(exm:f-one), \@ref(exm:f-two), \@ref(exm:f-three), \@ref(exm:f-four).

::: {.example}
Zero map $\mathbb{R}^n \to \mathbb{R}^m$ corresponds to *zero matrix* $M = 0$ with $M_{ai} = 0$.
:::

::: {.example}
Identity map $\mathbb{R}^n \to \mathbb{R}^n$ corresponds to *identity matrix*
\begin{align*}
    M = I = \begin{pmatrix}
    1 &  &  & \smash{\huge 0} \\
     & 1 & &  \\
     &  & \ddots &  \\
    \huge 0 &  &  & 1
    \end{pmatrix}
\end{align*} with $I_{ij} = \delta_{ij}$.
:::

::: {.example}
$\mathbb{R}^3 \to \mathbb{R}^3$, $\underline{x}' = T(\underline{x}) = M \underline{x}$ with 
\begin{align*}
    M &= \begin{pmatrix}
    3 & 1 & 5 \\
    -1 & 0 & -2 \\
    2 & 1 & 3
    \end{pmatrix}, 
    \underline{C}_1 = \begin{pmatrix}3 \\-1 \\2\end{pmatrix}, 
    \underline{C}_2 = \begin{pmatrix}1 \\0 \\1\end{pmatrix}, 
    \underline{C}_3 = \begin{pmatrix}5 \\-2 \\3\end{pmatrix} \\
    \operatorname{T} &= \operatorname{M} \\
    &= \operatorname{span} \{ \underline{C}_1, \underline{C}_2, \underline{C}_3 \} \\
    &= \operatorname{span} \{ \underline{C}_1, \underline{C}_2 \} \text{ since } \underline{C}_3 = 2 \underline{C}_1 - \underline{C}_2 \\
    \underline{R}_1 &= \begin{pmatrix}3 & 1 & 5\end{pmatrix} \\
    \underline{R}_2 &= \begin{pmatrix}-1 & 0 & 2\end{pmatrix} \\
    \underline{R}_3 &= \begin{pmatrix}2 & 1 & 3\end{pmatrix} \\
    \underline{R}_2 \wedge \underline{R}_3 &= \begin{pmatrix}2 & -1 & -1\end{pmatrix} \\
    &= \underline{u}, \text{ say } \bot \text{ all rows (in fact)} \\
    \ker (T) &= \ker (M) = \{ \lambda \underline{u} \}
\end{align*} 
:::

::: {.example}
Rotation through $\theta$ about $\underline{0}$ in $\mathbb{R}^2$
\begin{align*}
    \underline{e}_1 &= \begin{pmatrix}1 \\0\end{pmatrix} \mapsto \begin{pmatrix} \cos \theta \\ \sin \theta\end{pmatrix} = \underline{C}_1 \\
    \underline{e}_2 &= \begin{pmatrix}0 \\1\end{pmatrix} \mapsto \begin{pmatrix}- \sin \theta \\ \cos \theta\end{pmatrix} = \underline{C}_2 \\
    \implies M &= \begin{pmatrix}
    \cos \theta & - \sin \theta \\
    \sin \theta & \cos \theta
    \end{pmatrix}.
\end{align*} 
:::

::: {.example}
Dilation $\underline{x}' = M \underline{x}$ with scale factors $\alpha, \beta, \gamma$ along axes in $\mathbb{R}^3$.
\begin{align*}
    M = \begin{pmatrix}
    \alpha & 0 & 0 \\
    0 & \beta & 0 \\
    0 & 0 & \gamma
    \end{pmatrix}.
\end{align*} 
:::

::: {.example #reflection-matrix}
Reflection in plane $\bot \underline{n}$ (a unit vector).
\begin{align*}
    \underline{x}' &= H \underline{x} = \underline{x} - 2 (\underline{x} \cdot \underline{n}) \underline{n} \\
    x_i' &= x_i - 2 x_j n_j n_i \\
    &= (\delta_{ij} - 2 n_j n_i) x_j \\
    H_{ij} &= \delta_{ij} - 2 n_j n_i \\
    \text{e.g. } \underline{n} &= \frac{1}{\sqrt{3}} \begin{pmatrix} \\1 \\1\end{pmatrix},\ n_i n_j = \frac{1}{3} \; \forall \; i, j \\
    H &= \frac{1}{3} \begin{pmatrix}
    1 & -2 & -2 \\
    -2 & 1 & -2 \\
    -2 & -2 & 1
    \end{pmatrix}
\end{align*} 
:::

::: {.example}
Shear
\begin{align*}
    \underline{x}' &= S \underline{x} = \underline{x} + \lambda (\underline{b} \cdot \underline{x})\underline{a} \\
    x_i' &= S_{ij} x_j \\
    \text{with } S_{ij} &= \delta_{ij} + \lambda a_i b_j
\end{align*} 
e.g. in $\mathbb{R}^2$ with $\underline{a} = \begin{pmatrix}1 \\0\end{pmatrix}$ and $\underline{b} = \begin{pmatrix}0 \\1\end{pmatrix}$, 
\begin{align*}
    S = \begin{pmatrix}
    1 & \lambda \\
    0 & 1
    \end{pmatrix}.
\end{align*} 
:::

::: {.example}
Rotation in $\mathbb{R}^3$ with axis $\underline{n}$ and angle $\theta$,
\begin{align*}
    \underline{x}' &= R \underline{x} \hspace{1cm} x_i' = R_{ij} x_j \\
    \text{where } R_{ij} = \delta_{ij} \cos \theta + (1 - \cos \theta) n_i n_j - (\sin \theta) \epsilon_{ijk} n_k
\end{align*} (see Example Sheet 2).
:::

### Isometries, area and determinants in $\mathbb{R}^2$

Consider a linear map $\mathbb{R}^2 \to \mathbb{R}^2$ given by $2 \times 2$ matrix $M$:
\begin{align*}
    \underline{x} \mapsto \underline{x}' = M \underline{x}
\end{align*} 

i. When is $M$ an *isometry*, preserving lengths $|\underline{x}'| = |\underline{x}|$?

This is equivalent to preserving inner products $\underline{x}' \cdot \underline{y}' = \underline{x} \cdot \underline{y}$ [since $\underline{x} \cdot \underline{y} = \frac{1}{2} ( |\underline{x} + \underline{y}|^2 - |\underline{x}|^2 - |\underline{y}|^2)$].\
Necessary conditions are
\begin{align*}
    M \begin{pmatrix}
    1 \\
    0
    \end{pmatrix} &= \begin{pmatrix}\cos \theta \\\sin \theta\end{pmatrix} \text{ for some $\theta$; most general unit vector in $\mathbb{R}^2$} \\
    M \begin{pmatrix}0 \\1\end{pmatrix} = \pm \begin{pmatrix}- \sin \theta \\ \cos \theta\end{pmatrix} \text{ general unit vector $\bot M (1\ 0)^T$}.
\end{align*}
Simple to check that these conditions are also sufficient and have two cases
\begin{align*}
    M = R = \begin{pmatrix}
    \cos \theta & - \sin \theta \\
    \sin \theta & \cos \theta
    \end{pmatrix}
\end{align*}, a *rotation*
or \begin{align*}
    M = H = \begin{pmatrix}
    \cos \theta & \sin \theta \\
    \sin \theta & - \cos \theta
    \end{pmatrix}
\end{align*}, a *reflection*.

Compare with expression for reflection in \@ref(exm:reflection-matrix)

\begin{align*}
    H_{ij} &= \delta_{ij} - 2 n_i n_j \\
    \text{and note for } \underline{n} &= \begin{pmatrix}n_1 \\n_2\end{pmatrix} = \begin{pmatrix} -\sin \frac{\theta}{2} \\ \cos \frac{\theta}{}2\end{pmatrix} \\
    \text{we get } H &= \begin{pmatrix}
    1 - 2 \sin^2 \frac{\theta}{2} & 2 \sin \frac{\theta}{2} \cos \frac{\theta}{2} \\
    2 \sin \frac{\theta}{2} \cos \frac{\theta}{2} & 1 - 2 \cos^2 \frac{\theta}{2}
    \end{pmatrix}
\end{align*} agreeing with H above.
This is reflection in a line in $\mathbb{R}^2$ as shown

```{r 04-isometry, echo = FALSE, fig.cap = "", fig.align="center", out.height=300} 
knitr::include_graphics("figures/04-isometry.png") 
```

ii. How does $M$ change *areas* in $\mathbb{R}^2$ (in general)?

Consider unit square in $\mathbb{R}^2$, mapped to parallelogram as shown, with signed area $[M \underline{e}_1, M \underline{e}_1]$ "scalar cross product".

```{r 04-area, echo = FALSE, fig.cap = "", fig.align="center", out.height=300} 
knitr::include_graphics("figures/04-area.png") 
```

\begin{align*}
    \left[ \begin{pmatrix} M_{11} \\M_{21}\end{pmatrix}, \begin{pmatrix}M_{12} \\M_{22}\end{pmatrix} \right] &= M_{11} M_{22} - M_{12} M_{21} \\
    &= \det M 
\end{align*}, the determinant of $2 \times 2$ matrix.

This is the factor (with a sign) by which areas are scaled under $M$.

Now compare with (i):
\begin{align*}
    \det R = + 1,\ \det H = -1.
\end{align*}
In either case $| \det M | = + 1$.
Consider shear $S = \begin{pmatrix}1 & \lambda \\0 & 1\end{pmatrix}$; this has $\det S = + 1$ but it does not preserve lengths.

## Matrices for Linear Maps in General

Consider a linear map
\begin{align*}
    T : V \to W
\end{align*} between real or complex vector spaces of $\dim n, m$ respectively and choose bases $\{ \underline{e}_i \}$ with $i = 1, \ldots, n$ for $V$ and $\{ \underline{f}_a \}$ with $a = 1, \ldots, m$ for $W$.\
The matrix $M$ for $T$ wrt the bases in an $m \times n$ array with entries $M_{ai} \in \mathbb{R}$ or $\mathbb{C}$.
It is defined by 
\begin{align*}
    T(\underline{e}_i) &= \sum_a \underline{f}_a M_{ai} \text{ note index positions}.
\end{align*}
This is chose to ensure that $T(\underline{x}) = \underline{x}'$ where $\underline{x} = \sum_i x_i \underline{e}_i$ and $\underline{x}' = \sum_a x_a' \underline{f}_a$ iff $x_a' = \sum_i M_{ai} x_i$.\
\begin{align*}
    \text{i.e. } \begin{pmatrix}x_1' \\ \vdots \\ x_m'\end{pmatrix} = \begin{pmatrix}
    M_{11} & \ldots & M_{1n} \\
    \vdots &  & \vdots \\
    M_{m1} & \ldots & M_{mn} 
    \end{pmatrix} 
    \begin{pmatrix}x_1 \\ \vdots \\ x_n\end{pmatrix}.
\end{align*} 

*Moral*: given choice of bases $\{ \underline{e}_i \}$ and $\{ \underline{f}_a \}$\
$V$ is identified with $\mathbb{R}^n$ (or $\mathbb{C}^n$)\
$W$ is identified with $\mathbb{R}^m$ (or $\mathbb{C}^m$)\
$T$ is identified with $m \times n$ matrix $M$.

*Note*: there are natural ways to combine linear maps.\
If $S : V \to W$ is also linear, then so is $\alpha T + \beta S : V \to W$ defined by $(\alpha T + \beta S) (\underline{x}) = \alpha T(\underline{x}) + \beta S(\underline{x})$.\
Or if $S : U \to V$ is also linear, then so is $T \circ S : U \to W$ (composition of maps).

## Matrix Algebra

### Linear Combinations

If $M$ and $N$ are $m \times n$ matrices then $\alpha M + \beta N$ is an $m \times n$ matrix defined by 
\begin{align*}
    (\alpha M + \beta N)_{ai} &= \alpha M_{ai} + \beta N_{ai} \\
    (a = 1, \ldots, m&; i = 1, \ldots, n)
\end{align*} [If $M, N$ represent linear maps $T, S : V \to W$, then $\alpha M + \beta N$ represents $\alpha T + \beta S$, all w.r.t. same choice of bases].

### Matrix multiplication

If $A$ is an $m \times n$ matrix, entries $A_{ai} \in \mathbb{R}$ or $\mathbb{C}$\
and $B$ is an  $n \times p$ matrix, entries $B_{ir}$ then $AB$ is an $m \times p$ matrix defined by $(AB)_{ar} = A_{ai} B_{ir}$ ($\sum$ convention).
The product $AB$ is not defined unless no. of cols of $A = $ no. of rows of B.
\begin{align*}
    a &= 1, \ldots, m \\
    i &= 1, \ldots, n \\
    r &= 1, \ldots, p.
\end{align*} 

Matrix multiplication corresponds to the composition of linear maps.
\begin{align*}
    [(AB) \underline{x}]_a &= (AB)_{ar} x_r \text{ and compare} \\
    [A(B \underline{x})]_a &= A_{ai}(B \underline{x})_i = A_{ai} (B_{ir} x_r) \\
    &= (A_{ai} B_{ir}) x_r 
\end{align*} 

::: {.example}
\begin{align*}
    A &= \begin{pmatrix}1 & 3 \\-5 & 0 \\2 & 1\end{pmatrix},\ B = \begin{pmatrix}1 & 0 & -1 \\2 & -1 & 3\end{pmatrix} \\
    AB &= \begin{pmatrix}
    7 & -3 & 8 \\
    -5 & 0 & 5 \\
    4 & -1 & 1
    \end{pmatrix} \\
    BA &= \begin{pmatrix}
    -1 & 2 \\
    13 & 9
    \end{pmatrix}
\end{align*} 
:::

### Helpful points of view

i. Regarding $\underline{x} \in \mathbb{R}^n$ as a col vec or $n \times 1$ matrix, definition on matrix multiplication a matrix or vector agree.

ii. For product of $\underbrace{A}_{m \times n} \underbrace{B}_{n \times p}$ have columns $\underline{C}_r (B) \in \mathbb{R}^n$ and $\underline{C}_r (AB) \in \mathbb{R}^m$ related by $\underline{C}_r (AB) = A \underline{C}_r (B)$