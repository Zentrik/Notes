# Determinants and inverses

## Introduction

Consider a linear map 
\begin{align*}
    T : \mathbb{R}^n \to \mathbb{R}^n.
\end{align*} 
If $T$ is invertible then 
\begin{align*}
    \underbrace{\ker T = \{ \underline{0} \}}_\text{because $T$ is one-to-one} \text{ and } \underbrace{\operatorname{Im} T = \mathbb{R}^n}_\text{$T$ is onto}.
\end{align*}
These conditions are equivalent by rank-nullity theorem \@ref(thm:rank).
Conversely, if these conditions hold, then 
\begin{align*}
    \underline{e}_1' &= T(\underline{e}_1), \ldots, \underline{e}_n' = T(\underline{e}_n) \\
\end{align*} is a basis (where $\{ \underline{e}_i \}$ is the standard basis) and we can define a linear map $T^{-1}$ by $T^{^-1}(\underline{e}_1') = \underline{e}_1, \ldots, T^{-1}(\underline{e}_n') = \underline{e}_n$.

How can we test whether the conditions holds from matrix $M$ representing $T$: $T(\underline{x}) = M \underline{x}$ and how can we find $M^{-1}$ when they do hold?

For any $M$ ($n \times n$) we will define a related matrix $\widetilde{M}$ ($n \times n$) and a scalar, the *determinant* $\det M$ or $| M |$ such that 
\begin{align}
    \widetilde{M} M = (\det M )I (\#eq:inverse)
\end{align}
Then if $\det M \neq 0$, $M$ is invertible with 
\begin{align*}
    M^{-1} = \frac{1}{\det M} \widetilde{M}.
\end{align*} 

*For $n = 2$* we found in [Matrix Inverses] that \@ref(eq:inverse) holds with 
\begin{align*}
    M &= \begin{pmatrix}
    M_{11} & M_{12} \\
    M_{21} & M_{22}
    \end{pmatrix} \text{ and } \widetilde{M} = \begin{pmatrix}
    M_{22} & -M_{12} \\
    -M_{21} & M_{11}
    \end{pmatrix} \\
    \det M &= \begin{vmatrix}
    M_{11} & M_{12} \\
    M_{21} & M_{22}
    \end{vmatrix} = M_{11} M_{22} - M_{12} M_{21} \\
    &= [M \underline{e}_1, M \underline{e}_2] \\
    &= [\underline{C}_1 (M), \underline{C}_2 (M)] \\
    &= \epsilon_{ijk} M_{i1} M_{j2} \\
    \text{The factor by which areas are scaled under $M$} \\
    \det M \neq 0 &\iff \{ M \underline{e}_1, M \underline{e}_2 \} \text{ are linearly independent} \\
    &\iff \operatorname{Im}(M) = \mathbb{R}^2 \\
\end{align*} 

*For $n = 3$* consider similarly
\begin{align*}
    [M \underline{e}_1, M \underline{e}_2, M \underline{e}_3] \text{ (scalar triple product)} \\
    &= [\underline{C}_1 (M), \underline{C}_2 (M), \underline{C}_3 (M)] \\
    &= \epsilon_{ijk} M_{i1} M_{j2} M_{k3} \\
    &= \det M, \text{ defn for $n = 3$}. \\
    \text{This is the factor by which volumes are scaled under $M$ and} \\
    \det M \neq 0 &\iff  [M \underline{e}_1, M \underline{e}_2, M \underline{e}_3] \text{ are linearly independent} \\
    &\iff \operatorname{Im}(M) = \mathbb{R}^3 \\
\end{align*} 

Now define $\widetilde{M}$ from $M$ using row/ column notation 
\begin{align*}
    \underline{R}_1 (\widetilde{M}) &= \underline{C}_2 (M) \wedge \underline{C}_3 (M) \\
    \underline{R}_2 (\widetilde{M}) &= \underline{C}_3 (M) \wedge \underline{C}_1 (M) \\
    \underline{R}_3 (\widetilde{M}) &= \underline{C}_1 (M) \wedge \underline{C}_2 (M) \\
    \text{and note that} \\
    (\widetilde{M} M)_{ij} &= \underline{R}_i (\widetilde{M}) \cdot \underline{C}_j (M) \\
    &= \underbrace{(\underline{C}_1 (M) \cdot \underline{C}_2 (M) \wedge \underline{C}_3 (M))}_{\det M} \delta_{ij}
\end{align*} 

::: {.example}
\begin{align*}
    M &= \begin{pmatrix}
    1 & 3 & 0 \\
    0 & -1 & 2 \\
    4 & 1 & -1
    \end{pmatrix} \\
    \underline{C}_2 \wedge \underline{C}_3 &= \begin{pmatrix}3 \\-1 \\1\end{pmatrix} \wedge \begin{pmatrix}
    0 \\
    2 \\
    -1
    \end{pmatrix} = \begin{pmatrix}
    -1 \\
    3 \\
    6
    \end{pmatrix} \\
    \underline{C}_3 \wedge \underline{C}_1 &= \begin{pmatrix}
    0 \\
    2 \\
    -1
    \end{pmatrix} \wedge \begin{pmatrix}1 \\0 \\4\end{pmatrix} = \begin{pmatrix}
    8 \\
    -1 \\
    -2
    \end{pmatrix} \\
    \underline{C}_1 \wedge \underline{C}_2 &= \begin{pmatrix}
    1 \\
    0 \\
    4
    \end{pmatrix} \wedge \begin{pmatrix}3 \\-1 \\1\end{pmatrix} = \begin{pmatrix}
    4 \\
    11 \\
    -1
    \end{pmatrix} \\
    \widetilde{M} &= \begin{pmatrix}
    -1 & 3 & 6 \\
    8 & -1 & -2 \\
    4 & 11 & -1
    \end{pmatrix} \\
    \widetilde{M} M &= (\det M) I \text{ where} \\
    \det M &= \underline{C}_1 \cdot \underline{C}_2 \wedge \underline{C}_3 = 23.
\end{align*} 
:::

## $\epsilon$ and Alternating Forms

### $\epsilon$ and Permutation

Recall: a permutation $\sigma$ on the set $\{1, 2, \ldots, n \}$ is a bijection from this set to itself, specified by list $\sigma(1), \sigma(2), \sigma(n)$.
Permutation $\sigma$ form a group, the symmetric group $S_n$ of order $n!$. 
The *sign* or *signature* $\epsilon(\sigma) = (-1)^k$ where $k$ is the number of transpositions (two cycles, this is well defined).
The *alternating* or $\epsilon$ symbol in $\mathbb{R}^n$ or $\mathbb{C}^n$ is defined by
\begin{align*}
    \epsilon_{ij \ldots l} &= \begin{cases}
        +1 & \text{if } i, j, \ldots, l \text{ is an even permutation} \\
        -1 & \text{if } i, j, \ldots, l \text{ is an odd permutation} \\
        0 & \text{else}
    \end{cases} \\
    \epsilon(\sigma) &= (-1)^k \text{ with $\sigma$ product of $k$ transpositions} \\
    &= \pm 1.
\end{align*} 
If $\sigma$ is any permutation of $1, 2, \ldots, n$ then 
\begin{align*}
    \epsilon_{\sigma(1) \sigma(2) \ldots \sigma(n)} &= \epsilon(\sigma).
\end{align*} 

::: {.lemma}
\begin{align*}
    \epsilon_{\sigma(i) \sigma(j) \ldots \sigma(l)} = \epsilon(\sigma) \epsilon_{ij \ldots l}
\end{align*} 
$\epsilon$ is totally antisymmetric.
:::

::: {.proof}
If $i, j, \ldots l$ is *not* a permutation of $1, 2, \ldots, n$ then $RHS = LHS = 0$. \
If $i = \rho(1), j = \rho(2), \ldots, l = \rho(n)$ for some permutation $\rho$ then
\begin{align*}
    RHS &= \epsilon(\sigma) \epsilon (\rho) = \epsilon (\sigma \rho) = LHS.
\end{align*} 
(???)
:::

### Alternating Forms and Linear (In)dependence
 
Given $\underline{v}_1, \ldots, \underline{v}_n \in \mathbb{R}^n$ or $\mathbb{C}^n$ (these are $n$ vectors) the *alternating form* combines them to produce a scalar, defined by 
\begin{align*}
    [\underline{v}_1, \underline{v}_2, \ldots, \underline{v}_n] &= \epsilon_{ij \ldots l}(\underline{v}_1)_i (\underline{v}_2)_j \ldots (\underline{v}_n)_l \ \left(\sum \text{ convention}\right) \\
    &= \sum_\sigma \epsilon(\sigma) (\underline{v}_1)_{\sigma(1)} (\underline{v}_2)_{\sigma(2)} \ldots (\underline{v}_n)_{\sigma(n)} \ \left[\sum_\sigma \text{ means sum over all } \sigma \in S_n \right]
\end{align*} 

#### Properties
i. *Multilinear* 
\begin{align*}
    [\underline{v}_1, \ldots, \underline{v}_{p-1}, \alpha \underline{u} + \beta \underline{w}, \underline{v}_{p+1}, \underline{v}_n] &= \alpha [\underline{v}_1, \ldots, \underline{v}_{p-1}, \underline{u}, \underline{v}_{p+1}, \underline{v}_n] + \beta [\underline{v}_1, \ldots, \underline{w}, \underline{v}_{p+1}, \underline{v}_n]
\end{align*} 

ii. *Totally antisymmetric*
\begin{align*}
    [\underline{v}_{\sigma(1)}, \underline{v}_{\sigma(2)}, \ldots \underline{v}_{\sigma(n)}] &= \epsilon (\sigma) [\underline{v}_1, \underline{v}_2, \ldots, \underline{v}_n] 
\end{align*} 

iii. \begin{align*}
    [\underline{e}_1, \underline{e}_2, \ldots, \underline{e}_n] = 1
\end{align*} for $\underline{e}_i$ standard basis vectors.

Properties i, ii, iii fix the alternating form, and they also imply iv

iv. If $\underline{v}_p = \underline{v}_q$ for some $p \neq q$ then 
\begin{align*}
    [\underline{v}_1, \ldots, \underline{v}_p, \ldots, \underline{v}_q, \ldots, \underline{v}_n] &= 0
\end{align*} (from ii, exchanging $\underline{v}_p \leftrightarrow \underline{v}_q$ changes sign of alternating from).

v. If $\underline{v}_p = \sum_{i \neq p} \lambda_i \underline{v}$ then 
\begin{align*}
    [\underline{v}_1, \ldots, \underline{v}_p, \ldots, \underline{v}_n] &= 0
\end{align*} (sub in and use i and iv).

::: {.example #fone}
\begin{align*} \require{cancel}
    \text{In } \mathbb{C}^4, \underline{v}_1 &= \begin{pmatrix}i \\0 \\0 \\2\end{pmatrix}, \underline{v}_2 = \begin{pmatrix}0 \\0 \\5i \\0\end{pmatrix}, \\
    \underline{v}_3 &= \begin{pmatrix}3 \\2i \\0 \\0\end{pmatrix}, \begin{pmatrix}0 \\0 \\-i \\1\end{pmatrix} \\
    \implies [\underline{v}_1, \underline{v}_2, \underline{v}_3, \underline{v}_4] &= 5i [\underline{v}_1, \underline{e}_3, \underline{v}_3, \underline{v}_4] \\
    &= 5i [i \underline{e}_1 + \cancel{2 \underline{e}_4}, \underline{e}_3, \cancel{3 \underline{e}_1} + 2i \underline{e}_2, \cancel{-i \underline{e}_3} + \underline{e}_4] \\
    &= 5i \cdot i \cdot 2i) [\underline{e}_1, \underline{e}_3, \underline{e}_2, \underline{e}_4] \\
    &= (- 10 i) \cdot (-1) \\
    &= 10i
\end{align*} (in cancelling, we first cancel $i \underline{e}_3$ as there is another lone $\underline{e}_3$, then the $2\underline{e}_4$ and finally the $3\underline{e}_1$)
:::

Note: properties i and iii follow immediately from definition

::: {.proof name="Property ii"}
\begin{align*}
    [\underline{v}_{\sigma(1)}, \underline{v}_{\sigma(2)}, \ldots \underline{v}_{\sigma(n)}] &= \sum_\rho \epsilon (\rho) \underbrace{[\underline{v}_{\sigma(1)}]_{\rho(1)} \ldots [\underline{v}_{\sigma(n)}]_{\rho(n)}}_\text{each term can be re-written as: } \text{ ($\sigma$ fixed)} \\
    & [\underline{v}_1]_{\rho \sigma^{-1} (1)} \ldots [\underline{v}_n]_{\rho \sigma^{-1} (n)} \\
    &= \sum_\rho \epsilon (\sigma) \epsilon(\rho') [\underline{v}_1]_{\rho'(1)} \ldots [\underline{v}_n]_{\rho'(n)} \text{ where } \rho' = \rho \sigma^{-1} \\
    \text{ and } \sum_\rho &\text{ is equivalent to } \sum_{\rho'} \\
    &= \epsilon(\sigma) [\underline{v}_1, \underline{v}_2, \ldots, \underline{v}_n]
\end{align*} 
:::

::: {.proposition}
\begin{align*}
    [\underline{v}_1, \underline{v}_2, \ldots, \underline{v}_n] \neq 0 \iff \underline{v}_1, \ldots, \underline{v}_n \text{ are linearly independent}.
\end{align*} 
:::

::: {.proof}
$\implies$: \
Use property v.
If $\underline{v}_1, \ldots \underline{v}_n$ are linearly dependent then $\sum \alpha_i \underline{v}_i = \underline{0}$ where not all coefficients are zero.
Suppose wlog that $\alpha_p \neq 0$, then express $\underline{v}_p$ as a linear combination of $\underline{v}_i (i \neq p)$ and so
\begin{align*}
    [\underline{v}_1, \underline{v}_2, \ldots, \underline{v}_n] = 0.
\end{align*} 

$\Longleftarrow$: \
Note that $\underline{v}_1, \ldots, \underline{v}_n$ being linearly independent means that they span (in $\mathbb{R}^n$ or $\mathbb{C}^n$) so we can write the standard basis vectors as 
\begin{align*}
    \underline{e}_i &= A_{ai} \underline{v}_a \text{ for some } A_{ai} \in \mathbb{R} \text{ or } \mathbb{C}. \\
    \text{But then} & \\
    [\underline{e_1}, \ldots, \underline{e}_n] &= [A_{a1} \underline{v}_a, A_{b2} \underline{v}_b, \ldots, A_{cn} \underline{v}_c] \\
    &= A_{a1} A_{b2} \ldots A_{cn} [\underline{v}_a, \underline{v}_b, \ldots, \underline{v}_c] \\
    &= A_{a1} A_{b2} \ldots A_{cn} \epsilon_{ab \ldots c} [\underline{v}_1, \underline{v}_2, \ldots, \underline{v}_n] \text{ from property ii} \\
    LHS &= 1 \implies [\underline{v}_1, \underline{v}_2, \ldots, \underline{v}_n] \neq 0.
\end{align*}
:::

Example \@ref(exm:fone) are linearly independent.

## Determinants in $\mathbb{R}^n$ and $\mathbb{C}^n$

### Definition

:::{.definition name="Determinant"}
For an $n \times n$ matrix $M$ with columns
\begin{align*}
    \underline{C}_a &= M \underline{e}_a
\end{align*} 
the *determinant* $\det M$ or $|M| \in \mathbb{R}$ or $\mathbb{C}$ if defined by 
\begin{align*}
    \det M &= [\underline{C}_1, \underline{C}_2, \ldots, \underline{C}_n] \\
    &= [M \underline{e}_1, M \underline{e}_2, \ldots, M \underline{e}_n] \\
    &= \epsilon_{ij \ldots l} M_{i1} M_{j2} \ldots M_{ln} \\
    &= \sum_\sigma \epsilon(\sigma) M_{\sigma(1) 1} M_{\sigma(2) 2} \ldots M_{\sigma(n) n}
\end{align*} 
:::

::: {.proposition name="Transpose Property"}
\begin{align*}
    \det M &= \det M^T \\
    \text{so } \det M &= [\underline{R}_1, \underline{R}_2, \ldots, \underline{R}_n] \\
    &= \epsilon_{ij \ldots l} M_{1i} M_{2j} \ldots M_{nl} \\
    &= \sum_\sigma \epsilon(\sigma) M_{1 \sigma(1)} M_{2 \sigma(2)} \ldots M_{n \sigma(n)}
\end{align*} 
:::

::: {.example}
In $\mathbb{R}^3$ or $\mathbb{C}^3$
\begin{align*}
    \det M &= \epsilon_{ijk} M_{i1} M_{j2} M_{k3} \\
    &= M_{11} \begin{bmatrix}
    M_{22} & M_{23} \\
    M_{32} & M_{33}
    \end{bmatrix} - M_{21} \begin{bmatrix}
    M_{12} & M_{13} \\
    M_{32} & M_{33}
    \end{bmatrix} + M_{31} \begin{bmatrix}
    M_{12} & M_{13} \\
    M_{22} & M_{23}
    \end{bmatrix}
\end{align*} 
:::

::: {.properties}
$\det M$ is a function of rows or columns of $M$ that is 

i. multilinear

ii. totally anti-symmetric (or alternating)

iii. $\det I = 1$
:::

::: {.theorem}
$\det M \neq 0 \iff$ columns of $M$ are linearly independent.
$\iff$ row of $M$ are linearly independent
$\iff \operatorname{rank} M = n \ (M n \times n)$

\begin{align*}
    \det M \neq 0 &\iff \text{ columns of $M$ are linearly independent} \\
    &\iff \text{ row of $M$ are linearly independent} \\
    &\iff \operatorname{rank} M = n \ (M n \times n) \\
    &\iff \ker M = \{ \underline{0} \} \\
    &\iff M^{-1} \text{ exists}
\end{align*} 
:::

::: {.proof}
All equivalences follow immediately from earlier results including the discussion in [Introduction].
:::

::: {.proof name="Transpose property"}
Suffices to show 
\begin{align*}
    \sum_\sigma \epsilon(\sigma) M_{\sigma(1) 1} \ldots M_{\sigma(n) n} &= \sum_\sigma \epsilon(\sigma) M_{1 \sigma(1)} \ldots M_{n \sigma(n)}
\end{align*}
But in a given term on LHS
\begin{align*}
    M_{\sigma(1) 1} \ldots M_{\sigma(n) n} &= M_{1 \rho(1)} \ldots M_{n \rho(n)}
\end{align*} by reordering factors, where $\rho = \sigma^{-1}$.
Then $\epsilon(\sigma) = \epsilon(\rho)$ and $\sum_\sigma$ is equivalent to $\sum_\rho$, so result follows.
:::

### Evaluating determinants: expanding by rows or columns

For $M$, $n \times n$, for each entry $M_{ia}$ define the *minor* $M^{ia}$ to be the determinant of $(n - 1) \times (n - 1)$ matrix obtained from deleting row $i$ and column $a$ from $M$.

::: {.proposition}
\begin{align*}
    \det M &= \sum_i (-1)^{i + a} M_{ia} M^{ia} \hspace{.5cm} a \text{ fixed} \\
    &= \sum_a (-1)^{i + a} M_{ia} M^{ia} \hspace{.5cm} i \text{ fixed}
\end{align*} called expanding by (or about) column $a$ or row $i$ respectively.
:::

::: {.proof}
See section 5.4
:::

::: {.example}
\begin{align*}
    M &= \begin{pmatrix}
    i & 0 & 3 & 0 \\
    0 & 0 & 2i & 0 \\
    0 & 5i & 0 & -i \\
    2 & 0 & 0 & 1
    \end{pmatrix}.
\end{align*} Expand by row $3$ to find 
\begin{align*}
    \det M &= \sum_a (-1)^{3 + a} M_{3a} M^{3a} \\
    M_{31} &= M_{33} = 0 \\
    M_{32} &= 5i,\ M^{32} = \begin{vmatrix}
    i & 3 & 0 \\
    0 & 2i & 0 \\
    2 & 0 & 1
    \end{vmatrix} \\
    M_{34} &= -i, M^{34} = \begin{vmatrix}
    i & 0 & 3 \\
    0 & 0 & 2i \\
    2 & 0 & 0
    \end{vmatrix} \\
    M^{32} &= i \begin{vmatrix}2i & 0 \\0 & 1\end{vmatrix} - 3 \begin{vmatrix}
    0 & 0 \\
    2 & 1
    \end{vmatrix} \text{ (row 1)} \\
    &= i (2i) \\
    &= -2 \\
    M_{34} &= i \begin{vmatrix}
    0 & 2i \\
    0 & 0
    \end{vmatrix} + 3 \begin{vmatrix}
    0 & 0 \\
    2 & 0
    \end{vmatrix} \text{ (row 1)}\\
    &= 0 \\
    \det M &= (-1)^{3 + 2} 5i (-2) \\
    &= 10i \\
    \emph{or } \text{expand by column 2}: \\
    \det M &= \sum_i (-1)^{2 + i} M_{i2} M^{i2} \\
    &= (-1)^{2 + 3} M_{32} M^{32} \\
    &= 10i.
\end{align*} 
(Calculated this previously as example of alternating form in $\mathbb{C}^4$)
:::

::: {.lemma #block}
If $M = \left(
    \begin{array}{c|c} 
      A & O \\ 
      \hline 
      O & I 
    \end{array} \right)$ 
block form with $A$ a $r \times r$ matrix; $I \ (n - r) \times (n - r)$ identity, then $\det M = \det A$.
:::

::: {.proof}
For $r = 1$, result follows by expanding about column $n$ or row $n$, and for $r > 1$, continue process.
:::

### Simplifying determinants: Row and Column Operations:

From the definitions of $\det M$ in terms of columns (a) or rows (i) and the properties above (including [Alternating Forms and Linear (In)dependence]) we note the following

#### Row or column Scalings

If $\underline{R}_i \mapsto \lambda \underline{R}_i$ for some (fixed) $i$ \
or $\underline{C}_i \mapsto \lambda \underline{C}_i$ for some (fixed) $a$ \
then $\det M \mapsto \lambda \det M$.

If *all* rows or columns are scaled
\begin{align*}
    M &\mapsto \lambda M, \\
    \text{then } 
\end{align*} 

#### Row or column Operations

If $\underline{R}_i \mapsto \underline{R}_i + \lambda \underline{R}_j$ for $i \neq j$ \
or $\underline{C}_a \mapsto \underline{C}_a + \lambda \underline{C}_b$ for $a \neq b$ \
then $\det M \mapsto \det M$ as we can use multilinearity and then we have two rows being the same in one term so its 0.

#### Row or Column Exchanges

\begin{align*}
    \text{If } \underline{R}_i &\leftrightarrow \underline{R}_j \hspace{0.5cm} \text{for } i \neq j \\
    \text{If } \underline{C}_a &\leftrightarrow \underline{C}_b \hspace{0.5cm} \text{for } a \neq b \\
    \text{then } \det M &\mapsto -\det M.
\end{align*} 

::: {.example}
\begin{align*}
    A &= \begin{pmatrix}
    1 & 1 & a \\
    a & 1 & 1 \\
    1 & a & 1
    \end{pmatrix},\ a \in \mathbb{C} \\
    \underline{C}_1 &\mapsto \underline{C}_1 - \underline{C}_3 \\
    \det A &= \det \begin{pmatrix}
    1 - a & 1 & a \\
    a - 1 & 1 & 1 \\
    0 & a & 1
    \end{pmatrix} \\
    &= (1 - a) \det \begin{pmatrix}
    1 & 1 & a \\
    -1 & 1 & 1 \\
    0 & a & 1
    \end{pmatrix} \\
    \underline{C}_2 &\mapsto \underline{C}_2 - \underline{C}_3 \\
    \det A &= (1 - a) \det \begin{pmatrix}
    1 & 1-a & a \\
    -1 & 0 & 1 \\
    0 & a-1 & 1
    \end{pmatrix} \\
    &= (1 - a)^2 \det \begin{pmatrix}
    1 & 1 & a \\
    -1 & 0 & 1 \\
    0 & -1 & 1
    \end{pmatrix} \\
    \underline{R}_1 &\mapsto \underline{R}_1 + \underline{R}_2 + \underline{R}_3 \\
    \det A &= (1 - a)^2 \det \begin{pmatrix}
    0 & 0 & a + 2 \\
    -1 & 0 & 1 \\
    0 & -1 & 1
    \end{pmatrix} \\
    &= (1 - a)^2 (a + 2) \begin{vmatrix}
    -1 & 0 \\
    0 & -1
    \end{vmatrix} \\
    &= (1 - a)^2 (a + 2)
\end{align*} 
:::

### Multiplicative Property

::: {.theorem}
For $n \times n$ matrices $M$ and $N$
\begin{align*}
    \det (MN) = \det M \det N
\end{align*}  
This is based on the following lemma
:::

::: {.lemma}
\begin{align*}
    \epsilon_{i_1 \ldots i_n} M_{i_1 a_1} \ldots M_{i_n a_n} &= (\det M) \epsilon_{a_1 \ldots a_n}
\end{align*} 
:::

:::{.proof name="Proof of Theorem"}
\begin{align*}
    \det (MN) &= \epsilon_{i_1 \ldots i_n} (MN)_{i_1 1} \ldots (MN)_{i_n n} \\ 
    &= \epsilon_{i_1 \ldots i_n} M_{i_1 k_1} N_{k_1 1} \ldots  M_{i_n k_n} N_{k_n n} \\ 
    &= \epsilon_{i_1 \ldots i_n} M_{i_1 k_1} \ldots  M_{i_n k_n} N_{k_1 1} \ldots N_{k_n n} \\ 
    &= (\det M)  \epsilon_{k_1 \ldots k_n} N_{k_1 1} \ldots N_{k_n n} \text{by lemma } \\
    &= (\det M) (\det N)
\end{align*}  
:::

:::{.proof name="Proof of Lemma"}
Use total antisymmetry of LHS and RHS and then check taking $a_1 = 1, \ldots, a_n = n$.
:::

::: {.example}

\

i. If $M = \left(
    \begin{array}{c|c} 
      A & O \\ 
      \hline 
      O & B 
    \end{array} \right)$ 
block form, with $A$ $r \times r$ and $B$ $(n - r) \times (n - r)$ then $\det M = \det A \det B$.
Since $\left(
    \begin{array}{c|c} 
      A & O \\ 
      \hline 
      O & B 
    \end{array} \right) = \left(
    \begin{array}{c|c} 
        A & O \\ 
        \hline 
        O & I 
    \end{array} \right) \left(
    \begin{array}{c|c} 
        I & O \\ 
        \hline 
        O & B 
    \end{array} \right)$
and we can use Lemma \@ref(lem:block) above.
:::