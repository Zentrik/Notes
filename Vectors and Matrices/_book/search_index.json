[["index.html", "Vectors and Matrices 2021 - 2022 Lecture 1 0.1 Contents", " Vectors and Matrices 2021 - 2022 Author 07/10/2021 Lecture 1 0.1 Contents Complex Numbers Vectors in 3d Vectors in General, \\(\\mathbb{R}^n \\times \\mathbb{C}^n\\) Matrices and Linear Maps Determinants and Inverses Eigenvalues and Eigenvectors Changing Bases, Canonical Forms and Symmetries "],["complex-numbers.html", "1 Complex Numbers 1.1 Definitions 1.2 Basic properties and Consequences 1.3 Exponential and Trigonometric Functions 1.4 Transformations; lines and circles 1.5 Logarithms and Complex Powers", " 1 Complex Numbers 1.1 Definitions Construct \\(\\mathbb{C}\\) by adding an element \\(i\\) to real numbers \\(\\mathbb{R}\\), with \\[\\begin{align*} i^2 = -1. \\end{align*}\\] Any complex numbers \\(z \\in \\mathbb{C}\\) has the form \\(z = x + iy\\) with \\(x, y \\in \\mathbb{R}\\); \\(x = \\operatorname{Re}(z)\\), real part; \\(y = \\operatorname{Im}(z)\\), imaginary part. \\(\\mathbb{R} \\subset \\mathbb{C}\\) consisting of elements \\(x + i 0 = x\\) In the following, use the notation above and \\(z_1 = x_1 + i y_1\\), \\(z_2 = x_2 + i y_2\\) etc. Addition ( and subtraction) \\[\\begin{align*} z_1 \\pm z_2 = (x_1 \\pm x_2) + i (y_1 \\pm y_2) \\end{align*}\\] Multiplication \\[\\begin{align*} z_1 z_2 = (x_1 x_2 - y_1 y_2) + i(x_1 y_2 + x_2 y_1) \\end{align*}\\] If \\(z \\neq 0\\), observe from the definition \\[\\begin{align*} z^{\\text{-}1} = \\frac{x}{x^2 + y^2} - i \\frac{y}{x^2 + y^2} \\end{align*}\\] satisfies \\(z z^{\\text{-}1} = 1\\) Complex conjugate \\[\\begin{align*} \\overline{z} &amp;= z^{*} = x - iy \\\\ \\operatorname{Re}(z) &amp;= \\frac{1}{2} (z + \\overline{z}) \\\\ \\operatorname{Im}(z) &amp;= \\frac{1}{2i} (z - \\overline{z}) \\\\ \\overline{(\\overline{z})} &amp;= z \\\\ \\overline{z_1 + z_2} &amp;= \\overline{z_1} + \\overline{z_2} \\\\ \\overline{z_1 z_2} &amp;= (\\overline{z_1}) (\\overline{z_2}) \\end{align*}\\] Modulus is defined by \\(r = |z|\\), real and \\(\\geq 0\\), with \\[\\begin{align*} r^2 = |z|^2 = z \\overline{z} = x^2 + y^2 \\end{align*}\\] Argument \\(\\theta = \\arg{z}\\), real, defined for \\(z \\neq 0\\) by \\[\\begin{align*} z = r(\\cos \\theta + i \\sin \\theta) \\hspace{2cm} \\textbf{polar form} \\end{align*}\\] for some real \\(\\theta\\) \\[\\begin{align*} \\cos \\theta &amp;= \\frac{x}{(x^2 + y^2)^{1/2}},\\ \\sin \\theta = \\frac{y}{(x^2 + y^2)^{1/2}}, \\\\ &amp; \\implies \\tan \\theta = \\frac{y}{x} \\end{align*}\\] \\(\\arg(z)\\) is determined only \\(\\operatorname{mod} 2 \\pi\\), i.e. can change \\(\\theta \\to \\theta + 2n \\pi\\) where \\(n \\in \\mathbb{Z}\\). To make it unique we can restrict the range, e.g. principal value defined by \\(- \\pi &lt; \\theta \\leq \\pi\\) Argand diagram and Complex Plane Plot \\(\\operatorname{Re}(z)\\) and \\(\\operatorname{Im}(z)\\) on orthogonal axes, then \\(r = |z|\\) and \\(\\theta = \\arg(z)\\) are the length and angle shown. Example 1.1 For \\(z = -1 + i \\sqrt{3} = 2 (- \\frac{1}{2} + i \\frac{\\sqrt{3}}{2})\\), we have \\(|z| = 2\\) and \\(\\arg(z) = \\frac{2 \\pi}{3} + 2 n \\pi\\) where \\(n \\in \\mathbb{Z}\\). Note \\[\\begin{align*} \\tan \\theta = - \\sqrt{3} \\\\ \\implies \\theta = \\frac{2 \\pi}{3} + 2 n \\pi \\\\ = \\arg(z) \\\\ \\textbf{or } \\theta = - \\frac{\\pi}{3} + 2 n \\pi \\\\ = \\arg(-z) \\end{align*}\\] 1.2 Basic properties and Consequences \\(\\mathbb{C}\\) with operations \\(+, \\times\\) is a field, i.e.  \\(\\mathbb{C}\\) with \\(+\\) is an abelian group. \\(\\mathbb{C} \\setminus \\{0\\}\\) with \\(\\times\\) is an abelian group. distributive laws hold, e.g. \\(z_1(z_2 + z_3) = z_1 z_2 + z_1 z_3\\). Fundamental Theorem of Algebra A polynomial of degree \\(n\\) with coefficients in \\(\\mathbb{C}\\) can be written as a product of \\(n\\) linear factors. \\[\\begin{align*} p(z) &amp;= c_n z^n + \\ldots + c_1 z + c_0 \\ \\ \\text{ where } c_i \\in \\mathbb{C}, c_n \\neq 0 \\\\ &amp;= c_n ( z - \\alpha_1) \\ldots (z - \\alpha_n) \\text{ where } \\alpha_i \\in \\mathbb{C}. \\end{align*}\\] Hence \\(p(z) = 0\\) has at least one root and \\(n\\) roots connected with multiplicity (if we count duplicates as separate). Addition and subtraction can be viewed as parallelogram constructions Complex conjugation is reflection in real axis Proposition 1.1 Modulus/ length obeys composition property \\(|z_1 z_2| = |z_1| |z_2|\\). Triangle inequality \\(|z_1 + z_2| \\leq |z_1| + |z_2|\\) Proof (Triangle inequality). Compare \\[\\begin{align*} \\text{LHS}^2 &amp;= (z_1 + z_2) \\overline{z_1 + z_2} \\\\ \\text{RHS}^2 &amp;= |z_1|^2 + 2|z_1||z_2| + |z_2|^2 \\end{align*}\\] Compare cross terms: \\[\\begin{align*} &amp; z_1\\overline{z_2} + z_2 \\overline{z_1} \\leq 2 |z_1| |z_2| \\\\ &amp; \\iff \\frac{1}{2} (z_1 \\overline{z_2} + \\overline{(z_1 \\overline{z_2})}) \\leq |z_1| |\\overline{z_2}| \\\\ &amp; \\iff \\operatorname{Re}(z_1 \\overline{z_2}) \\leq |z_1 \\overline{z_2}| \\end{align*}\\] An alternative form of the triangle inequality: replace \\(z_2\\) by \\(z_2 - z_1\\) and rearrange to get \\[\\begin{align*} | z_2 - z_1 | &amp;\\geq |z_2| - |z_1| \\\\ \\text{or} &amp;\\geq |z_1| - |z_2| \\\\ \\text{So } |z_2 - z_1| &amp;\\geq \\left| |z_2| - |z_1| \\right| \\end{align*}\\] Proposition 1.2 \\[\\begin{align*} &amp;z_1 = r_1 (\\cos \\theta_1 + i \\sin \\theta_1),\\ z_2 = r_2 (\\cos \\theta_2 + i \\sin \\theta_2) \\\\ &amp;\\implies z_1 z_2 = r_1 r_2 (\\cos (\\theta_1 + \\theta_2) + i \\sin (\\theta_1 + \\theta_2)) \\end{align*}\\] i.e. moduli multiply and args add Proof. Direct check: calculate \\(z_1 z_2\\) and use standard trig formulas. Theorem 1.1 (De Moivre's Theorem) \\[\\begin{align*} (\\cos \\theta + i \\sin \\theta)^n = \\cos n \\theta + i \\sin n \\theta,\\ \\text{ for } n \\in \\mathbb{Z} \\end{align*}\\] Note for \\(z \\neq 0\\), \\(z^0 = 1\\) and \\(z^{-n} = (z^{-1})^n\\) for \\(n &gt; 0\\). Proof. Use 1.2 and induction 1.3 Exponential and Trigonometric Functions Define \\(\\exp, \\cos, \\sin\\) as functions on \\(\\mathbb{C}\\) by \\[\\begin{align*} \\exp(z) &amp;= e^z = \\sum_{n=0}^{\\infty} \\frac{1}{n!} z^n \\\\ \\cos(z) &amp;= \\frac{1}{2} (e^{iz} + e^{-iz}) \\\\ &amp;= 1 - \\frac{1}{2!}z^2 + \\frac{1}{4!}z^4 \\ldots \\\\ \\sin(z) &amp;= \\frac{1}{2i} (e^{iz} - e^{-iz}) \\\\ &amp;= z - \\frac{1}{3!}z^{3} + \\frac{1}{5!}z^5 + \\ldots \\end{align*}\\] These series converge \\(\\forall \\; z \\in \\mathbb{C}\\) and such series can be multiplied, rearranged and differentiated. Furthermore \\(e^{z}e^{w} = e^{z + w}\\) and from this we can see \\(e^{0} = 1\\) and \\((e^{z})^{n} = e^{nz}\\) for \\(n \\in \\mathbb{Z}\\). For the positive integers this is trivial and for the negative integers we can know that \\(e^z e^{-z} = 1\\) so \\(e^{-z} = (e^{z})^{-1}\\). Lemma 1.1 For \\(z = x + iy\\): \\(e^z = e^x ( \\cos y + i \\sin y)\\) \\(\\exp\\) on \\(\\mathbb{C}\\) takes all complex values except \\(0\\). \\(e^z = 1 \\iff z = 2n \\pi i,\\ n \\in \\mathbb{Z}\\) Proof.  i. \\(e^{x + iy} = e^x e^{iy}\\) but \\(e^{iy} = \\cos y + i \\sin y\\) \\(|e^z| = e^x\\), so \\(|e^z|\\) take all real values \\(&gt; 0\\). \\(\\arg{e^z} = y\\) taking all possible values. \\[\\begin{align*} e^z = 1 &amp;\\iff e^x = 1, \\cos y = 1, \\sin y = 0 \\\\ &amp;\\iff x = 0, y = 2 \\pi n \\end{align*}\\] Returning to polar form 1.2, this can be written \\(z = r ( \\cos \\theta + i \\sin \\theta) = re^{i \\theta}\\) for \\(r = |z|\\) and \\(\\theta = \\arg z\\). De Moivres Theorem 1.1 now follows from \\((e^{i \\theta})^n = e^{i n \\theta}\\). 1.3.1 Roots of units \\(z\\) is an Nth root of unity if \\(z^n = 1\\). To find all solutions: \\[\\begin{align*} &amp;&amp; z &amp;= r e^{i \\theta} \\text{ satisfying } z^N = 1 \\\\ &amp;\\iff &amp; r^N e^{i N \\theta} &amp;= 1 \\\\ &amp;\\iff &amp; r^N &amp;= 1 \\text{ and } N \\theta = 2n \\pi,\\ n \\in \\mathbb{Z} \\end{align*}\\] This gives N distinct solutions. \\[\\begin{align*} z &amp;= e^{2 \\pi i n/N} \\\\ &amp;= \\cos \\frac{2\\pi n}{N} + i \\sin \\frac{2 \\pi n}{N} \\\\ \\text{The only distinct cases are when } n &amp;= 0, 1, 2 \\ldots N - 1 \\text{ due to periodicity} \\\\ &amp;= \\omega^n, \\text{ where } \\omega = e^{2 \\pi i / N} \\\\ \\end{align*}\\] These solutions lie at vertices of a regular N-gon. Figure 1.1: N = 6 1.4 Transformations; lines and circles Consider the following transformations on \\(\\mathbb{C}\\) (maps \\(\\mathbb{C} \\to \\mathbb{C}\\)) \\[\\begin{align*} z &amp;\\mapsto z + a &amp; &amp;(\\text{translation}) \\\\ z &amp;\\mapsto \\lambda z &amp; &amp;(\\text{scaling by } \\lambda \\in \\mathbb{R}) \\\\ z &amp;\\mapsto e^{i \\alpha} z &amp; &amp;(\\text{rotation by } \\alpha \\in \\mathbb{R}) \\\\ z &amp;\\mapsto \\overline{z} &amp; &amp;(\\text{reflection in the real axis}) \\\\ z &amp;\\mapsto \\frac{1}{z} &amp; &amp;(\\text{inversion}) \\end{align*}\\] Consider a general point on a line in \\(\\mathbb{C}\\) through \\(z_0\\) and parallel to \\(w \\neq 0\\) (fixed \\(z_0, w \\in \\mathbb{C}\\)): \\[\\begin{align*} z = z_0 + \\lambda w,\\ \\lambda \\in \\mathbb{R} \\end{align*}\\] To eliminate \\(\\lambda\\), take the conjugate to get \\(\\overline{z} = \\overline{z_0} + \\lambda \\overline{w}\\) and equate \\(\\lambda\\) to get \\(\\overline{w}z - w\\overline{z} = \\overline{w} z_0 - w \\overline{z_0}\\). Consider a general point on a circle in \\(\\mathbb{C}\\) with centre \\(c \\in \\mathbb{C}\\) and radius \\(\\rho \\in \\mathbb{R}^{++}\\). \\[\\begin{align*} z = c + \\rho &amp;e^{i \\alpha}, \\text{ for any } \\alpha \\in \\mathbb{R} \\\\ \\text{Equivalently} \\\\ |z - c| &amp;= \\rho \\\\ \\text{or } \\\\ |z - c|^2 &amp;= \\rho^2 \\\\ |z|^2 - \\overline{c}z - c \\overline{z} &amp;= \\rho^2 - |c|^2 \\end{align*}\\] Möbius transformations are generated by translations, scalings, rotations and inversion. They can be viewed as acting on \\(\\mathbb{C}_\\infty = \\mathbb{C} \\cup \\{ \\infty \\}\\) - geometrically a sphere (see IA Groups). We add \\(\\infty\\) to deal with \\(0^{-1}\\)? 1.5 Logarithms and Complex Powers Define \\(w = \\log z,\\ z \\in \\mathbb{C},\\ z \\neq 0\\) by \\(e^w = \\exp w = z\\) i.e. \\(\\log\\) is the inverse of \\(\\exp\\), but \\(\\exp\\) is many-to-one (\\(e^z = e^{z + 2n \\pi i}\\)) and so \\(\\log\\) is multi-valued. \\[\\begin{align*} z &amp;= r e^{i \\theta} \\\\ &amp;= e^{\\log r} e^{i \\theta} \\\\ &amp;= e^{\\log r + i \\theta} \\\\ \\implies \\log z &amp;= \\log r + i \\theta \\\\ &amp;= \\log |z| + i \\arg z \\end{align*}\\] Multiple values of \\(\\arg\\) and \\(\\log\\) are related: \\[\\begin{align*} \\theta &amp;\\to \\theta + 2n \\pi,\\ n \\in \\mathbb{Z} \\\\ \\log z &amp;\\to \\log z + 2n \\pi i,\\ n \\in \\mathbb{Z} \\end{align*}\\] To make them single valued we can restrict e.g. \\(0 \\leq \\theta &lt; 2 \\pi\\) or \\(-\\pi &lt; \\theta \\leq \\pi\\) (principal value). Example 1.2 \\[\\begin{align*} z &amp;= -3i = 3 (-i) \\\\ &amp;= e^{\\log 3}e^{-i \\pi / 2 + 2n \\pi i} \\\\ &amp;= e^{\\log 3 -i \\pi / 2 + 2n \\pi i} \\\\ \\log z &amp;= \\log 3 -i \\pi / 2 + 2n \\pi i \\\\ \\arg z &amp;= 3 \\pi / 2 \\\\ &amp;\\text{or } - \\pi / 2 \\text{ with the restrictions above} \\end{align*}\\] Define complex powers by \\[\\begin{align*} z^\\alpha = e^{\\alpha \\log z},\\ z \\in \\mathbb{C},\\ z \\neq 0,\\ \\alpha \\in \\mathbb{C} \\end{align*}\\] This is multi-valued in general under the change \\(\\arg z \\to \\arg z + 2n\\pi\\) \\[\\begin{align*} z^\\alpha \\to z^\\alpha e^{2 \\pi i n \\alpha} \\end{align*}\\] If \\(\\alpha = p \\in \\mathbb{Z}\\) then \\(z^\\alpha = z^p\\) is unique If \\(\\alpha = \\frac{p}{q} \\in \\mathbb{Q}\\) then \\(z^\\alpha = z^{\\frac{p}{q}}\\) takes finitely many values. But in general we have infinitely many values Example 1.3 \\[\\begin{align*} (1 + i)^{\\frac{1}{2}}: 1 + i &amp;= \\sqrt{2} e^{i \\pi /4} \\\\ &amp;= e^{\\frac{1}{2} \\log 2 + i \\pi /4} \\\\ \\log(1 + i) &amp;= \\frac{1}{2} \\log 2 + i \\pi / 4 + 2 n \\pi i \\\\ \\implies (1 + i)^{\\frac{1}{2}} &amp;= e^{\\frac{1}{2} \\log (1 + i)} \\\\ &amp;= e^{\\frac{1}{4} \\log 2 + i \\pi / 8 + n \\pi i} \\\\ &amp;= 2^{\\frac{1}{4}} e^{i \\pi / 8} (-1)^n \\end{align*}\\] Example 1.4 \\[\\begin{align*} (-3i)^i &amp;= e^{i \\log (-3i)} \\\\ &amp;= e^{i (\\log 3 - i \\pi /2 + 2 n \\pi i)} \\\\ &amp;= e^{i \\log 3} e^{\\pi /2 - 2 n \\pi},\\ n \\in \\mathbb{Z} \\end{align*}\\] "],["vectors-in-3-dimensions.html", "2 Vectors in 3 Dimensions 2.1 Vector Addition and Scalar Multiplication 2.2 Scalar or Dot Product 2.3 Orthonormal Bases and Components 2.4 Vector or Cross Product 2.5 Triple products 2.6 Lines, Planes and Other Vector Equations 2.7 Index (suffix) Notation and the Summation convention", " 2 Vectors in 3 Dimensions A vector is a quantity with magnitude and direction (e.g. force, electric and magnetic fields) - all examples modelled on position. We take a geometrical approach to position vectors in 3d space based on standard (Euclidean) notions of points, lines, planes, length, angle etc. Chose point \\(O\\) as origin, the points \\(A\\), \\(B\\) have position vectors \\[\\begin{align*} \\underline{a} = \\overrightarrow{OA},\\ \\underline{b} = \\overrightarrow{OB} \\end{align*}\\] lengths denoted by \\(|\\underline{a}| = | \\overrightarrow{OA}|\\), \\(\\underline{O}\\) is the position vector for \\(O\\). 2.1 Vector Addition and Scalar Multiplication Scalar Multiplication Given \\(\\underline{a}\\), position vector for \\(A\\) and a scalar \\(\\lambda \\in \\mathbb{R}\\), \\(\\lambda \\underline{a}\\) is position vector of point \\(A&#39;\\) on \\(OA\\) with \\[\\begin{align*} |\\lambda \\underline{a}| = |\\overrightarrow{OA&#39;}| = |\\lambda| |\\underline{a}| \\end{align*}\\] as shown Say \\(\\underline{a}\\) and \\(\\underline{b}\\) are parallel, \\(\\underline{a} \\parallel \\underline{b} \\iff \\underline{a} = \\lambda \\underline{b}\\) or \\(\\underline{b} = \\lambda \\underline{a}\\). Definition allows \\(\\lambda \\leq 0\\) so \\(\\underline{a} \\parallel \\underline{0}\\) for any \\(\\underline{a}\\). Addition Given \\(\\underline{a}, \\underline{b}\\) position vectors of \\(A, B\\) construct a parallelogram \\(OACB\\) and define \\(\\underline{a} + \\underline{b} = \\underline{c}\\), position vector of point \\(C\\), provided \\(\\underline{a} \\nparallel \\underline{b}\\); if \\(\\underline{a} \\parallel \\underline{b}\\) then we can write \\(\\underline{a} = \\alpha \\underline{u}\\), \\(\\underline{b} = \\beta \\underline{u}\\) for some \\(\\underline{u}\\) and then \\(\\underline{a} + \\underline{b} = (\\alpha + \\beta) \\underline{u}\\). Properties For any vectors \\(\\underline{a}, \\underline{b}, \\underline{c}\\) \\[\\begin{align*} \\underline{a} + \\underline{0} &amp;= \\underline{0} + \\underline{a} = \\underline{a} &amp; &amp;(\\underline{0} \\text{ is identity for } +) \\\\ \\exists \\; a \\text{ such that } \\\\ \\underline{a} + (- \\underline{a}) &amp;= (- \\underline{a}) + \\underline{a} = \\underline{0} &amp; &amp;(-\\underline{a} \\text{ is the inverse for } \\underline{a}) \\\\ \\underline{a} + \\underline{b} &amp;= \\underline{b} + \\underline{a} &amp; &amp;(+ \\text{ is commutative}) \\\\ \\underline{a} + (\\underline{b} + \\underline{c} ) &amp;= ( \\underline{a} + \\underline{b} ) + \\underline{c} &amp; &amp;(+ \\text{ is associative}) \\\\ \\lambda ( \\underline{a} + \\underline{b}) &amp;= \\lambda \\underline{a} + \\lambda \\underline{b} &amp;&amp;\\\\ (\\lambda + \\mu) \\underline{a} &amp;= \\lambda \\underline{a} + \\mu \\underline{a} &amp;&amp;\\\\ \\lambda (\\mu \\underline{a}) &amp;= (\\lambda \\mu) \\underline{a}, &amp; &amp;\\lambda, \\mu \\in \\mathbb{R} \\end{align*}\\] All of these can be checked geometrically, e.g. associativity of vector addition using a parallelepiped Linear Combinations and Span A linear combination of vectors \\(\\underline{a}, \\underline{b}, \\dots, \\underline{c}\\) is an expression \\[\\begin{align*} \\alpha \\underline{a} + \\beta \\underline{b} + \\dots + \\gamma \\underline{c} \\text{ for some } \\alpha, \\beta, \\dots, \\gamma \\in \\mathbb{R}. \\end{align*}\\] The span of a set of vectors is \\(\\operatorname{span} \\{ \\underline{a}, \\underline{b}, \\dots, \\underline{c} \\} = \\{\\alpha \\underline{a} + \\beta \\underline{b} + \\dots + \\gamma \\underline{c} : \\alpha, \\beta, \\dots, \\gamma \\in \\mathbb{R} \\}\\). If \\(a \\neq 0\\) then \\(\\operatorname{span} \\{ \\underline{a} \\} = \\{ \\lambda \\underline{a} \\}\\), which is a line through \\(O\\) and \\(A\\). If \\(\\underline{a} \\nparallel \\underline{b}\\) (this also means \\(\\underline{a}, \\underline{b} \\neq \\underline{0}\\)) then \\(\\operatorname{span} \\{ \\underline{a}, \\underline{b}\\} = \\{ \\alpha \\underline{a} + \\beta \\underline{b} : \\alpha, \\beta \\in \\mathbb{R}\\}\\), which is a plane through \\(O, A, B\\). 2.2 Scalar or Dot Product Definition 2.1 Given \\(\\underline{a}\\) and \\(\\underline{b}\\) let \\(\\theta\\) be the angle between them; then \\[\\begin{align*} \\underline{a} \\cdot \\underline{b} = | \\underline{a} | | \\underline{b} | \\cos \\theta \\end{align*}\\] scalar or dot product or inner product Figure 2.1: \\(\\theta\\) defined unless \\(|\\underline{a}|\\) or \\(| \\underline{b}| = 0\\) and then \\(\\underline{a} \\cdot \\underline{b} = 0\\) Properties \\[\\begin{align*} \\underline{a} \\cdot \\underline{b} &amp;= \\underline{b} \\cdot \\underline{a} \\\\ \\underline{a} \\cdot \\underline{a} = |\\underline{a}|^2 &amp;\\geq 0 \\text{ and } = 0 \\iff \\underline{a} = \\underline{0} \\\\ (\\lambda \\underline{a}) \\cdot \\underline{b} &amp;= \\lambda (\\underline{a} \\cdot \\underline{b}) = \\underline{a} \\cdot (\\lambda \\underline{b}) \\\\ \\underline{a} \\cdot ( \\underline{b} + \\underline{c}) &amp;= \\underline{a} \\cdot \\underline{b} + \\underline{a} \\cdot \\underline{c} \\end{align*}\\] Interpretation For \\(\\underline{a} \\neq \\underline{0}\\), consider \\(\\underline{u} = \\frac{\\underline{a}}{|\\underline{a}|}\\) \\[\\begin{align*} \\underline{u} \\cdot \\underline{b} = \\frac{1}{|\\underline{a}|} \\underline{a} \\cdot \\underline{b} \\end{align*}\\] is the component of \\(\\underline{b}\\) along \\(\\underline{a}\\) We can resolve \\(\\underline{b} = \\underline{b}_\\parallel + \\underline{b}_\\perp\\). \\(\\underline{b}_\\parallel = (\\underline{b} \\cdot \\underline{u}),\\ \\underline{b}_\\perp = b - (\\underline{b} \\cdot \\underline{u})\\) where \\(\\underline{u} = \\frac{\\underline{a}}{|\\underline{a}|}\\). Note \\(\\underline{a} \\cdot \\underline{b} = \\underline{a} \\cdot \\underline{b}_\\parallel\\).   In general, vectors \\(\\underline{a}\\) and \\(\\underline{b}\\) are orthogonal or perpendicular, written \\(\\underline{a} \\perp \\underline{b} \\iff \\underline{a} \\cdot \\underline{b} = 0\\). 2.3 Orthonormal Bases and Components Choose vectors \\(\\underline{e}_1, \\underline{e}_2, \\underline{e}_3\\) that are orthonormal i.e. each of unit length and mutually perpendicular. \\[\\begin{align*} \\underline{e}_i \\cdot \\underline{e}_j = \\begin{cases} 1 &amp; \\text{if } i = j \\\\ 0 &amp; \\text{if } i \\neq j \\end{cases} \\end{align*}\\] This is equivalent to choosing Cartesian axes along these directions, \\(\\{ \\underline{e}_i \\}\\) is a basis: any vector can be expressed \\[\\begin{align*} \\underline{a} = \\sum_{i} a_i \\underline{e}_i = a_1 \\underline{e}_1 + a_2 \\underline{e}_2 + a_3 \\underline{e}_3 \\end{align*}\\] and each component is uniquely determined \\[\\begin{align*} a_i = \\underline{e}_i \\cdot \\underline{a}. \\end{align*}\\] Each \\(\\underline{a}\\) can now be identified with a set of components in \\[\\begin{align*} \\begin{pmatrix} a_1 &amp; a_2 &amp; a_3 \\end{pmatrix} \\text{ or } \\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{pmatrix} \\end{align*}\\] Note \\[\\begin{align*} \\underline{a} \\cdot \\underline{b} &amp;= \\left( \\sum_i a_i \\underline{e}_i \\right) \\cdot \\left( \\sum_j b_j \\underline{e}_j \\right) \\\\ &amp;= a_1 b_1 + a_2 b_2 + a_3 b_3 \\\\ |\\underline{a}|^2 &amp;= a_1^2 + a_2^2 + a_3^2 \\quad \\text{(Pythagoras)} \\end{align*}\\] \\(\\underline{e}_1, \\underline{e}_2, \\underline{e}_3\\) are also often written \\(\\underline{i}, \\underline{j}, \\underline{k}\\). 2.4 Vector or Cross Product Definition 2.2 Given \\(\\underline{a}\\) and \\(\\underline{b}\\), let \\(\\theta\\) be the angle between them measure in the sense shown relative to a unit normal \\(\\underline{n}\\) to the plane they span; then \\[\\begin{align*} \\underline{a} \\wedge \\underline{b} = \\underline{a} \\times \\underline{b} = |\\underline{a}| | \\underline{b}| \\sin \\theta \\, \\underline{n} \\end{align*}\\] is vector or cross product.1 Properties \\[\\begin{align*} \\underline{a} \\wedge \\underline{b} &amp;= - \\underline{b} \\wedge \\underline{a} \\\\ (\\lambda \\underline{a}) \\wedge \\underline{b} &amp;= \\lambda (\\underline{a} \\wedge \\underline{b}) = \\underline{a} \\wedge (\\lambda \\underline{b}) \\\\ \\underline{a} \\wedge (\\underline{b} + \\underline{c}) &amp;= \\underline{a} \\wedge \\underline{b} + \\underline{a} \\wedge \\underline{c} \\\\ \\underline{a} \\wedge \\underline{b} &amp;= \\underline{0} \\iff \\underline{a} \\parallel \\underline{b} \\\\ \\underline{a} \\wedge \\underline{b} &amp;\\perp \\underline{a} \\text{ and } \\underline{b} \\\\ \\underline{a} \\cdot (\\underline{a} \\wedge \\underline{b}) &amp;= \\underline{b} \\cdot (\\underline{a} \\wedge \\underline{b}) = 0 \\end{align*}\\] Interpretations \\(\\underline{a} \\wedge \\underline{b}\\) is the vector area shown: \\(|\\underline{a} \\wedge \\underline{b}|\\) is the scalar area. The direction of the normal \\(\\underline{n}\\) gives orientation of the parallelogram in space. Fix \\(\\underline{a}\\) and consider \\(\\underline{x} \\perp \\underline{a}\\); then \\(\\underline{x} \\mapsto \\underline{a} \\wedge \\underline{x}\\) scales \\(| \\underline{x} |\\) by a factor \\(|\\underline{a}|\\) and rotates \\(\\underline{x}\\) by \\(\\pi / 2\\) in the plane \\(\\perp \\underline{a}\\) as shown: Component expressions Consider \\(\\underline{e}_1, \\underline{e}_2, \\underline{e}_3\\) orthonormal basis as in Orthonormal Bases and Components but assume in addition \\[\\begin{align*} \\underline{e}_1 \\wedge \\underline{e}_2 = \\underline{e}_3 = -\\underline{e}_2 \\wedge \\underline{e}_1 \\\\ \\underline{e}_2 \\wedge \\underline{e}_3 = \\underline{e}_1 = -\\underline{e}_3 \\wedge \\underline{e}_2 \\\\ \\underline{e}_3 \\wedge \\underline{e}_1 = \\underline{e}_2 = -\\underline{e}_1 \\wedge \\underline{e}_3 \\\\ \\end{align*}\\] (all equalities follow from any one). This is called a right-handed orthonormal basis. Now for \\[\\begin{align*} \\underline{a} &amp;= \\sum_{i} a_i \\underline{e}_i = a_1 \\underline{e}_1 + a_2 \\underline{e}_2 + a_3 \\underline{e}_3 \\\\ \\underline{b} &amp;= \\sum_j b_j \\underline{e}_j = b_1 \\underline{e}_1 + b_2 \\underline{e}_2 + b_3 \\underline{e}_3 \\\\ \\underline{a} \\wedge \\underline{b} &amp;= (a_2 b_3 - a_3 b_2)\\underline{e}_1 + (a_3 b_1 - a_1 b_3) \\underline{e}_2 + (a_1 b_2 - a_2 b_1) \\underline{e}_3 \\end{align*}\\] 2.5 Triple products Scalar Triple Product Definition 2.3 \\[\\begin{align*} \\underline{a} \\cdot (\\underline{b} \\wedge \\underline{c}) &amp;= \\underline{b} \\cdot (\\underline{c} \\wedge \\underline{a}) = \\underline{c} \\cdot ( \\underline{a} \\wedge \\underline{b}) \\\\ &amp;= - \\underline{a} \\cdot (\\underline{c} \\wedge \\underline{b}) = - \\underline{b} \\cdot (\\underline{a} \\wedge \\underline{c}) = - \\underline{c} \\cdot (\\underline{b} \\wedge \\underline{a}) \\\\ &amp;= \\left[ \\underline{a}, \\underline{b}, \\underline{c} \\right] \\end{align*}\\] Interpretation: \\(|\\underline{c} \\cdot \\underline{a} \\wedge \\underline{b}|\\) is the volume of a parallelepiped shown The signed volume is \\(\\underline{c} \\cdot \\underline{a} \\wedge \\underline{b}\\); if \\(\\underline{c} \\cdot \\underline{a} \\wedge \\underline{b} &gt; 0\\) then we say \\(\\underline{a}, \\underline{b}, \\underline{c}\\) is a right-handed set Note: \\(\\underline{a} \\cdot \\underline{b} \\wedge \\underline{c} = 0\\) iff \\(\\underline{a}, \\underline{b}, \\underline{c}\\) are co-planar meaning one of them lies in the plane spanned by the other two. e.g. \\(\\underline{c} = \\alpha \\underline{a} + \\beta \\underline{b}\\) belonging to \\(\\operatorname{span} \\{ a, b \\}\\) Example 2.1 \\[\\begin{align*} \\underline{a} &amp;= \\begin{pmatrix}2 &amp; 0 &amp; -1\\end{pmatrix} \\\\ \\underline{b} &amp;= \\begin{pmatrix}7 &amp; -3 &amp; 5 \\end{pmatrix} \\\\ \\implies \\underline{a} \\wedge \\underline{b} &amp;= (0.5 - (-1)(-3)) \\underline{e}_1 + ((-1)7 - 2.5) \\underline{e}_2 + (2(-3) - 0.7) \\underline{e}_3 \\\\ &amp;= \\begin{pmatrix}-3 &amp; -17 &amp; -6 \\end{pmatrix} \\end{align*}\\] To test whether \\(\\underline{a}, \\underline{b}, \\underline{c}\\) are co-planar with \\(\\underline{c} = \\begin{pmatrix}3 &amp; -3 &amp; 7 \\end{pmatrix}\\). \\[\\begin{align*} \\underline{c} \\cdot \\underline{a} \\wedge \\underline{b} &amp;= 3(-3) + (-3)(-17) + 7(-6) \\\\ &amp;= 0; \\end{align*}\\] consistent with \\(\\underline{c} = \\underline{b} - 2 \\underline{a}\\). Vector Triple Product \\[\\begin{align*} \\underline{a} \\wedge (\\underline{b} \\wedge \\underline{c}) &amp;= (\\underline{a} \\cdot \\underline{c}) \\underline{b} - (\\underline{a} \\cdot \\underline{b}) \\underline{c} \\\\ (\\underline{a} \\wedge \\underline{b}) \\wedge \\underline{c} &amp;= (\\underline{a} \\cdot \\underline{c}) \\underline{b} - (\\underline{b} \\cdot \\underline{c}) \\underline{a} \\end{align*}\\] Forms of RHS is constrained by definitions above, and we could check explicitly. We will return to these formulae using index notation and summation convention. 2.6 Lines, Planes and Other Vector Equations 2.6.1 Lines General point on a line through \\(\\underline{a}\\) with direction \\(\\underline{u} \\neq \\underline{0}\\) has position vector \\[\\begin{align*} \\underline{r} = \\underline{a} + \\lambda \\underline{u},\\ \\lambda \\in \\mathbb{R} \\end{align*}\\] in parametric form. Alternative form without parameter \\(\\lambda\\) obtained by crossing with \\(\\underline{u}\\): \\[\\begin{align*} \\underline{u} \\wedge \\underline{r} = \\underline{u} \\wedge \\underline{a} \\end{align*}\\] Conversely \\(\\underline{u} \\wedge (\\underline{r} - \\underline{a}) = \\underline{0}\\) and this holds \\(\\iff \\underline{r} - \\underline{a} = \\lambda \\underline{u}\\) for some \\(\\lambda\\). Now consider \\[\\begin{align*} \\underline{u} \\wedge \\underline{r} &amp;= \\underline{c} \\text{ where } \\underline{u}, \\underline{c} \\text{ are given vectors and } \\underline{u} \\neq \\underline{0} \\\\ \\text{Note } \\underline{u} \\cdot (\\underline{u} \\wedge \\underline{r}) &amp;= \\underline{u} \\cdot \\underline{c} \\\\ &amp;= 0 \\end{align*}\\] If \\(\\underline{u} \\cdot \\underline{c} \\neq 0\\) then we have a contradiction i.e. no solutions. If \\(\\underline{u} \\cdot \\underline{c} = 0\\), try a particular solution by considering \\[\\begin{align*} \\underline{u} \\wedge (\\underline{u} \\wedge \\underline{c}) &amp;= \\overbrace{(\\underline{u} \\cdot \\underline{c})}^0 \\underline{u} - (\\underline{u} \\cdot \\underline{u}) \\underline{c} \\\\ &amp;= - |\\underline{u}|^2 \\underline{c} \\end{align*}\\] Hence \\(\\underline{a} = - \\frac{1}{|\\underline{u}|^2} (\\underline{u} \\wedge \\underline{c})\\) is a solutions. The general solution is \\[\\begin{align*} \\underline{r} = \\underline{a} + \\lambda \\underline{u} \\end{align*}\\] as \\(\\underline{u} \\wedge (\\underline{a} + \\lambda \\underline{u}) = \\underline{u} \\wedge \\underline{a}\\) 2.6.2 Planes The general point on a plane through \\(\\underline{a}\\) with directions \\(\\underline{u}, \\underline{v}\\) in plane (\\(\\underline{u} \\nparallel \\underline{v}\\)) has position vector \\[\\begin{align*} \\underline{r} = \\underline{a} + \\lambda \\underline{u} + \\mu \\underline{v}, \\text{ where } \\lambda, \\mu \\in \\mathbb{R} \\end{align*}\\] in parametric form. An alternative form without parameters is obtained by by dotting with a normal vector \\(\\underline{n} = \\underline{u} \\wedge \\underline{v} \\neq 0\\) (since \\(\\underline{u} \\nparallel \\underline{v}\\) but is not necessarily a unit vector). This gives \\[\\begin{align*} \\underline{n} \\cdot \\underline{r} &amp;= \\underline{n} \\cdot \\underline{a} \\\\ &amp;= \\kappa \\text{ (a constant)} \\end{align*}\\] Note component of \\(\\underline{r}\\) along \\(\\underline{n}\\) is \\[\\begin{align*} \\frac{\\underline{n} \\cdot \\underline{r}}{|\\underline{n}|} = \\frac{\\kappa}{|\\underline{n}|} \\end{align*}\\] Figure 2.2: Clearly a plane Moreover \\(|\\kappa| / |\\underline{n}|\\) is perpendicular distance of plane from \\(\\underline{0}\\). 2.6.3 Other Vector Equations Consider equations for \\(\\underline{r}\\) (unknown) written in vector notation with given constant vectors. Possible approaches: We can re-write and convert to some standard form Example 2.2 \\[\\begin{align*} |\\underline{r}|^2 + \\underline{r} \\cdot \\underline{a} &amp;= k \\\\ \\text{Complete the square:} \\\\ \\left| \\underline{r} + \\frac{1}{2} \\underline{a} \\right|^2 &amp;= \\left( \\underline{r} + \\frac{1}{2} \\underline{a} \\right) \\cdot \\left( \\underline{r} + \\frac{1}{2} \\underline{a} \\right) \\\\ &amp;= k + \\frac{1}{4} |\\underline{a}|^2. \\end{align*}\\] This is the equation of a sphere, centre \\(-\\frac{1}{2} \\underline{a}\\) and radius \\((k + \\frac{1}{4} |\\underline{a}|^2)^{1 / 2}\\) provided \\(k + \\frac{1}{4} |\\underline{a}|^2 &gt; 0\\) For equations linear in \\(\\underline{r}\\) Try dotting and crossing with constant vectors to learn more. Example 2.3 Another example of a vector equation is \\[\\begin{align} \\underline r + \\underline a \\times (\\underline b \\times \\underline r) = \\underline c \\tag{2.1} \\end{align}\\] where \\(\\underline a, \\underline b, \\underline c\\) are fixed. We can dot with \\(\\underline a\\) to eliminate the second term: \\[\\begin{align} \\underline a \\cdot \\underline r = \\underline a \\cdot \\underline c \\tag{2.2} \\end{align}\\] Note that using the dot product loses information  this is simply a tool to make deductions; (2.2) does not contain the full information of (2.1), i.e. (2.2) \\(\\ \\not\\!\\!\\!\\!\\implies\\) (2.1). Combining (2.1) and (2.2), and using the formula for the vector triple product, we get \\[\\begin{align} \\underline r + (\\underline a \\cdot \\underline r) \\underline b - (\\underline a \\cdot \\underline b) \\underline r &amp; = \\underline c \\tag{2.3} \\\\ \\implies \\underline r + (\\underline a \\cdot \\underline c) \\underline b - (\\underline a \\cdot \\underline b) \\underline r &amp; = \\underline c \\notag \\end{align}\\] This eliminates the dependency on \\(\\underline r\\) inside the dot product. Now, we can factorise, leaving \\[\\begin{align} (1 - \\underline a \\cdot \\underline b) \\underline r = \\underline c - (\\underline a \\cdot \\underline c) \\underline b \\tag{2.4} \\end{align}\\] If \\(1 - \\underline a \\cdot \\underline b \\neq 0\\) then \\(\\underline r\\) has a single solution, \\(\\underline{r} = (\\underline{c} - (\\underline{a} \\cdot \\underline{c}) \\underline{b} ) / (1 - \\underline{a} \\cdot b)\\), a point. If \\(1 - \\underline{a} \\cdot \\underline{b} = 0\\) and \\(\\underline c - (\\underline a \\cdot \\underline c) \\underline b \\neq 0\\) implies (2.4) is inconsistent so no solution If \\(1 - \\underline{a} \\cdot \\underline{b} = 0\\) and \\(\\underline c - (\\underline a \\cdot \\underline c) \\underline b = 0\\) We can now combine this expression for \\(\\underline c\\) into (2.3) and eliminate the \\((1- \\underline a \\cdot \\underline b)\\) term, to get \\[\\begin{align*} (\\underline a \\cdot \\underline r - \\underline a \\cdot \\underline c) \\underline b = \\underline 0 \\end{align*}\\] This shows us that (given that \\(\\underline b\\) is non-zero) the solutions to the equation are given by (2.2), which is the equation of a plane, so (2.2) \\(\\implies\\) (2.1). Can try expressing \\(\\underline{r} = \\alpha \\underline{a} + \\beta \\underline{b} + \\gamma \\underline{c}\\) for some non-co-planar \\(\\underline{a}, \\underline{b}, \\underline{c}\\) and solve for \\(\\alpha, \\beta, \\gamma\\). Can choose basis and use index/ matrix notation. 2.7 Index (suffix) Notation and the Summation convention 2.7.1 Components; \\(\\delta\\) and \\(\\epsilon\\) Write vectors \\(\\underline{a}, \\underline{b}, \\dots\\), in terms of components \\(a_i, b_i, \\dots\\), wrt. an orthonormal, right-handed basis \\[\\begin{align*} \\underline{e}_1, \\underline{e}_2, \\underline{e}_3. \\end{align*}\\] Indices or suffices \\(i, j, k, l, p, q \\dots\\) take values \\(1, 2, 3\\). Then \\[\\begin{align*} &amp;&amp; \\underline{c} &amp;= \\alpha \\underline{a} + \\beta \\underline{b} \\\\ &amp;\\iff &amp; c_i &amp;= [\\alpha \\underline{a} + \\beta \\underline{b}] \\\\ &amp;&amp; &amp;= \\alpha a_i + \\beta b_i \\text{ for } i = 1, 2, 3 \\textbf{ free index} \\\\ &amp;&amp; \\underline{a} \\cdot \\underline{b} &amp;= \\sum_i a_i b_i = \\sum_j a_j b_j \\\\ &amp;&amp; \\underline{x} &amp;= \\underline{a} + (\\underline{b} \\cdot \\underline{c}) \\underline{d} \\\\ &amp;\\iff &amp; x_j &amp;= a_j + \\left( \\sum_k b_k c_k \\right) d_j \\text{ for } j = 1, 2, 3 \\text{ free index} \\end{align*}\\] Definition 2.4 (Kronecker Delta) \\[\\begin{align*} \\delta_{ij} &amp;= \\begin{cases} 1 &amp; \\text{if } i = j \\\\ 0 &amp; \\text{else}\\\\ \\end{cases} \\\\ \\delta_{ij} &amp;= \\delta_{ji} \\text{ symmetric} \\\\ \\text{Written out as an array} &amp;\\text{ or matrix}\\\\ \\begin{pmatrix} \\delta_{11} &amp; \\delta_{12} &amp; \\delta_{13} \\\\ \\delta_{21} &amp; \\delta_{22} &amp; \\delta_{23} \\\\ \\delta_{31} &amp; \\delta_{32} &amp; \\delta_{33} \\end{pmatrix} &amp;= \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\\\ \\text{Then } \\underline{e}_i \\cdot \\underline{e}_j &amp;= \\delta_{ij} \\\\ \\text{and } \\underline{a} \\cdot \\underline{b} &amp;= \\left( \\sum_i a_i \\underline{e}_i \\right) \\cdot \\left( \\sum_j b_j \\underline{e}_j \\right) \\\\ &amp;= \\sum_{ij} a_i b_j \\underline{e}_i \\cdot \\underline{e}_j \\\\ &amp;= \\sum_{ij} a_i b_j \\delta_{ij} \\\\ &amp;= \\sum_i a_i b_i \\end{align*}\\] Definition 2.5 (Levi-Civita Epsilon) \\[\\begin{align*} \\epsilon_{ijk} &amp;= \\begin{cases} + 1 &amp; \\text{if } (i, j, k) \\text{ is an even permutation of } (1, 2, 3) \\\\ - 1 &amp; \\text{if } (i, j, k) \\text{ is an odd permutation of } (1, 2, 3) \\\\ 0 &amp; \\text{else} \\end{cases} \\\\ \\text{i.e. } \\epsilon_{123} &amp;= \\epsilon_{231} = \\epsilon_{312} = + 1 \\\\ \\epsilon_{321} &amp;= \\epsilon_{213} = \\epsilon_{132} = - 1 \\\\ \\epsilon_{ijk} &amp;= 0 \\text{ if any two index values match} \\\\ \\end{align*}\\] \\(\\epsilon_{ijk}\\) is totally antisymmetric: exchanging any pairs of indices produces a change in sign. Then \\[\\begin{align*} \\underline{e}_i \\wedge \\underline{e}_j &amp;= \\sum_k \\epsilon_{ijk} \\ \\underline{e}_k \\\\ \\text{e.g. } \\underline{e}_2 \\wedge \\underline{e}_1 &amp;= \\sum_k \\epsilon_{21k} \\ \\underline{e}_k \\\\ &amp;= \\epsilon_{213} \\ \\underline{e}_3 \\\\ &amp;= - \\underline{e}_3 \\\\ \\text{And } \\underline{a} \\wedge \\underline{b} &amp;= \\left( \\sum_i a_i \\underline{e}_i \\right) \\wedge \\left( \\sum_j b_j \\underline{e}_j \\right) \\\\ &amp;= \\sum_{ij} a_i b_j \\underline{e}_i \\wedge \\underline{e}_j \\\\ &amp;= \\sum_{ij} a_i b_j \\left( \\sum_k \\epsilon_{ijk} \\ \\underline{e}_k \\right) \\\\ &amp;= \\sum_k \\left( \\sum_{ij} \\epsilon_{ijk} a_i b_j \\right) \\underline{e}_k \\\\ \\text{Hence } (\\underline{a} \\wedge \\underline{b})_k &amp;= \\sum_{ij} \\epsilon_{ijk} a_i b_j \\\\ \\text{e.g. } (\\underline{a} \\wedge \\underline{b})_3 &amp;= \\sum_{ij} \\epsilon_{ij3} a_i b_j \\\\ &amp;= \\epsilon_{123} a_1 b_2 + \\epsilon_{213} a_2 b_1 \\\\ &amp;= a_1 b_2 - a_2 b_1. \\end{align*}\\] 2.7.2 Summation Convention With component/ index notation, we observe that indices that appear twice in a given term are (usually) summed over. In the summation convention we omit \\(\\sum\\) signs for repeated indices: the sum is understood. Example 2.4 \\[\\begin{align*} a_i \\delta_{ij} &amp;= a_1 \\delta_{1j} + a_2 \\delta_{2j} + a_3 \\delta_{3j} \\\\ &amp;= \\begin{cases} a_1 &amp; \\text{if} j = 1 \\\\ a_2 &amp; \\text{if} j = 2 \\\\ a_3 &amp; \\text{if} j = 3 \\end{cases} \\\\ a_i \\delta_{ij} &amp;= a_j \\end{align*}\\] Example 2.5 \\[\\begin{align*} \\underline{a} \\cdot \\underline{b} &amp;= \\delta_{ij} a_i b_j \\\\ &amp;= a_i b_i \\end{align*}\\] Example 2.6 \\[\\begin{align*} (\\underline{a} \\wedge \\underline{b})_i = \\epsilon_{ijk} a_j b_k \\end{align*}\\] Example 2.7 \\[\\begin{align*} \\underline{a} \\cdot \\underline{b} \\wedge c &amp;= \\epsilon_{ijk} a_i b_j c_k \\\\ \\end{align*}\\] Example 2.8 \\[\\begin{align*} \\delta_{ii} = \\delta_{11} + \\delta_{22} + \\delta_{33} = 3 \\end{align*}\\] Example 2.9 \\[\\begin{align*} \\left[ (\\underline{a} \\cdot \\underline{c}) \\underline{b} - (\\underline{a} \\cdot \\underline{b}) \\underline{c} \\right]_i &amp;= (\\underline{a} \\cdot \\underline{c}) \\underline{b}_i - (\\underline{a} \\cdot \\underline{b}) \\underline{c}_i \\\\ &amp;= a_j c_j b_i - a_j b_j c_i \\end{align*}\\] 2.7.3 Rules An index that occurs exactly once in any term must appear once in every term and it can take any value - a free index. An index occurring exactly twice in a given term is summed over - a repeated/ contracted or dummy index. No index can occur more than twice. 2.7.4 Application Proof of the vector triple product identity. Consider Proof. \\[\\begin{align*} \\left[ \\underline{a} \\wedge (\\underline{b} \\wedge \\underline{c}) \\right]_i &amp;= \\epsilon_{ijk} a_j (\\underline{b} \\wedge \\underline{c})_k \\\\ &amp;= \\epsilon_{ijk} a_j \\epsilon_{kpq} b_p c_q \\\\ &amp;= \\epsilon_{ijk} \\epsilon_{pqk} a_j b_p c_q \\\\ \\epsilon_{ijk} \\epsilon_{pqk} &amp;= \\delta_{ip}\\delta_{jq} - \\delta_{iq} \\delta_{jp} \\text{ see below} \\\\ \\left[ \\underline{a} \\wedge (\\underline{b} \\wedge \\underline{c}) \\right]_i &amp;= (\\delta_{ip}\\delta_{jq} - \\delta_{iq}\\delta_{jp}) a_j b_p c_q \\\\ &amp;= a_j \\delta_{ip} b_p \\delta_{jq} c_q - a_j \\delta_{jp} b_p \\delta_{iq} c_q \\\\ &amp;= a_j b_i c_j - a_j b_j c_i,\\ \\delta_{ij} x_j = x_i = \\delta_{ji} x_j \\\\ &amp;= (a_j c_j) b_i - (a_j b_j) c_i \\\\ &amp;= (\\underline{a} \\cdot \\underline{c}) b_i - (\\underline{a} \\cdot \\underline{b}) c_i \\\\ &amp;= \\left[ (\\underline{a} \\cdot \\underline{c}) \\underline{b} - (\\underline{a} \\cdot \\underline{b}) \\underline{c} \\right]_i \\\\ \\text{True for } i &amp;= 1, 2, 3 \\text{ hence} \\\\ \\underline{a} \\wedge (\\underline{b} \\wedge \\underline{c}) &amp;= (\\underline{a} \\cdot \\underline{c}) \\underline{b} - (\\underline{a} \\cdot \\underline{b}) \\underline{c} \\end{align*}\\] 2.7.5 \\(\\epsilon \\epsilon\\) identities \\[\\begin{align*} \\epsilon_{ijk} \\epsilon_{pqk} &amp;= \\sum_k \\epsilon_{ijk} \\epsilon_{pqk} \\\\ &amp;= \\delta_{ip}\\delta_{jq} - \\delta_{iq} \\delta_{jp} \\\\ &amp;= \\epsilon_{kij} \\epsilon_{kpq} \\end{align*}\\]2 Check: RHS and LHS are both antisymmetric under \\(i \\leftrightarrow j\\) or \\(p \\leftrightarrow q\\). So both sides vanish if \\(i = j\\) or \\(p = q\\). Now suffices to check e.g. \\(i = p = 1\\) and \\(j = q = 2\\) \\[\\begin{align*} LHS &amp;= \\epsilon_{123} \\epsilon_{123} = 1 \\\\ RHS &amp;= \\delta_{11} \\delta_{22} - \\delta_{12} \\delta_{21} = 1 \\end{align*}\\] or \\(i = q = 1\\) and \\(j = p = 2\\) \\[\\begin{align*} LHS &amp;= \\epsilon_{123} \\epsilon_{213} = 1(-1) = -1 \\\\ RHS &amp;= \\delta_{12} \\delta_{21} - \\delta_{11} \\delta_{22} = -1 \\end{align*}\\] All other index choices work similarly, proof by exhaustion. \\(\\epsilon_{ijk} \\epsilon_{pjk} = 2\\delta_{ip}\\) contract result above \\[\\begin{align*} \\epsilon_{ijk} \\epsilon_{pqk} &amp;= \\delta_{ip}\\delta_{jj} - \\delta_{ij} \\delta_{jp} \\\\ &amp;= 3\\delta_{ip} - \\delta_{ip} \\\\ &amp;= 2 \\delta_{ip} \\end{align*}\\] \\(\\epsilon_{ijk} \\epsilon_{ijk} = 6\\) \\(\\underline{n}\\) is defined up to a choice of sign if \\(\\underline{a} \\nparallel \\underline{b}\\), but changing the sign of \\(\\underline{n}\\) means changing \\(\\theta\\) to \\(2\\pi - \\theta\\) so definition is unchanged; \\(\\underline{n}\\) is not defined if \\(\\underline{a} \\parallel \\underline{b}\\) and \\(\\theta\\) is not defined if \\(\\underline{a}\\) or \\(\\underline{b} = 0\\), but \\(\\underline{a} \\wedge \\underline{b} = \\underline{0}\\) in these cases. expected to know this and quote it "],["vectors-in-general-mathbbrn-and-mathbbcn.html", "3 Vectors in General; \\(\\mathbb{R}^n\\) and \\(\\mathbb{C}^n\\) 3.1 Vectors in \\(\\mathbb{R}^n\\) 3.2 Vector Spaces 3.3 Bases and Dimension 3.4 Vectors in \\(\\mathbb{C}^n\\)", " 3 Vectors in General; \\(\\mathbb{R}^n\\) and \\(\\mathbb{C}^n\\) 3.1 Vectors in \\(\\mathbb{R}^n\\) 3.1.1 Definitions If we regard vectors as sets of components, it is easy to generalise from 3 to \\(n\\) dimensions. Definition 3.1 \\(\\mathbb{R}^n = \\{ \\underline{x} = (x_1, x_2, \\dots, x_n) : x_i \\in \\mathbb{R} \\}\\) Definition 3.2   addition \\(\\underline{x} + \\underline{y} = (x_1 + y_1, \\dots, x_n + y_n)\\) for any \\(\\underline{x}, \\underline{y} \\in \\mathbb{R}^n\\) scalar multiplication \\(\\lambda x = (\\lambda x_1, \\dots, \\lambda x_n)\\) for any \\(\\underline{x} \\in \\mathbb{R}^n\\) and \\(\\lambda \\in \\mathbb{R}\\) Definition 3.3 (Inner/ scalar product) \\(\\underline{x} \\cdot \\underline{y} = \\sum_i x_i y_i = x_1 y_1 + \\dots + x_n y_n\\) Symmetric \\(\\underline{x} \\cdot \\underline{y} = \\underline{y} \\cdot \\underline{x}\\) Bilinear (linear in each vector) \\[\\begin{align*} (\\lambda \\underline{x} + \\lambda&#39; \\underline{x}) \\cdot \\underline{y} &amp;= \\lambda (\\underline{x} \\cdot \\underline{y}) + \\lambda&#39; (\\underline{x}&#39; \\cdot \\underline{y}) \\\\ (\\lambda \\underline{y} + \\lambda&#39; \\underline{y}) \\cdot \\underline{x} &amp;= \\lambda (\\underline{y} \\cdot \\underline{x}) + \\lambda&#39; (\\underline{y}&#39; \\cdot \\underline{x}) \\\\ \\end{align*}\\] Positive definite \\(\\underline{x} \\cdot \\underline{x} = \\sum_i x^2_i \\geq 0\\) and \\(= 0 \\iff \\underline{x} = \\underline{0}\\). The length or norm of vector \\(\\underline{x}\\) is \\(| \\underline{x} | \\geq 0\\) defined by \\(| \\underline{x} |^2 = \\underline{x} \\cdot \\underline{x}\\). For \\(\\underline{x} \\in \\mathbb{R}^n\\) we can write \\(\\underline{x} = \\sum_i x_i \\underline{e}_i\\) where \\[\\begin{align*} \\underline{e}_1 &amp;= (1, 0, \\dots, 0) \\\\ \\underline{e}_2 &amp;= (0, 1, \\dots, 0) \\\\ &amp;\\;\\;\\vdots \\\\ \\underline{e}_n &amp;= (0, 0, \\dots, 1). \\end{align*}\\] We call \\(\\{ \\underline{e}_i \\}\\) the standard basis for \\(\\mathbb{R}^n\\). Note it is orthonormal: \\[\\begin{align*} \\underline{e}_i \\cdot \\underline{e}_j = \\delta_{ij} = \\begin{cases} 1 &amp; \\text{ if } i = j \\\\ 0 &amp; \\text{ if } i \\neq j \\end{cases} \\end{align*}\\] 3.1.2 Cauchy-Schwarz and Triangle Inequalities Proposition 3.1 (Cauchy-Schwarz) \\(|\\underline{x} \\cdot \\underline{y}| \\leq |\\underline{x}| | \\underline{y}|\\) for some \\(\\underline{x}, \\underline{y} \\in \\mathbb{R}^n\\) and equality holds iff \\(\\underline{x} = \\lambda \\underline{y}\\) or \\(\\underline{y} = \\lambda \\underline{x}\\) \\((\\underline{x} \\parallel \\underline{y})\\) for some \\(\\lambda \\in \\mathbb{R}\\). Deductions reveal geometrical aspects of the inner product: Set \\(\\underline{x} \\cdot \\underline{y} = |\\underline{x}| |\\underline{y}| \\cos \\theta\\) to define angle \\(\\theta\\) between \\(\\underline{x}\\) and \\(\\underline{y}\\) Triangle inequality holds \\(| \\underline{x} + \\underline{y} | \\leq |\\underline{x}| + |\\underline{y}|\\). Proof (Cauchy-Schwarz). If \\(\\underline{y} = 0\\), the result is immediate. If \\(\\underline{y} \\neq 0\\), consider \\[\\begin{align*} |\\underline{x} - \\lambda \\underline{y}|^2 &amp;= (\\underline{x} - \\lambda \\underline{y}) \\cdot (\\underline{x} - \\lambda \\underline{y}) \\\\ &amp;= |\\underline{x}|^2 - 2 \\lambda \\underline{x} \\cdot \\underline{y} + \\lambda^2 | \\underline{y} | \\geq 0 \\end{align*}\\] This is a quadratic in \\(\\lambda \\in \\mathbb{R}\\) with at most one real root, so discriminant satisfies \\((-2 \\underline{x} \\cdot \\underline{y})^2 - 4 |\\underline{x}|^2 |\\underline{y}|^2 \\leq 0\\) as required. Equality holds iff discriminant \\(= 0\\) iff \\(\\lambda \\underline{y} = \\underline{x}\\) for some \\(\\lambda \\in \\mathbb{R}\\). Proof (Triangle inequality). \\[\\begin{align*} LHS^2 &amp;= |\\underline{x} + \\underline{y}|^2 = |\\underline{x}|^2 + 2 \\underline{x} \\cdot \\underline{y} + |\\underline{y}|^2 \\\\ RHS^2 &amp;= (|\\underline{x}| + |\\underline{y}|)^2 = |\\underline{x}|^2 + 2 |\\underline{x}| |\\underline{y}| + |\\underline{y}|^2 \\end{align*}\\] and compare using Cauchy-Schwarz. 3.1.3 Comments Inner product on \\(\\mathbb{R}^n\\) \\[\\begin{align*} \\underline{a} \\cdot \\underline{b} = \\delta_{ij} a_i b_j \\hspace{0.5cm} \\left(\\sum \\text{ convention and } i, j = 1, \\dots, n \\right) \\\\ \\end{align*}\\] Component definition matches geometrical definition for n = 3 Scalar or Dot Product. In \\(\\mathbb{R}^3\\) we also have a cross product with component definition \\((\\underline{a} \\wedge \\underline{b})_i = \\epsilon_{ijk} a_j b_k\\) (geometric definition given in Vector or Cross Product) In \\(\\mathbb{R}^n\\) we have \\(\\epsilon_{\\underbrace{ij...l}_\\text{$n$ indices}}\\) (totally antisymmetric). Cannot use this to define vector-valued product except in \\(n = 3\\). But in \\(\\mathbb{R}^2\\) we have \\(\\epsilon_{ij}\\) with \\(\\epsilon_{12} = \\epsilon_{21} = 1\\) and we can use this to define an additional scalar cross product in 2d. \\[\\begin{align*} [\\underline{a}, \\underline{b}] &amp;= \\epsilon_{ij} a_i b_j \\\\ &amp;= a_1 b_2 - a_2 b_1 \\text{ for } \\underline{a}, \\underline{b} \\in \\mathbb{R}^2 \\\\ \\end{align*}\\] Geometrically, this gives (signed) area of parallelogram \\([\\underline{a}, \\underline{b}] = |\\underline{a}| |\\underline{b}| \\sin \\theta\\). Compare with \\([\\underline{a}, \\underline{b}, \\underline{c}] = \\underline{a} \\cdot \\underline{b} \\wedge \\underline{c} = \\epsilon_{ijk} a_i b_j c_k\\) which is the (signed) volume of a parallelepiped. 3.2 Vector Spaces 3.2.1 Axioms; span; subspaces Let \\(V\\) be a set of objects called vectors with operations \\[\\begin{align*} \\left.\\begin{aligned} &amp;i. &amp; \\underline{v} + \\underline{w} &amp;\\in V \\\\ &amp;ii. &amp; \\lambda \\underline{v} &amp;\\in V \\end{aligned}\\right\\} \\text{defined } \\forall \\; v, w \\in V, \\text{ and } \\forall \\; \\lambda \\in \\mathbb{R}. \\end{align*}\\] Then \\(V\\) is a real vector space if \\(V\\) is an abelian group under addition and \\[\\begin{align*} \\lambda ( \\underline{v} + \\underline{w}) &amp;= \\lambda \\underline{v} + \\lambda \\underline{w} \\\\ (\\lambda + \\mu) \\underline{v} &amp;= \\lambda \\underline{v} + \\mu \\underline{v} \\\\ \\lambda (\\mu \\underline{v}) &amp;= (\\lambda \\mu) \\underline{v} \\\\ 1 \\underline{v} &amp;= \\underline{v} \\end{align*}\\]3 (the first three are same as Vector Addition and Scalar Multiplication) These axioms or key properties apply to geometrical vectors with \\(V\\) being a 3d space or to vectors in \\(V = \\mathbb{R}^n\\), as above, as well as other examples. For vectors \\(\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_r \\in V\\) we can form a linear combination \\(\\lambda_1 \\underline{v}_1 + \\lambda_2 \\underline{v}_2 + \\dots + \\lambda_r \\underline{v}_r \\in V\\) for any \\(\\lambda_i \\in \\mathbb{R}\\); the span is defined \\(\\operatorname{span} \\{ \\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_r \\} = \\{ \\sum_i \\lambda_i \\underline{v}_i : \\lambda_i \\in \\mathbb{R} \\}\\). A subspace of \\(V\\) is a subset that is itself a vector space. Note \\(V\\) and \\(\\{ 0 \\}\\) are subspaces. \\(\\operatorname{span} \\{ \\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_r \\}\\) is a subspace for any vectors \\(\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_r\\). Note: a non-empty subset \\(U \\subseteq V\\) is a subspace iff \\[\\begin{align*} \\underline{v}, \\underline{w} \\in U \\implies \\lambda \\underline{v} + \\mu \\underline{w} \\in U \\quad \\forall \\; \\lambda, \\mu \\in \\mathbb{R}. \\end{align*}\\] Example 3.1 In \\(\\mathbb{R}^3\\), a line or plane through \\(\\underline{0}\\) is a subspace but if it doesnt contain \\(\\underline{0}\\) it is not a subspace. e.g. \\[\\begin{gather} \\underline{v}_1 = \\begin{pmatrix}1 \\\\0 \\\\-1\\end{pmatrix}, \\underline{v}_2 = \\begin{pmatrix}1 \\\\1 \\\\-2\\end{pmatrix}, \\underline{n} = \\begin{pmatrix}1 \\\\1 \\\\1\\end{pmatrix} \\\\ \\operatorname{span} \\{ \\underline{v}_1, \\underline{v}_2\\} = \\{ \\underline{r} : \\underline{n} \\cdot \\underline{r} = 0 \\} \\text{ this is a plane and subspace} \\\\ \\text{But } \\{ \\underline{r} : \\underline{n} \\cdot \\underline{r} = 1 \\} \\text{ this is a plane but not a subspace } [\\underline{r}, \\underline{r}&#39; \\text{ on plane then } (\\underline{r} + \\underline{r}&#39;) \\cdot \\underline{n} = 2] \\\\ \\end{gather}\\] 3.2.2 Linear Dependence and Independence For vectors \\(\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_r \\in V\\) (where \\(V\\) is a real vector space) consider the linear relation \\[\\begin{align*} \\lambda_1 \\underline{v}_1 + \\lambda_2 \\underline{v}_2 + \\dots + \\lambda_r \\underline{v}_r = \\underline{0} \\tag{3.1} \\end{align*}\\] If (3.1) \\(\\implies \\lambda_i = 0\\) for every \\(i\\) then the vectors form a linearly independent set (they obey only the trivial linear relation with \\(\\lambda_i = 0\\)). If (3.1) holds with at least one \\(\\lambda_i \\neq 0\\) then the vectors form a linearly dependent set (they obey a non-trivial linear relation). Example 3.2 \\[\\begin{gather} \\left\\{ \\begin{pmatrix}1 \\\\0\\end{pmatrix}, \\begin{pmatrix}0 \\\\1\\end{pmatrix}, \\begin{pmatrix}0 \\\\2\\end{pmatrix} \\right\\} \\text{ are linearly dependent because:} \\\\ 0 \\begin{pmatrix}1 \\\\0\\end{pmatrix} + 2 \\begin{pmatrix}0 \\\\1\\end{pmatrix} + (-1) \\begin{pmatrix}0 \\\\2\\end{pmatrix} = \\underline{0} \\end{gather}\\] Note we cannot express \\(\\begin{pmatrix}1 \\\\0\\end{pmatrix}\\) in terms of the others. Example 3.3 Any set containing \\(\\underline{0}\\) is linearly dependent. e.g. \\[\\begin{align*} \\left\\{ \\begin{pmatrix}1 \\\\0\\end{pmatrix}, \\begin{pmatrix}0 \\\\0\\end{pmatrix}\\right\\} \\text{ have} \\\\ 0 \\begin{pmatrix}1 \\\\0\\end{pmatrix} + 412 \\begin{pmatrix}0 \\\\0\\end{pmatrix}= \\underline{0} \\end{align*}\\] which is a non-trivial linear relation. Example 3.4 \\(\\left\\{ \\underline{a}, \\underline{b}, \\underline{c} \\right\\}\\) in \\(\\mathbb{R}^3\\) are linearly independent if \\(\\underline{a} \\cdot \\underline{b} \\wedge \\underline{c} \\neq 0\\). Consider \\(\\alpha \\underline{a} + \\beta \\underline{b} + \\gamma \\underline{c} = \\underline{0}\\). Take dot with \\(\\underline{b} \\wedge \\underline{c}\\) to get \\(\\alpha \\underline{a} \\cdot \\underline{b} \\wedge \\underline{c} = 0 \\implies \\alpha = 0\\) and \\(\\beta = \\gamma = 0\\) similarly. 3.2.3 Inner product This is an additional structure on a real vector space \\(V\\), also characterised by axioms. For \\(\\underline{v}, \\underline{w} \\in V\\) write inner product \\(\\underline{v} \\cdot \\underline{w}\\) or \\((\\underline{v}, \\underline{w}) \\in \\mathbb{R}\\). This satisfies axioms corresponding to the properties in Definitions Symmetric Bilinear Positive definite Lemma 3.1 In a real vector space \\(V\\) with inner product, if \\(\\underline{v}_1, \\dots, \\underline{v}_r\\) are non-zero and orthogonal: \\[\\begin{align*} (\\underline{v}_i, \\underline{v}_i) \\neq 0 \\text { and } (\\underline{v}_i, \\underline{v}_j) = 0 \\text{ where } i \\neq j \\text{ and for fixed } i. \\end{align*}\\] then \\(\\underline{v}_1, \\dots, \\underline{v}_r\\) are linearly independent. Proof. \\[\\begin{align*} \\text{Consider } \\sum_i \\alpha_i \\underline{v}_i &amp;= \\underline{0} \\\\ (\\underline{v}_j, \\sum_i \\alpha_i \\underline{v}_i) &amp;= \\sum_i \\alpha_i (\\underline{v}_j, \\underline{v}_i) \\\\ 0 &amp;= \\alpha_j (\\underline{v}_j, \\underline{v}_j) \\text{ all fixed j} \\\\ \\implies \\alpha_j &amp;= 0. \\end{align*}\\] 3.3 Bases and Dimension For a vector space \\(V\\), a basis is a set \\[\\begin{align*} \\mathcal{B} = \\{ \\underline{e}_1, \\dots, \\underline{e}_n \\} \\end{align*}\\] such that \\(\\mathcal{B}\\) spans \\(V\\), i.e. any \\(\\underline{v} \\in V\\) can be written \\(\\underline{v} = \\sum_{i=1}^{n} v_i \\underline{e_i}\\) \\(\\mathcal{B}\\) is linearly independent. Given ii., the coefficients \\(v_i\\) in i. are unique since \\[\\begin{align*} \\sum_i v_i \\underline{e}_i &amp;= \\sum v_i&#39; \\underline{e}_i \\\\ \\implies \\sum_i (v_i - v_i&#39;) \\underline{e}_i &amp;= \\underline{0} \\\\ \\implies v_i &amp;= v_i&#39; \\end{align*}\\] \\(v_i\\) are components of \\(\\underline{v}\\) w.r.t. \\(\\mathcal{B}\\) Example 3.5 Standard basis for \\(\\mathbb{R}^n\\) consisting of \\[\\begin{align*} \\underline{e}_1 &amp;= (1, 0, \\dots, 0) \\\\ \\underline{e}_2 &amp;= (0, 1, \\dots, 0) \\\\ &amp;\\;\\;\\vdots \\\\ \\underline{e}_n &amp;= (0, 0, \\dots, 1). \\end{align*}\\] is a basis according to general definition. \\[\\begin{align*} \\underline{x} = \\begin{pmatrix}x_1 \\\\\\vdots \\\\x_n\\end{pmatrix} = x_1 \\underline{e}_1 + \\dots + x_n \\underline{e}_n \\\\ \\end{align*}\\] \\(\\underline{x} = \\underline{0} \\iff x_1 = x_2 = \\dots = x_n = 0\\) Many other bases can be chosen Example 3.6 In \\(\\mathbb{R}^2\\) we have bases \\[\\begin{align*} \\left\\{ \\begin{pmatrix}0 \\\\1\\end{pmatrix}, \\begin{pmatrix}1 \\\\1\\end{pmatrix} \\right\\} \\text{ or } \\left\\{ \\begin{pmatrix}1 \\\\1\\end{pmatrix}, \\begin{pmatrix}1 \\\\ -1\\end{pmatrix} \\right\\} \\end{align*}\\] or \\(\\{ \\underline{a}, \\underline{b} \\}\\) for any \\(\\underline{a}, \\underline{b} \\in \\mathbb{R}^2\\) with \\(\\underline{a} \\nparallel \\underline{b}\\). Example 3.7 In \\(\\mathbb{R}^3, \\{ \\underline{a}, \\underline{b}, \\underline{c} \\}\\) is a basis iff \\(\\underline{a} \\cdot \\underline{b} \\wedge \\underline{c} \\neq 0\\). Consider previous example of plane through \\(\\underline{0}\\), subspace in \\(\\mathbb{R}^3\\) \\[\\begin{align*} \\underline{n} \\cdot \\underline{r} = 0 \\text{ with } \\underline{n} = \\begin{pmatrix}1 \\\\1 \\\\1\\end{pmatrix} \\end{align*}\\] we have \\[\\begin{align*} \\{ \\underline{v}_1, \\underline{v}_2 \\} \\text{ basis with } \\underline{v}_1 = \\begin{pmatrix}1 \\\\0 \\\\-1\\end{pmatrix}, \\underline{v}_2 = \\begin{pmatrix}1 \\\\1 \\\\-2\\end{pmatrix} \\end{align*}\\] not normalised or orthogonal. But we could choose orthonormal basis \\[\\begin{align*} \\{ \\underline{u}_1, \\underline{u}_2 \\} \\text{ with } \\underline{u}_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\-1 \\\\ 0 \\end{pmatrix} \\text{ and } \\underline{u}_2 = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} 1 \\\\1 \\\\ -2 \\end{pmatrix}. \\end{align*}\\] Theorem 3.1 If \\(\\{ \\underline{e}_1, \\dots, \\underline{e}_n \\}\\) and \\(\\{ \\underline{f}_1, \\dots, \\underline{f}_m \\}\\) are bases for a real vector space \\(V\\), then \\(n = m\\). Definition 3.4 The number of vectors in any basis is the dimension of \\(V\\), \\(\\dim V\\). Note: \\(\\mathbb{R}^n\\) has dimension \\(n\\). Proof. \\[\\begin{align*} \\underline{f}_a &amp;= \\sum_i A_{ai} \\underline{e}_i \\\\ \\text{and } \\underline{e}_i &amp;= \\sum_i B_{ia} \\underline{f}_a \\\\ \\end{align*}\\] for constants \\(A_{ai}\\) and \\(B_{ia}\\) and we use ranges of indices \\(i, j = 1, \\dots, n\\) and \\(a, b = 1, \\dots, m\\).4 \\[\\begin{align*} \\implies \\underline{f}_a &amp;= \\sum_i A_{ai} \\left( \\sum_b B_{ib} \\underline{f}_b \\right) \\\\ &amp;= \\sum_b \\left( \\sum_i A_{ai} B_{ib} \\right) \\underline{f}_b. \\end{align*}\\] But coeffs w.r.t a basis are unique so \\(\\sum_i A_{ai} B_{ib} = \\delta_{ab}\\). Similarly, \\[\\begin{align*} \\underline{e}_i &amp;= \\sum_j \\left( \\sum_a B_{ia} A_{aj} \\right) \\underline{e}_j \\end{align*}\\] and hence \\[\\begin{align*} \\sum_a B_{ia} A_{aj} = \\delta_{ij}. \\end{align*}\\] Now \\[\\begin{align*} \\sum_{i, a} A_{ai} B_{ia} &amp;= \\sum_a \\delta_{aa} = m \\\\ &amp;= \\sum_{i, a} B_{ia} A_{ai} = \\sum_i \\delta_{ii} = n \\\\ \\implies m &amp;= n \\end{align*}\\] 5 Note: by convention the vector space \\(\\{ 0 \\}\\) has dimension \\(0\\). Not every vector space is finite dimensional. Proposition 3.2 Let \\(V\\) be a vector space of dimension \\(n\\) If \\(Y = \\{ \\underline{w}_1, \\dots, \\underline{w}_m \\}\\) spans V, then \\(m \\geq n\\) and if \\(m &gt; n\\) vectors can be removed from \\(Y\\) to get a basis. If \\(X = \\{ \\underline{u}_1, \\dots, \\underline{u}_k \\}\\) are linearly independent then \\(k \\leq n\\) and if \\(k &lt; n\\) vectors can be added to \\(X\\) to get a basis. 3.4 Vectors in \\(\\mathbb{C}^n\\) 3.4.1 Definitions Definition 3.5 \\(\\mathbb{C}^n = \\{ \\underline{z} = (z_1, \\dots, z_n) : z_j \\in \\mathbb{C} \\}\\) Definition 3.6 addition: \\(\\underline{z} + \\underline{w} = \\left( z_1 + w_1, \\dots, z_n + w_n \\right)\\) scalar multiplication: \\(\\lambda \\underline{z} = (\\lambda z_1, \\dots, \\lambda z_n)\\) for any \\(\\underline{z}, \\underline{w} \\in \\mathbb{C}^n\\). Taking real scalars \\(\\lambda, \\mu \\in \\mathbb{R}\\), \\(\\mathbb{C}^n\\) is a real vector space obeying axioms or key properties in Axioms; span; subspaces. Taking complex scalars \\(\\lambda, \\mu \\in \\mathbb{C}\\), \\(\\mathbb{C}^n\\) is a complex vector space - same axioms/ key properties hold, and definitions of linear combinations, linear (in)dependence, span, bases, dimension all generalise to complex scalars. The distinction matters \\[\\begin{align*} e.g. \\ \\underline{z} &amp;= (z_1, \\dots, z_n) \\in \\mathbb{C}^n \\\\ \\text{with } z_j &amp;= x_j + i y_j \\hspace{0.5cm} x_j, y_j \\in \\mathbb{R} \\\\ \\text{then } \\underline{z} &amp;= \\sum_j x_j \\underline{e}_j + \\sum_j y_j \\underline{f}_j \\text{ is a real linear combination} \\\\ \\text{where } \\underline{e}_j &amp;= \\begin{pmatrix}0 &amp; \\dots &amp; 1 &amp; \\dots &amp; 0\\end{pmatrix} \\\\ \\underline{f}_j &amp;= \\begin{pmatrix}0 &amp; \\dots &amp; i &amp; \\dots &amp; 0\\end{pmatrix} \\\\ \\end{align*}\\] \\(\\therefore \\{\\underline{e}_1, \\dots, \\underline{e}_n, \\underline{f}_1, \\dots, \\underline{f}_n \\}\\) is a basis for \\(\\mathbb{C}^n\\) as a real vector space. So the real dimension is \\(2n\\). But \\(\\underline{z} = \\sum_j z_j \\underline{e}_j\\) and \\(\\{ \\underline{e}_1, \\dots, \\underline{e}_n \\}\\) is a basis for \\(\\mathbb{C}^n\\) as a complex vector space, dimension \\(n\\) (over \\(\\mathbb{C}\\)). 3.4.2 Inner Product Inner product or scalar product on \\(\\mathbb{C}^n\\) is defined by \\[\\begin{align*} (\\underline{z}, \\underline{w}) &amp;= \\sum_j \\bar{z}_j w_j = \\bar{z}_1 w_1 + \\dots + \\bar{z}_n w_n \\\\ \\end{align*}\\] 3.4.2.1 Properties hermitian \\[\\begin{align*} (\\underline{w}, \\underline{z}) = \\overline{(\\underline{z}, \\underline{w})} \\end{align*}\\] linear/ anti-linear \\[\\begin{align*} (\\underline{z}, \\lambda \\underline{w} + \\lambda&#39; \\underline{w}&#39;) &amp;= \\lambda (\\underline{z}, \\underline{w}) + \\lambda&#39; (\\underline{z}, \\underline{w}&#39;) \\\\ (\\mu \\underline{z} + \\mu&#39; \\underline{z}&#39;, \\underline{w}) &amp;= \\bar{\\mu} (\\underline{z}, \\underline{w}) + \\overline{\\mu}&#39; (\\underline{z}&#39;, \\underline{w}) \\end{align*}\\] positive definite \\[\\begin{align*} (\\underline{z}, \\underline{z}) &amp;= \\sum_i |z_i|^2 \\in \\mathbb{R}_{\\geq 0} \\\\ &amp;= 0 \\iff \\underline{z} = \\underline{0} \\end{align*}\\] Define length or norm of \\(\\underline{z}\\) to be \\(|\\underline{z}| \\geq 0\\) with \\(|\\underline{z}|^2 = (\\underline{z}, \\underline{z})\\). Define \\(\\underline{z}, \\underline{w} \\in \\mathbb{C}^n\\) to be orthogonal if \\((\\underline{z}, \\underline{w}) = 0\\). Note, the standard basis \\(\\{ \\underline{e}_j \\}\\) for \\(\\mathbb{C}^n\\) (see definitions) is orthonormal \\[\\begin{align*} (\\underline{e}_i, \\underline{e}_j) = \\delta_{ij}. \\end{align*}\\] Also if \\(\\underline{z}_1, \\underline{z}_2, \\dots, \\underline{z}_k\\) are non-zero and orthogonal in sense above, then they are linearly independent over \\(\\mathbb{C}\\) (same argument as in real case). Example 3.8 Complex inner product on \\(\\mathbb{C}\\) (\\(n = 1\\)) is \\[\\begin{align*} (z, w) &amp;= \\bar{z} w \\\\ \\text{Let } z &amp;= a_1 + i a_2,\\ w = b_1 + b_2 \\\\ \\text{Then } \\underline{a} &amp;= (a_1, a_2) \\in \\mathbb{R}^2,\\ \\underline{b} = (b_1, b_2) \\in \\mathbb{R}^2 \\text{ the corresponding vectors} \\\\ \\bar{z} w &amp;= (a_1 b_1 + a_2 b_2) + i (a_1 b_2 - a_2 b_1) \\\\ &amp;= \\underline{a} \\cdot \\underline{b} + i [\\underline{a}, \\underline{b}] \\end{align*}\\] recover scalar dot and scalar cross product in \\(\\mathbb{R}^2\\). dont need to state these in tripos This is true since \\(\\{ \\underline{e}_i\\}\\) and \\(\\{ \\underline{f}_a \\}\\) are bases Dont need to memorise proof but you there may be a question where you are lead through the proof "],["matrices-and-linear-maps.html", "4 Matrices and Linear Maps 4.1 Introduction 4.2 Geometrical Examples 4.3 Matrices as Linear Maps \\(\\mathbb{R}^n \\to \\mathbb{R}^m\\) 4.4 Matrices for Linear Maps in General 4.5 Matrix Algebra 4.6 Orthogonal and Unitary Matrices", " 4 Matrices and Linear Maps 4.1 Introduction 4.1.1 Definitions Definition 4.1 A linear map or linear transformation is a function \\[\\begin{align*} T : V \\to W \\end{align*}\\] between vector spaces \\(V\\ (\\dim n)\\) and \\(W\\ (\\dim m)\\) such that \\[\\begin{align*} T(\\lambda \\underline{x} + \\mu \\underline{y}) &amp;= \\lambda T(\\underline{x}) + \\mu T(\\underline{y}) \\\\ \\forall \\; \\underline{x}, \\underline{y} \\in V \\\\ \\forall \\; \\lambda, \\mu \\in \\mathbb{R} \\text{ or } \\mathbb{C} \\\\ \\end{align*}\\] for \\(V, W\\) both real or complex vector spaces.6 Note: a linear map is completely determined by its action on a basis \\(\\{ \\underline{e}_1, \\dots, \\underline{e}_n \\}\\) for \\(V\\), since \\[\\begin{align*} T\\left( \\sum_i x_i \\underline{e}_i \\right) = \\sum_i x_i T(\\underline{e}_i) \\end{align*}\\] \\(\\underline{x}&#39; = T(\\underline{x}) \\in W\\) is the image of \\(\\underline{x} \\in V\\) under T. \\(\\operatorname{Im}(T) = \\{ \\underline{x}&#39; \\in W: \\underline{x}&#39; = T(\\underline{x}) \\text{ for some } \\underline{x} \\in V \\}\\) is the image of \\(T\\). \\(\\ker(T) = \\{ \\underline{x} \\in V: \\underline{x}&#39; = T(\\underline{x}) = \\underline{0} \\}\\) is the kernel of \\(T\\). Lemma 4.1 \\(\\ker(T)\\) is a subspace of \\(V\\) and \\(\\operatorname{Im}(T)\\) is a subspace of \\(W\\). Proof. \\(\\underline{x}, \\underline{y} \\in \\ker(T) \\implies T(\\lambda \\underline{x} + \\mu \\underline{y}) = \\lambda T(\\underline{x}) + \\mu T(\\underline{y}) = \\underline{0}\\) and \\(\\underline{0} \\in \\ker(T)\\), so results follows. Also \\(\\underline{0} \\in \\operatorname{Im}(T)\\) and \\(\\underline{x}&#39;, \\underline{y}&#39; \\in \\operatorname{Im}(T)\\) then \\(T(\\lambda \\underline{x} + \\mu \\underline{y}) = \\lambda T(\\underline{x}) + \\mu T(\\underline{y}) = \\lambda \\underline{x}&#39; + \\mu \\underline{y}&#39; \\in \\operatorname{Im}(T)\\) for some \\(\\underline{x}, \\underline{y} \\in V\\). Example 4.1 Zero linear map \\(T : V \\to W\\) is given by \\(T(\\underline{x}) = \\underline{0} \\ \\forall \\; \\underline{x} \\in V\\). \\(\\operatorname{Im}(T) = \\{ \\underline{0} \\}\\) and \\(\\ker(T) = V\\) Example 4.2 For \\(V = W\\), the identity linear map \\(T: V \\to V\\) is given by \\(T(\\underline{x}) = \\underline{x} \\; \\forall \\; x \\in V\\). \\(\\operatorname{Im}(T) = V\\) and \\(\\ker(T) = \\{ \\underline{0} \\}\\) Example 4.3 \\(V = W = \\mathbb{R}^3\\), \\(\\underline{x}&#39; = T(\\underline{x})\\) given by \\[\\begin{align*} x_1&#39; &amp;= 3 x_1 + x_2 + 5 x_3 \\\\ x_2&#39; &amp;= - x_1 - 2 x_3 \\\\ x_3&#39; &amp;= 2 x_1 + x_2 + 3 x_3 \\\\ \\ker(T) &amp;= \\left\\{ \\lambda \\begin{pmatrix}2 \\\\-1 \\\\-1\\end{pmatrix} \\right\\} \\hspace{0.5cm} (\\dim 1) \\\\ \\operatorname{Im}(T) &amp;= \\left\\{ \\lambda \\begin{pmatrix}3 \\\\-1 \\\\2\\end{pmatrix} + \\mu \\begin{pmatrix}1 \\\\0 \\\\1\\end{pmatrix} \\right\\} \\hspace{0.5cm} (\\dim 2) \\end{align*}\\] 4.1.2 Rank and Nullity \\(\\dim \\operatorname{Im}(T)\\) is the rank of \\(T\\) (\\(\\leq n\\)). \\(\\dim \\ker(T)\\) is the nullity of \\(T\\) (\\(\\leq n\\)). Theorem 4.1 (rank-nullity) For \\(T : V \\to W\\) a linear map, 4.1 \\[\\begin{align*} \\operatorname{rank}(T) + \\operatorname{null}(T) &amp;= n = \\dim V \\end{align*}\\] Example 4.4 same as those in Definitions \\(\\operatorname{rank}(T) + \\operatorname{null}(T) = 0 + n = n\\) \\(\\operatorname{rank}(T) + \\operatorname{null}(T) = n + 0 = n\\) \\(\\operatorname{rank}(T) + \\operatorname{null}(T) = 2 + 1 = 3\\) Non-examinable Proof. Let \\(\\underline{e}_1, \\dots, \\underline{e}_k\\) be a basis for \\(\\ker(T)\\) so \\(T(\\underline{e}_i) = 0\\) for \\(i = 1, \\dots, k\\). Extend by \\(\\underline{e}_{k + 1}, \\dots, \\underline{e}_n\\) to get a basis for \\(V\\). Claim \\[\\begin{align*} \\mathcal{B} &amp;= \\{ T(\\underline{e}_{k + 1}), \\dots, T(\\underline{e}_n) \\} \\end{align*}\\] is the basis for \\(\\operatorname{Im}(T)\\). The result then follows since \\(\\operatorname{null}(T) = k\\) and \\(\\operatorname{rank}(T) = n - k\\), implying \\(\\operatorname{null}(T) + \\operatorname{rank}(T) = n\\). To check claim: \\(\\mathcal{B}\\) spans \\(\\operatorname{Im}(T)\\) since \\(\\underline{x} = \\sum_{i=1}^{n} x_i \\underline{e}_i\\) \\[\\begin{align*} \\implies T(\\underline{x}) = \\sum_{i=1}^{n} x_i T(\\underline{e}_i) = \\sum_{i = k + 1}^{n} x_i T(\\underline{e}_i) \\end{align*}\\] \\(\\mathcal{B}\\) is linearly independent since \\[\\begin{align*} \\sum_{i=k+1}^{n} \\lambda_i T(\\underline{e}_i) &amp;= \\underline{0} \\\\ \\implies T(\\sum_{i=k+1}^{n} \\lambda_i \\underline{e}_i) &amp;= \\underline{0} \\\\ \\implies \\sum_{i=k+1}^{n} \\lambda_i \\underline{e}_i &amp;\\in \\ker(T) \\\\ \\implies \\sum_{i=k+1}^{n} \\lambda_i \\underline{e}_i &amp;= \\sum_{i=1}^{k} \\mu_i \\underline{e}_i \\\\ \\end{align*}\\] But \\(\\underline{e}_1, \\dots, \\underline{e}_n\\) are linearly independent in \\(V\\) \\[\\begin{align*} \\implies \\lambda_i &amp;= 0 \\\\ \\mu_i &amp;= 0 \\end{align*}\\] 4.2 Geometrical Examples 4.2.1 Rotations In \\(\\mathbb{R}^2\\), a rotation about \\(\\underline{0}\\) through angle \\(\\theta\\) is defined by \\[\\begin{align*} \\underline{e}_1 &amp;\\mapsto \\underline{e}_1&#39; = \\ \\; (\\cos \\theta) \\underline{e}_1 + (\\sin \\theta) \\underline{e}_2 \\\\ \\underline{e}_2 &amp;\\mapsto \\underline{e}_2&#39; = -(\\sin \\theta) \\underline{e}_1 + (\\cos \\theta) \\underline{e}_2 \\end{align*}\\] In \\(\\mathbb{R}^3\\), rotation about axis given by \\(\\underline{e}_3\\) is defined as above, with \\[\\begin{align*} \\underline{e}_3 \\mapsto \\underline{e}_3&#39; = \\underline{e}_3 \\end{align*}\\] Now consider rotation about any axis \\(\\underline{n}\\) (a unit vector). Given \\(\\underline{x}\\), resolve \\(\\parallel\\) and \\(\\perp\\) to \\(\\underline{n}\\): \\[\\begin{align*} \\underline{x} &amp;= \\underline{x}_\\parallel + \\underline{x}_\\perp \\hspace{0.5cm} \\text{with } \\underline{x}_\\parallel = (\\underline{x} \\cdot \\underline{n}) \\underline{n} \\ (\\implies \\underline{n} \\cdot \\underline{x}_\\perp = \\underline{0}) \\\\ \\text{Under rotation} \\\\ \\underline{x}_\\parallel &amp;\\mapsto \\underline{x}_\\parallel&#39; = \\underline{x}_\\parallel \\\\ \\underline{x}_\\perp &amp;\\mapsto \\underline{x}_\\perp&#39; = (\\cos \\theta) \\underline{x}_\\perp + (\\sin \\theta) \\underline{n} \\wedge \\underline{x}, \\end{align*}\\] by considering plane \\(\\perp \\underline{n}\\), comparing to rotation in \\(\\mathbb{R}^2\\) and noting that \\(|\\underline{x}_\\perp| = | \\underline{n} \\wedge \\underline{x} |\\). \\[\\begin{align*} \\underline{x} \\mapsto \\underline{x}&#39; &amp;= \\underline{x}_\\parallel&#39; + \\underline{x}_\\perp&#39; \\\\ &amp;= (\\cos \\theta) \\underline{x} + (1 - \\cos \\theta) (\\underline{n} \\cdot \\underline{x}) \\underline{n} + \\sin \\theta \\underline{n} \\wedge \\underline{x}. \\end{align*}\\] 4.2.2 Reflections Consider a reflection in a plane in \\(\\mathbb{R}^3\\) (or line in \\(\\mathbb{R}^2\\)) through \\(\\underline{0}\\) with unit normal \\(\\underline{n}\\). Given \\(\\underline{x}\\), resolve \\(\\parallel\\) and \\(\\perp\\) to \\(\\underline{n}\\): \\[\\begin{align*} \\underline{x}_\\parallel &amp;\\mapsto \\underline{x}_\\parallel&#39; = -\\underline{x}_\\parallel \\\\ \\underline{x}_\\perp &amp;\\mapsto \\underline{x}_\\perp&#39; = \\underline{x}_\\perp \\end{align*}\\] \\[\\begin{align*} \\underline{x} &amp;\\mapsto \\underline{x}&#39; = \\underline{x} - 2 (\\underline{x} \\cdot \\underline{n}) \\underline{n} \\\\ \\end{align*}\\] 4.2.3 Dilations A dilation by scale factors \\(\\alpha, \\beta, \\gamma\\) (real, \\(&gt; 0\\)) along axes \\(\\underline{e}_1, \\underline{e}_2, \\underline{e}_3\\) in \\(\\mathbb{R}^3\\) is defined by \\[\\begin{align*} \\underline{x} &amp;= x_1 \\underline{e}_1 + x_2 \\underline{e}_2 + x_3 \\underline{e}_3 \\\\ \\mapsto \\underline{x}&#39; &amp;= \\alpha x_1 \\underline{e}_1 + \\beta x_2 \\underline{e}_2 + \\gamma x_3 \\underline{e}_3. \\end{align*}\\] A dilation maps a unit cube to a cuboid. 4.2.4 Shears Given \\(\\underline{a}, \\underline{b}\\) orthogonal unit vectors define a shear with parameter \\(\\lambda\\) by \\[\\begin{align*} \\underline{x} &amp;\\mapsto \\underline{x}&#39; = \\underline{x} + \\lambda (\\underline{x} \\cdot \\underline{b}) \\underline{a} \\end{align*}\\] Definition applies in \\(\\mathbb{R}^n\\) and \\(\\underline{u}&#39; = u\\) for any vector \\(\\underline{u} \\perp \\underline{b}\\). 4.3 Matrices as Linear Maps \\(\\mathbb{R}^n \\to \\mathbb{R}^m\\) 4.3.1 Definitions Consider a linear map \\(T : \\mathbb{R}^n \\to \\mathbb{R}^m\\) and standard bases \\(\\{ \\underline{e}_i \\}\\) and \\(\\{ \\underline{f}_a \\}\\) respectively. Let \\(\\underline{x}&#39; = T(\\underline{x})\\) with \\[\\begin{align*} \\underline{x} = \\sum_i x_i \\underline{e}_i = \\begin{pmatrix}x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix},\\ \\underline{x}&#39; = \\sum_a x_a&#39; \\underline{f}_a = \\begin{pmatrix}x_1&#39; \\\\ \\vdots \\\\ x_m&#39;\\end{pmatrix} \\end{align*}\\] Linearity implies \\(T\\) is determined by \\(T(\\underline{e}_i) = \\underline{e}_i&#39; = \\underline{C}_i \\in \\mathbb{R}^m \\ (i = 1, \\dots, n)\\); take these ase columns of a \\(m \\times n\\) array or matrix \\(M\\) with rows \\[\\begin{align*} \\underline{R}_a \\in \\mathbb{R}^n \\ (a = 1, \\dots, m). \\end{align*}\\] \\(M\\) has entries \\(M_{ai} \\in \\mathbb{R}\\) where \\(a\\) labels rows and \\(i\\) labels columns. \\[\\begin{align*} \\begin{pmatrix} \\uparrow &amp; &amp; \\uparrow \\\\ \\underline{C}_1 &amp; \\dots &amp; \\underline{C}_n \\\\ \\downarrow &amp; &amp; \\downarrow \\end{pmatrix} = M = \\begin{pmatrix} \\leftarrow &amp; \\underline{R}_1 &amp; \\rightarrow \\\\ &amp; \\vdots &amp; \\\\ \\leftarrow &amp; \\underline{R}_m &amp; \\rightarrow \\end{pmatrix} \\end{align*}\\] \\((\\underline{C}_i)_a = M_{ai} = (\\underline{R}_a)_i\\). Action of \\(T\\) is given by matrix \\(M\\) multiplying vector \\(\\underline{x}\\), \\(\\underline{x}&#39; = M \\underline{x}\\) defined by \\(x_a&#39; = M_{ai} x_i\\) (\\(\\sum\\) convention). This follows from definitions above since \\[\\begin{align*} \\underline{x}&#39; &amp;= T\\left( \\sum_i x_i \\underline{e}_i \\right) = \\sum_i x_i \\underline{C}_i \\\\ \\implies (\\underline{x}&#39;)_a &amp;= \\sum_i x_i (\\underline{C}_i)_a \\\\ &amp;= \\sum_i M_{ai} x_i \\\\ &amp;= \\sum_i (\\underline{R}_a) x_i \\\\ &amp;= \\underline{R}_a \\cdot x \\end{align*}\\] Now regard properties of \\(T\\) as properties of \\(M\\). \\(\\operatorname{Im}(T) = \\operatorname{Im}(M) = \\operatorname{span} \\{ \\underline{C}_1, \\dots, \\underline{C}_n \\}\\), the image of M (or T) is the span of the columns. \\(\\ker(T) = \\ker(M) = \\{\\underline{x} : \\underline{R}_a \\cdot \\underline{x} = 0 \\; \\forall \\; a \\}\\), kernel of M is subspace \\(\\perp\\) all rows. 4.3.2 Examples Refer to Examples 4.1, 4.2, 4.3, 4.4. Example 4.5 Zero map \\(\\mathbb{R}^n \\to \\mathbb{R}^m\\) corresponds to zero matrix \\(M = 0\\) with \\(M_{ai} = 0\\). Example 4.6 Identity map \\(\\mathbb{R}^n \\to \\mathbb{R}^n\\) corresponds to identity matrix \\[\\begin{align*} M = I = \\begin{pmatrix} 1 &amp; &amp; &amp; \\smash{\\huge 0} \\\\ &amp; 1 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ \\huge 0 &amp; &amp; &amp; 1 \\end{pmatrix} \\end{align*}\\] with \\(I_{ij} = \\delta_{ij}\\). Example 4.7 \\(\\mathbb{R}^3 \\to \\mathbb{R}^3\\), \\(\\underline{x}&#39; = T(\\underline{x}) = M \\underline{x}\\) with \\[\\begin{align*} M &amp;= \\begin{pmatrix} 3 &amp; 1 &amp; 5 \\\\ -1 &amp; 0 &amp; -2 \\\\ 2 &amp; 1 &amp; 3 \\end{pmatrix}, \\underline{C}_1 = \\begin{pmatrix}3 \\\\-1 \\\\2\\end{pmatrix}, \\underline{C}_2 = \\begin{pmatrix}1 \\\\0 \\\\1\\end{pmatrix}, \\underline{C}_3 = \\begin{pmatrix}5 \\\\-2 \\\\3\\end{pmatrix} \\\\ \\operatorname{T} &amp;= \\operatorname{M} \\\\ &amp;= \\operatorname{span} \\{ \\underline{C}_1, \\underline{C}_2, \\underline{C}_3 \\} \\\\ &amp;= \\operatorname{span} \\{ \\underline{C}_1, \\underline{C}_2 \\} \\text{ since } \\underline{C}_3 = 2 \\underline{C}_1 - \\underline{C}_2 \\\\ \\underline{R}_1 &amp;= \\begin{pmatrix}3 &amp; 1 &amp; 5\\end{pmatrix} \\\\ \\underline{R}_2 &amp;= \\begin{pmatrix}-1 &amp; 0 &amp; 2\\end{pmatrix} \\\\ \\underline{R}_3 &amp;= \\begin{pmatrix}2 &amp; 1 &amp; 3\\end{pmatrix} \\\\ \\underline{R}_2 \\wedge \\underline{R}_3 &amp;= \\begin{pmatrix}2 &amp; -1 &amp; -1\\end{pmatrix} \\\\ &amp;= \\underline{u}, \\text{ say } \\perp \\text{ all rows (in fact)} \\\\ \\ker (T) &amp;= \\ker (M) = \\{ \\lambda \\underline{u} \\} \\end{align*}\\] Example 4.8 Rotation through \\(\\theta\\) about \\(\\underline{0}\\) in \\(\\mathbb{R}^2\\) \\[\\begin{align*} \\underline{e}_1 &amp;= \\begin{pmatrix}1 \\\\0\\end{pmatrix} \\mapsto \\begin{pmatrix} \\cos \\theta \\\\ \\sin \\theta\\end{pmatrix} = \\underline{C}_1 \\\\ \\underline{e}_2 &amp;= \\begin{pmatrix}0 \\\\1\\end{pmatrix} \\mapsto \\begin{pmatrix}- \\sin \\theta \\\\ \\cos \\theta\\end{pmatrix} = \\underline{C}_2 \\\\ \\implies M &amp;= \\begin{pmatrix} \\cos \\theta &amp; - \\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{pmatrix}. \\end{align*}\\] Example 4.9 Dilation \\(\\underline{x}&#39; = M \\underline{x}\\) with scale factors \\(\\alpha, \\beta, \\gamma\\) along axes in \\(\\mathbb{R}^3\\). \\[\\begin{align*} M = \\begin{pmatrix} \\alpha &amp; 0 &amp; 0 \\\\ 0 &amp; \\beta &amp; 0 \\\\ 0 &amp; 0 &amp; \\gamma \\end{pmatrix}. \\end{align*}\\] Example 4.10 Reflection in plane \\(\\perp \\underline{n}\\) (a unit vector). \\[\\begin{align*} \\underline{x}&#39; &amp;= H \\underline{x} = \\underline{x} - 2 (\\underline{x} \\cdot \\underline{n}) \\underline{n} \\\\ x_i&#39; &amp;= x_i - 2 x_j n_j n_i \\\\ &amp;= (\\delta_{ij} - 2 n_j n_i) x_j \\\\ H_{ij} &amp;= \\delta_{ij} - 2 n_j n_i \\\\ \\text{e.g. } \\underline{n} &amp;= \\frac{1}{\\sqrt{3}} \\begin{pmatrix}1 \\\\1 \\\\1\\end{pmatrix},\\ n_i n_j = \\frac{1}{3} \\quad \\forall \\; i, j \\\\ H &amp;= \\frac{1}{3} \\begin{pmatrix} 1 &amp; -2 &amp; -2 \\\\ -2 &amp; 1 &amp; -2 \\\\ -2 &amp; -2 &amp; 1 \\end{pmatrix} \\end{align*}\\] Example 4.11 Shear \\[\\begin{align*} \\underline{x}&#39; &amp;= S \\underline{x} = \\underline{x} + \\lambda (\\underline{b} \\cdot \\underline{x})\\underline{a} \\\\ x_i&#39; &amp;= S_{ij} x_j \\\\ \\text{with } S_{ij} &amp;= \\delta_{ij} + \\lambda a_i b_j \\end{align*}\\] e.g. in \\(\\mathbb{R}^2\\) with \\(\\underline{a} = \\begin{pmatrix}1 \\\\0\\end{pmatrix}\\) and \\(\\underline{b} = \\begin{pmatrix}0 \\\\1\\end{pmatrix}\\), \\[\\begin{align*} S = \\begin{pmatrix} 1 &amp; \\lambda \\\\ 0 &amp; 1 \\end{pmatrix}. \\end{align*}\\] Example 4.12 Rotation in \\(\\mathbb{R}^3\\) with axis \\(\\underline{n}\\) and angle \\(\\theta\\), \\[\\begin{align*} \\underline{x}&#39; &amp;= R \\underline{x} \\hspace{1cm} x_i&#39; = R_{ij} x_j \\\\ \\text{where } R_{ij} &amp;= \\delta_{ij} \\cos \\theta + (1 - \\cos \\theta) n_i n_j - (\\sin \\theta) \\epsilon_{ijk} n_k \\end{align*}\\] (see Example Sheet 2). 4.3.3 Isometries, area and determinants in \\(\\mathbb{R}^2\\) Consider a linear map \\(\\mathbb{R}^2 \\to \\mathbb{R}^2\\) given by \\(2 \\times 2\\) matrix \\(M\\): \\[\\begin{align*} \\underline{x} \\mapsto \\underline{x}&#39; = M \\underline{x} \\end{align*}\\] When is \\(M\\) an isometry, preserving lengths \\(|\\underline{x}&#39;| = |\\underline{x}|\\)? This is equivalent to preserving inner products \\(\\underline{x}&#39; \\cdot \\underline{y}&#39; = \\underline{x} \\cdot \\underline{y}\\) [since \\(\\underline{x} \\cdot \\underline{y} = \\frac{1}{2} ( |\\underline{x} + \\underline{y}|^2 - |\\underline{x}|^2 - |\\underline{y}|^2)\\)]. Necessary conditions are \\[\\begin{align*} M \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} &amp;= \\begin{pmatrix}\\cos \\theta \\\\\\sin \\theta\\end{pmatrix} \\text{ for some $\\theta$; most general unit vector in $\\mathbb{R}^2$} \\\\ M \\begin{pmatrix}0 \\\\1\\end{pmatrix} &amp;= \\pm \\begin{pmatrix}- \\sin \\theta \\\\ \\cos \\theta\\end{pmatrix} \\text{ general unit vector $\\perp M (1\\ 0)^T$}. \\end{align*}\\] Simple to check that these conditions are also sufficient and have two cases \\[\\begin{align*} M = R = \\begin{pmatrix} \\cos \\theta &amp; - \\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{pmatrix}, \\end{align*}\\] a rotation, or \\[\\begin{align*} M = H = \\begin{pmatrix} \\cos \\theta &amp; \\sin \\theta \\\\ \\sin \\theta &amp; - \\cos \\theta \\end{pmatrix}, \\end{align*}\\] a reflection. Compare with expression for reflection in 4.10 \\[\\begin{align*} H_{ij} &amp;= \\delta_{ij} - 2 n_i n_j \\\\ \\text{and note for } \\underline{n} &amp;= \\begin{pmatrix}n_1 \\\\n_2\\end{pmatrix} = \\begin{pmatrix} -\\sin \\frac{\\theta}{2} \\\\ \\cos \\frac{\\theta}{2}\\end{pmatrix} \\\\ \\text{we get } H &amp;= \\begin{pmatrix} 1 - 2 \\sin^2 \\frac{\\theta}{2} &amp; 2 \\sin \\frac{\\theta}{2} \\cos \\frac{\\theta}{2} \\\\ 2 \\sin \\frac{\\theta}{2} \\cos \\frac{\\theta}{2} &amp; 1 - 2 \\cos^2 \\frac{\\theta}{2} \\end{pmatrix} \\end{align*}\\] agreeing with H above. This is reflection in a line in \\(\\mathbb{R}^2\\) as shown How does \\(M\\) change areas in \\(\\mathbb{R}^2\\) (in general)? Consider unit square in \\(\\mathbb{R}^2\\), mapped to parallelogram as shown, with signed area \\([M \\underline{e}_1, M \\underline{e}_1]\\) scalar cross product. \\[\\begin{align*} \\left[ \\begin{pmatrix} M_{11} \\\\M_{21}\\end{pmatrix}, \\begin{pmatrix}M_{12} \\\\M_{22}\\end{pmatrix} \\right] &amp;= M_{11} M_{22} - M_{12} M_{21} \\\\ &amp;= \\det M, \\end{align*}\\] the determinant of \\(2 \\times 2\\) matrix. This is the factor (with a sign) by which areas are scaled under \\(M\\). Now compare with (i): \\[\\begin{align*} \\det R = + 1,\\ \\det H = -1. \\end{align*}\\] In either case \\(| \\det M | = + 1\\). Consider shear \\(S = \\begin{pmatrix}1 &amp; \\lambda \\\\0 &amp; 1\\end{pmatrix}\\); this has \\(\\det S = + 1\\) but it does not preserve lengths. 4.4 Matrices for Linear Maps in General Consider a linear map \\[\\begin{align*} T : V \\to W \\end{align*}\\] between real or complex vector spaces of \\(\\dim n, m\\) respectively and choose bases \\(\\{ \\underline{e}_i \\}\\) with \\(i = 1, \\dots, n\\) for \\(V\\) and \\(\\{ \\underline{f}_a \\}\\) with \\(a = 1, \\dots, m\\) for \\(W\\). The matrix \\(M\\) for \\(T\\) wrt the bases in an \\(m \\times n\\) array with entries \\(M_{ai} \\in \\mathbb{R}\\) or \\(\\mathbb{C}\\). It is defined by \\[\\begin{align*} T(\\underline{e}_i) &amp;= \\sum_a \\underline{f}_a M_{ai} \\text{ note index positions}. \\end{align*}\\] This is chose to ensure that \\(T(\\underline{x}) = \\underline{x}&#39;\\) where \\(\\underline{x} = \\sum_i x_i \\underline{e}_i\\) and \\(\\underline{x}&#39; = \\sum_a x_a&#39; \\underline{f}_a\\) iff \\(x_a&#39; = \\sum_i M_{ai} x_i\\). \\[\\begin{align*} \\text{i.e. } \\begin{pmatrix}x_1&#39; \\\\ \\vdots \\\\ x_m&#39;\\end{pmatrix} = \\begin{pmatrix} M_{11} &amp; \\dots &amp; M_{1n} \\\\ \\vdots &amp; &amp; \\vdots \\\\ M_{m1} &amp; \\dots &amp; M_{mn} \\end{pmatrix} \\begin{pmatrix}x_1 \\\\ \\vdots \\\\ x_n\\end{pmatrix}. \\end{align*}\\] Moral: given choice of bases \\(\\{ \\underline{e}_i \\}\\) and \\(\\{ \\underline{f}_a \\}\\) \\(V\\) is identified with \\(\\mathbb{R}^n\\) (or \\(\\mathbb{C}^n\\)) \\(W\\) is identified with \\(\\mathbb{R}^m\\) (or \\(\\mathbb{C}^m\\)) \\(T\\) is identified with \\(m \\times n\\) matrix \\(M\\). Note: there are natural ways to combine linear maps. If \\(S : V \\to W\\) is also linear, then so is \\(\\alpha T + \\beta S : V \\to W\\) defined by \\((\\alpha T + \\beta S) (\\underline{x}) = \\alpha T(\\underline{x}) + \\beta S(\\underline{x})\\). Or if \\(S : U \\to V\\) is also linear, then so is \\(T \\circ S : U \\to W\\) (composition of maps). 4.5 Matrix Algebra 4.5.1 Linear Combinations If \\(M\\) and \\(N\\) are \\(m \\times n\\) matrices then \\(\\alpha M + \\beta N\\) is an \\(m \\times n\\) matrix defined by \\[\\begin{align*} (\\alpha M + \\beta N)_{ai} &amp;= \\alpha M_{ai} + \\beta N_{ai} \\\\ (a = 1, \\dots, m&amp;; i = 1, \\dots, n) \\end{align*}\\] [If \\(M, N\\) represent linear maps \\(T, S : V \\to W\\), then \\(\\alpha M + \\beta N\\) represents \\(\\alpha T + \\beta S\\), all w.r.t. same choice of bases]. 4.5.2 Matrix multiplication If \\(A\\) is an \\(m \\times n\\) matrix, entries \\(A_{ai} \\in \\mathbb{R}\\) or \\(\\mathbb{C}\\) and \\(B\\) is an \\(n \\times p\\) matrix, entries \\(B_{ir}\\) then \\(AB\\) is an \\(m \\times p\\) matrix defined by \\((AB)_{ar} = A_{ai} B_{ir}\\) (\\(\\sum\\) convention). The product \\(AB\\) is not defined unless no. of cols of $A = $ no. of rows of B. \\[\\begin{align*} a &amp;= 1, \\dots, m \\\\ i &amp;= 1, \\dots, n \\\\ r &amp;= 1, \\dots, p. \\end{align*}\\] Matrix multiplication corresponds to the composition of linear maps. \\[\\begin{align*} [(AB) \\underline{x}]_a &amp;= (AB)_{ar} x_r \\text{ and compare} \\\\ [A(B \\underline{x})]_a &amp;= A_{ai}(B \\underline{x})_i = A_{ai} (B_{ir} x_r) \\\\ &amp;= (A_{ai} B_{ir}) x_r \\end{align*}\\] Example 4.13 \\[\\begin{align*} A &amp;= \\begin{pmatrix}1 &amp; 3 \\\\-5 &amp; 0 \\\\2 &amp; 1\\end{pmatrix},\\ B = \\begin{pmatrix}1 &amp; 0 &amp; -1 \\\\2 &amp; -1 &amp; 3\\end{pmatrix} \\\\ AB &amp;= \\begin{pmatrix} 7 &amp; -3 &amp; 8 \\\\ -5 &amp; 0 &amp; 5 \\\\ 4 &amp; -1 &amp; 1 \\end{pmatrix} \\\\ BA &amp;= \\begin{pmatrix} -1 &amp; 2 \\\\ 13 &amp; 9 \\end{pmatrix} \\end{align*}\\] 4.5.2.1 Helpful points of view Regarding \\(\\underline{x} \\in \\mathbb{R}^n\\) as a col vec or \\(n \\times 1\\) matrix, definition on matrix multiplication a matrix or vector agree. For product of \\(\\underbrace{A}_{m \\times n} \\underbrace{B}_{n \\times p}\\) have columns \\(\\underline{C}_r (B) \\in \\mathbb{R}^n\\) and \\(\\underline{C}_r (AB) \\in \\mathbb{R}^m\\) related by \\(\\underline{C}_r (AB) = A \\underline{C}_r (B)\\) \\[\\begin{align*} AB &amp;= \\begin{pmatrix} &amp; \\vdots &amp; \\\\ \\longleftarrow &amp; \\underline{R}_a (A) &amp; \\longrightarrow \\\\ &amp; \\vdots &amp; \\end{pmatrix} \\begin{pmatrix} &amp; \\big\\uparrow &amp; \\\\ \\dots &amp; \\underline{C}_r(B) &amp; \\dots \\\\ &amp; \\big\\downarrow &amp; \\end{pmatrix} \\\\ (AB)_{ar} &amp;= [\\underline{R}_a (A)]_i [\\underline{C}_r (B)]_i \\\\ &amp;= \\underline{R}_a (A) \\cdot \\underline{C}_r (B) \\text{ dot product in $\\mathbb{R}^n$ for real matrices.} \\end{align*}\\] 4.5.2.2 Properties of matrix products \\[\\begin{align*} (\\lambda M + \\mu N) P &amp;= \\lambda (MP) + \\mu (NP) \\\\ P(\\lambda M + \\mu N) &amp;= \\lambda (PM) + \\mu (PN) \\\\ (MN)P &amp;= M (NP) \\end{align*}\\] 4.5.3 Matrix Inverses Consider \\(A\\) \\(m \\times n\\) matrix and \\(B, C\\) \\(n \\times m\\) matrices, \\(B\\) is a left inverse for \\(A\\) if \\[\\begin{align*} BA = I_n; \\end{align*}\\] \\(C\\) is a right inverse for \\(A\\) if \\[\\begin{align*} AC = I_m. \\end{align*}\\] If \\(m = n\\), \\(A\\) is square, one of these implies the other and \\(B = C = A^{-1}\\), the inverse. \\[\\begin{align*} A A^{-1} = A^{-1} A = I. \\end{align*}\\] Not every matrix has an inverse; if it does it is called invertible or non-singular. Consider map \\(\\mathbb{R}^n \\to \\mathbb{R}^n\\) given by real matrix \\(M\\). If \\(\\underline{x}&#39; = M \\underline{x}\\) and \\(M^{-1}\\) exists then \\(\\underline{x} = M^{-1} \\underline{x}&#39;\\). For \\(n = 2\\) \\[\\begin{align*} x_1&#39; &amp;= M_{11} x_1 + M_{12} x_2 \\\\ x_2&#39; &amp;= M_{21} x_1 + M_{22} x_2 \\\\ \\implies M_{22} x_1&#39; - M_{12} x_2&#39; &amp;= (\\det M) x_1 \\\\ \\text{and } - M_{21} x_1&#39; + M_{11} x_2&#39; &amp;= (\\det M) x_2 \\\\ \\end{align*}\\] So if \\(\\det M = M_{11} M_{22} - M_{12} M_{21} \\neq 0\\) then \\(M^{-1} = \\frac{1}{\\det M} \\begin{pmatrix} M_{22} &amp; -M_{12} \\\\ -M_{21} &amp; M_{11} \\end{pmatrix}\\). Example 4.14 \\[\\begin{align*} R (\\theta) &amp;= \\begin{pmatrix} \\cos \\theta &amp; - \\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{pmatrix} \\\\ R(\\theta)^{-1} &amp;= R (-\\theta) \\end{align*}\\] Example 4.15 Reflection. \\[\\begin{align*} H (\\theta) &amp;= \\begin{pmatrix} \\cos \\theta &amp; \\sin \\theta \\\\ - \\sin \\theta &amp; \\cos \\theta \\end{pmatrix} \\\\ H(\\theta)^{-1} &amp;= H (\\theta) \\end{align*}\\] Example 4.16 \\[\\begin{align*} S (\\lambda) &amp;= \\begin{pmatrix} 1 &amp; \\lambda \\\\ 0 &amp; 1 \\end{pmatrix} \\\\ S(\\lambda)^{-1} &amp;= S (-\\lambda) \\end{align*}\\] 4.5.4 Transpose and Hermitian Conjugate 4.5.4.1 Transpose Definition 4.2 (Transpose) If \\(M\\) is an \\(m \\times n\\) matrix, the transpose \\(M^T\\) is an \\(n \\times m\\) matrix defined by \\[\\begin{align*} (M^T)_{ia} &amp;= M_{ai} \\ ``\\text{exchange rows and columns&quot;} \\\\ [a &amp;= 1, \\dots, m;\\ i = 1, \\dots, n] \\end{align*}\\] 4.5.4.1.1 Properties \\[\\begin{align*} (\\alpha A + \\beta B)^T &amp;= \\alpha A^T + \\beta B^T \\ (A, B \\ m \\times n) \\\\ (AB)^T &amp;= B^T A^T \\\\ \\text{Check: } [(AB)^T]_{ra} &amp;= (AB)_{ar} \\\\ &amp;= A_{ai} B_{ir} \\ \\left(\\sum \\text{ convention} \\right) \\\\ &amp;= (A^T)_{ia} (B^T)_{ri} \\\\ &amp;= (B^T)_{ri} (A^T)_{ia} \\\\ &amp;= (B^T A^T)_{ra} \\end{align*}\\] Note: \\(\\underline{x} = \\begin{pmatrix}x_1 \\\\\\vdots \\\\x_n\\end{pmatrix}\\) is a column vector or a \\(n \\times 1\\) matrix. \\(\\implies \\underline{x}^T = \\begin{pmatrix}x_1 &amp; \\dots &amp; x_n\\end{pmatrix}\\) is a row vector or \\(1 \\times n\\) matrix. Inner product on \\(\\mathbb{R}^n\\) is \\(\\underline{x} \\cdot \\underline{y} = \\underline{x}^T \\underline{y}\\), a scalar or \\(1 \\times 1\\) matrix, but \\(\\underline{y} \\underline{x}^T = M\\), a \\(n \\times n\\) matrix with \\(M_{ij} = y_i x_j\\). Definition 4.3 (Symmetric and Antisymmetric matrix) If \\(M\\) is square, \\(n \\times n\\), then \\(M\\) is symmetric iff \\(M^T = M\\) or \\(M_{ij} = M_{ji}\\) and antisymmetric iff \\(M^T = - M\\) or \\(M_{ij} = - M_{ji}\\). Any square matrix can be written as a sum of a symmetric and antisymmetric parts: \\[\\begin{align*} M = S + A \\text{ where } S &amp;= \\frac{1}{2} (M + M^T) \\\\ \\text{and } A &amp;= \\frac{1}{2} (M - M^T) \\end{align*}\\] Example 4.17 If \\(A\\) is \\(3 \\times 3\\) antisymmetric, then it can be rewritten in terms of vec \\(\\underline{a}\\) \\[\\begin{align*} A &amp;= \\begin{pmatrix} 0 &amp; a_3 &amp; -a_2 \\\\ -a_3 &amp; 0 &amp; a_1 \\\\ a_2 &amp; -a_1 &amp; 0 \\end{pmatrix} \\\\ A_{ij} &amp;= \\epsilon_{ijk} a_k \\text{ and } a_k = \\frac{1}{2} \\epsilon_{kij} A_{ij} \\\\ \\text{Then } (A \\underline{x})_i &amp;= A_{ij} x_j = \\epsilon_{ijk} a_k x_j \\\\ &amp;= (\\underline{x} \\wedge \\underline{a})_i \\end{align*}\\] 4.5.4.2 Hermitian Conjugate Definition 4.4 (Hermitian Conjugate) If \\(M\\) is a \\(m \\times n\\) matrix the hermitian conjugate \\(M^\\dagger\\) is defined by \\((M^\\dagger)_{ia} = \\overline{M}_{ai}\\) or \\(M^\\dagger = \\overline{M}^T = \\overline{M^T}\\) 4.5.4.2.1 Properties \\[\\begin{align*} (\\alpha A + \\beta B)^\\dagger &amp;= \\overline{\\alpha} A^\\dagger + \\overline{\\beta} B^\\dagger \\\\ (AB)^\\dagger &amp;= B^\\dagger A^\\dagger \\\\ \\end{align*}\\] Note: \\(\\underline{z} = \\begin{pmatrix}x_1 \\\\\\vdots \\\\x_n\\end{pmatrix}\\) is a column vector or a \\(n \\times 1\\) matrix. \\(\\implies \\underline{z}^\\dagger = \\begin{pmatrix}x_1 &amp; \\dots &amp; x_n\\end{pmatrix}\\) is a row vector or \\(1 \\times n\\) matrix. Inner product on \\(\\mathbb{C}^n\\) is \\((\\underline{z}, \\underline{w}) = \\underline{z}^\\dagger \\underline{w}\\), a scalar or \\(1 \\times 1\\) matrix. Definition 4.5 (Hermitian and Anti-Hermitian matrix) If \\(M\\) is square \\(n \\times n\\) then \\(M\\) is hermitian if \\(M^\\dagger = M\\) or \\(M_{ij} = \\overline{M}_{ji}\\) and anti-hermitian if \\(M^\\dagger = - M\\) or \\(M_{ij} = - \\overline{M}_{ji}\\). 4.5.4.3 Trace Definition 4.6 (Trace) For any square \\(n \\times n\\) matrix \\(M\\), the trace is defined by \\[\\begin{align*} \\operatorname{tr} (M) = M_{ii} \\text{ (the sum of diagonal entries.)} \\end{align*}\\] 4.5.4.3.1 Properties \\[\\begin{align*} \\operatorname{tr} (\\alpha M + \\beta N) &amp;= \\alpha \\operatorname{tr} (M) + \\beta \\operatorname{tr}(N) \\\\ \\operatorname{tr} (MN) &amp;= \\operatorname{tr} (NM) \\\\ [ \\text{check : } (MN)_{ii} &amp;= M_{ia} N_{ai} \\\\ &amp;= N_{ai} M_{ia} \\\\ &amp;= (NM)_{aa} ] \\\\ \\operatorname{tr} (M) &amp;= \\operatorname{tr} (M^T) \\\\ \\operatorname{tr} (I_n) &amp;= n \\\\ [I_{ij} = \\delta_{ij} \\text{ and } I_{ii} = \\delta_{ii} = n] \\end{align*}\\] We previously decomposed \\(M = S + A\\) (symmetric/ anti-symmetric parts). Let \\(T = S - \\frac{1}{n} (\\operatorname{tr} (S)) I\\) or \\(T_{ij} = S_{ij} - \\frac{1}{n} \\operatorname{tr} (S) \\delta_{ij}\\), then \\(T_{ii} = \\operatorname{T} = 0\\); and note \\(\\operatorname{M} = \\operatorname{S}\\) and \\(\\operatorname{A} = 0\\)7. So \\(M = \\underset{\\text{symm and traceless}}{T} + \\underset{\\text{antisymm part}}{A} + \\underset{\\text{pure trace}}{\\frac{1}{n} \\operatorname{tr}(M) I}\\) Example 4.18 \\[\\begin{align*} M &amp;= \\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 1 &amp; 2 &amp; 3 \\end{pmatrix} \\\\ S &amp;= \\begin{pmatrix} 1 &amp; 3 &amp; 2 \\\\ 3 &amp; 4 &amp; 5 \\\\ 2 &amp; 4 &amp; 3 \\end{pmatrix} \\text{ and } A = \\begin{pmatrix} 0 &amp; -1 &amp; 1 \\\\ 1 &amp; 0 &amp; 2 \\\\ -1 &amp; -2 &amp; 0 \\end{pmatrix} \\\\ \\operatorname{tr}(S) &amp;= \\operatorname{tr}(M) = 9 \\\\ T &amp;= \\begin{pmatrix} -2 &amp; 3 &amp; 2 \\\\ 3 &amp; 2 &amp; 4 \\\\ 2 &amp; 4 &amp; 0 \\end{pmatrix} \\\\ M &amp;= T + A + 3I \\\\ \\text{Furthermore } A \\underline{x} &amp;= \\underline{x} \\wedge \\underline{a} \\text{ where } \\underline{a} = \\begin{pmatrix}2 &amp; -1 &amp; -1\\end{pmatrix}. \\end{align*}\\] 4.6 Orthogonal and Unitary Matrices Definition 4.7 (Orthogonal matrix) A real \\(n \\times n\\) matrix is orthogonal iff \\[\\begin{align*} u^T u &amp;= u u^T = I \\\\ \\text{i.e. } u^T &amp;= u^{-1}. \\end{align*}\\] These conditions can be written \\[\\begin{align*} (u^T u)_{ij} &amp;= (u u^T)_{ij} = I_{ij} \\\\ \\underbrace{u_{ki} u_{kj}}_\\text{cols of $u$ are othornormal} &amp;= \\underbrace{u_{ik} u_{jk}}_\\text{rows of $u$ are othornormal} = \\delta_{ij} \\end{align*}\\] [recall \\([\\underline{C}_i (u)]_k = u_{ki} = [R_k(u)]_i\\)] \\[\\begin{align*} \\underbrace{\\begin{pmatrix} &amp; \\vdots &amp; \\\\ \\longleftarrow &amp; \\underline{C}_i &amp; \\longrightarrow \\\\ &amp; \\vdots &amp; \\end{pmatrix}}_{u^T} \\underbrace{\\begin{pmatrix} &amp; \\big\\uparrow &amp; \\\\ \\dots &amp; \\underline{C}_j &amp; \\dots \\\\ &amp; \\big\\downarrow &amp; \\end{pmatrix}}_{u} &amp;= I \\\\ \\underline{C}_i \\cdot \\underline{C}_j = \\delta_{ij} \\end{align*}\\] Equivalent definition: u is orthogonal iff it preserves the inner product on \\(\\mathbb{R}^n\\). \\[\\begin{align*} (u \\underline{x}) \\cdot (u \\underline{y}) &amp;= \\underline{x} \\cdot \\underline{y} \\quad \\forall \\; x, y \\in \\mathbb{R}^n \\end{align*}\\] To check equivalence, write this as \\[\\begin{align*} (u \\underline{x}) \\cdot (u \\underline{y}) &amp;= x^T (u^T u) y \\\\ &amp;= x^T y \\iff u^T u = I \\\\ &amp;= \\underline{x} \\cdot \\underline{y} \\quad \\forall \\; x, y \\in \\mathbb{R}^n \\end{align*}\\] Note, since \\(\\underline{C}_i = u \\underline{e}_i\\), columns being orthonormal \\(\\iff (u \\underline{e}_i) \\cdot (u \\underline{e}_j) = \\underline{e}_i \\cdot \\underline{e}_j = \\delta_{ij}\\). Example 4.19 In \\(\\mathbb{R}^2\\) we found all orthogonal matrices Isometries, area and determinants in \\(\\mathbb{R}^2\\): rotations \\(R(\\theta) = \\begin{pmatrix} \\cos \\theta &amp; - \\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{pmatrix}\\) and reflections \\(H(\\theta) = \\begin{pmatrix} \\cos \\theta &amp; \\sin \\theta \\\\\\sin \\theta &amp; - \\cos \\theta \\end{pmatrix}\\). Clearly \\(R(\\theta)^T = R(-\\theta) = R(\\theta)^{-1}\\). \\[\\begin{align*} R(\\theta)^T &amp;= R(-\\theta) = R(\\theta)^{-1} \\\\ H(\\theta)^T &amp;= H(\\theta) = H(\\theta)^{-1}. \\end{align*}\\] In \\(\\mathbb{R}^3\\) we found the matrix \\(R(\\theta)\\) for rotation through \\(\\theta\\) about axis \\(\\underline{n}\\), 4.12. \\[\\begin{align*} R(\\theta)^T &amp;= R(-\\theta) \\\\ \\text{Since } R(\\theta)_{ij} &amp;= R(-\\theta)_{ji} \\end{align*}\\] and can check explicitly \\[\\begin{align*} R(\\theta)^T R(\\theta) &amp;= R(-\\theta)R(\\theta) = I \\\\ \\text{or } R(\\theta)_{ki} R(\\theta)_{kj} &amp;= \\delta_{ij} \\end{align*}\\] Definition 4.8 (Unitary matrix) A complex \\(n \\times n\\) matrix \\(u\\) is unitary iff \\[\\begin{align*} u^\\dagger u &amp;= u u^\\dagger = I \\\\ \\text{i.e. } u^\\dagger &amp;= u^{-1}. \\end{align*}\\] Equivalent definition: u is unitary iff it preserves the inner product on \\(\\mathbb{C}^n\\). \\[\\begin{align*} (u \\underline{z}, u \\underline{w}) &amp;= (\\underline{z}, \\underline{w}) \\quad \\forall \\; z, w \\in \\mathbb{C}^n \\end{align*}\\] To check equivalence, write this as \\[\\begin{align*} (u \\underline{z})^\\dagger (u \\underline{w}) &amp;= \\underline{z}^\\dagger (u^\\dagger u) \\underline{w} \\\\ &amp;= \\underline{z}^\\dagger \\underline{w} \\iff u^\\dagger u = I \\\\ &amp;= (\\underline{z}, \\underline{w}) \\quad \\forall \\; \\underline{z}, \\underline{w} \\end{align*}\\] Note, since \\(\\underline{C}_i = u \\underline{e}_i\\), columns being orthonormal \\(\\iff (u \\underline{e}_i) \\cdot (u \\underline{e}_j) = \\underline{e}_i \\cdot \\underline{e}_j = \\delta_{ij}\\). Mostly concerned with \\(V = \\mathbb{R}^n,\\ W = \\mathbb{R}^m\\) or \\(V = \\mathbb{C}^n,\\ W = \\mathbb{C}^m\\) \\(A\\) has zeros on its diagonal. "],["determinants-and-inverses.html", "5 Determinants and inverses 5.1 Introduction 5.2 \\(\\epsilon\\) and Alternating Forms 5.3 Determinants in \\(\\mathbb{R}^n\\) and \\(\\mathbb{C}^n\\) 5.4 Minors, Cofactors and Inverses 5.5 System of Linear Equations", " 5 Determinants and inverses 5.1 Introduction Consider a linear map \\[\\begin{align*} T : \\mathbb{R}^n \\to \\mathbb{R}^n. \\end{align*}\\] If \\(T\\) is invertible then \\[\\begin{align*} \\underbrace{\\ker T = \\{ \\underline{0} \\}}_\\text{because $T$ is one-to-one} \\text{ and } \\underbrace{\\operatorname{Im} T = \\mathbb{R}^n}_\\text{$T$ is onto}. \\end{align*}\\] These conditions are equivalent by rank-nullity theorem 4.1. Conversely, if these conditions hold, then \\[\\begin{align*} \\underline{e}_1&#39; &amp;= T(\\underline{e}_1), \\dots, \\underline{e}_n&#39; = T(\\underline{e}_n) \\\\ \\end{align*}\\] is a basis (where \\(\\{ \\underline{e}_i \\}\\) is the standard basis) and we can define a linear map \\(T^{-1}\\) by \\(T^{^-1}(\\underline{e}_1&#39;) = \\underline{e}_1, \\dots, T^{-1}(\\underline{e}_n&#39;) = \\underline{e}_n\\). How can we test whether the conditions holds from matrix \\(M\\) representing \\(T:\\) \\(T(\\underline{x}) = M \\underline{x}\\) and how can we find \\(M^{-1}\\) when they do hold? For any \\(M\\) (\\(n \\times n\\)) we will define a related matrix \\(\\widetilde{M}\\) (\\(n \\times n\\)) and a scalar, the determinant \\(\\det M\\) or \\(| M |\\) such that \\[\\begin{align} \\widetilde{M} M = (\\det M )I \\tag{5.1} \\end{align}\\] Then if \\(\\det M \\neq 0\\), \\(M\\) is invertible with \\[\\begin{align*} M^{-1} = \\frac{1}{\\det M} \\widetilde{M}. \\end{align*}\\] For \\(n = 2\\) we found in Matrix Inverses that (5.1) holds with \\[\\begin{align*} M &amp;= \\begin{pmatrix} M_{11} &amp; M_{12} \\\\ M_{21} &amp; M_{22} \\end{pmatrix} \\text{ and } \\widetilde{M} = \\begin{pmatrix} M_{22} &amp; -M_{12} \\\\ -M_{21} &amp; M_{11} \\end{pmatrix} \\\\ \\det M &amp;= \\begin{vmatrix} M_{11} &amp; M_{12} \\\\ M_{21} &amp; M_{22} \\end{vmatrix} = M_{11} M_{22} - M_{12} M_{21} \\\\ &amp;= [M \\underline{e}_1, M \\underline{e}_2] \\\\ &amp;= [\\underline{C}_1 (M), \\underline{C}_2 (M)] \\\\ &amp;= \\epsilon_{ijk} M_{i1} M_{j2} \\end{align*}\\] The factor by which areas are scaled under \\(M\\) \\[\\begin{align*} \\det M \\neq 0 &amp;\\iff \\{ M \\underline{e}_1, M \\underline{e}_2 \\} \\text{ are linearly independent} \\\\ &amp;\\iff \\operatorname{Im}(M) = \\mathbb{R}^2 \\end{align*}\\] For \\(n = 3\\) consider similarly \\[\\begin{align*} [M \\underline{e}_1, M \\underline{e}_2, M \\underline{e}_3]&amp; \\text{ (scalar triple product)} \\\\ &amp;= [\\underline{C}_1 (M), \\underline{C}_2 (M), \\underline{C}_3 (M)] \\\\ &amp;= \\epsilon_{ijk} M_{i1} M_{j2} M_{k3} \\\\ &amp;= \\det M, \\text{ defn for $n = 3$}. \\end{align*}\\] This is the factor by which volumes are scaled under \\(M\\) and \\[\\begin{align*} \\det M \\neq 0 &amp;\\iff [M \\underline{e}_1, M \\underline{e}_2, M \\underline{e}_3] \\text{ are linearly independent} \\\\ &amp;\\iff \\operatorname{Im}(M) = \\mathbb{R}^3 \\end{align*}\\] Now define \\(\\widetilde{M}\\) from \\(M\\) using row/ column notation \\[\\begin{align*} \\underline{R}_1 (\\widetilde{M}) &amp;= \\underline{C}_2 (M) \\wedge \\underline{C}_3 (M) \\\\ \\underline{R}_2 (\\widetilde{M}) &amp;= \\underline{C}_3 (M) \\wedge \\underline{C}_1 (M) \\\\ \\underline{R}_3 (\\widetilde{M}) &amp;= \\underline{C}_1 (M) \\wedge \\underline{C}_2 (M) \\\\ \\text{and note that} \\\\ (\\widetilde{M} M)_{ij} &amp;= \\underline{R}_i (\\widetilde{M}) \\cdot \\underline{C}_j (M) \\\\ &amp;= \\underbrace{(\\underline{C}_1 (M) \\cdot \\underline{C}_2 (M) \\wedge \\underline{C}_3 (M))}_{\\det M} \\delta_{ij} \\end{align*}\\] Example 5.1 \\[\\begin{align*} M &amp;= \\begin{pmatrix} 1 &amp; 3 &amp; 0 \\\\ 0 &amp; -1 &amp; 2 \\\\ 4 &amp; 1 &amp; -1 \\end{pmatrix} \\\\ \\underline{C}_2 \\wedge \\underline{C}_3 &amp;= \\begin{pmatrix}3 \\\\-1 \\\\1\\end{pmatrix} \\wedge \\begin{pmatrix} 0 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 3 \\\\ 6 \\end{pmatrix} \\\\ \\underline{C}_3 \\wedge \\underline{C}_1 &amp;= \\begin{pmatrix} 0 \\\\ 2 \\\\ -1 \\end{pmatrix} \\wedge \\begin{pmatrix}1 \\\\0 \\\\4\\end{pmatrix} = \\begin{pmatrix} 8 \\\\ -1 \\\\ -2 \\end{pmatrix} \\\\ \\underline{C}_1 \\wedge \\underline{C}_2 &amp;= \\begin{pmatrix} 1 \\\\ 0 \\\\ 4 \\end{pmatrix} \\wedge \\begin{pmatrix}3 \\\\-1 \\\\1\\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 11 \\\\ -1 \\end{pmatrix} \\\\ \\widetilde{M} &amp;= \\begin{pmatrix} -1 &amp; 3 &amp; 6 \\\\ 8 &amp; -1 &amp; -2 \\\\ 4 &amp; 11 &amp; -1 \\end{pmatrix} \\\\ \\widetilde{M} M &amp;= (\\det M) I \\text{ where} \\\\ \\det M &amp;= \\underline{C}_1 \\cdot \\underline{C}_2 \\wedge \\underline{C}_3 = 23. \\end{align*}\\] 5.2 \\(\\epsilon\\) and Alternating Forms 5.2.1 \\(\\epsilon\\) and Permutation Recall: a permutation \\(\\sigma\\) on the set \\(\\{1, 2, \\dots, n \\}\\) is a bijection from this set to itself, specified by list \\(\\sigma(1), \\sigma(2), \\sigma(n)\\). Permutation \\(\\sigma\\) form a group, the symmetric group \\(S_n\\) of order \\(n!\\). The sign or signature \\(\\epsilon(\\sigma) = (-1)^k\\) where \\(k\\) is the number of transpositions (two cycles, this is well defined). The alternating or \\(\\epsilon\\) symbol in \\(\\mathbb{R}^n\\) or \\(\\mathbb{C}^n\\) is defined by \\[\\begin{align*} \\epsilon_{ij \\dots l} &amp;= \\begin{cases} +1 &amp; \\text{if } i, j, \\dots, l \\text{ is an even permutation} \\\\ -1 &amp; \\text{if } i, j, \\dots, l \\text{ is an odd permutation} \\\\ 0 &amp; \\text{else} \\end{cases} \\\\ \\epsilon(\\sigma) &amp;= (-1)^k \\text{ with $\\sigma$ product of $k$ transpositions} \\\\ &amp;= \\pm 1. \\end{align*}\\] If \\(\\sigma\\) is any permutation of \\(1, 2, \\dots, n\\) then \\[\\begin{align*} \\epsilon_{\\sigma(1) \\sigma(2) \\dots \\sigma(n)} &amp;= \\epsilon(\\sigma). \\end{align*}\\] Lemma 5.1 \\[\\begin{align*} \\epsilon_{\\sigma(i) \\sigma(j) \\dots \\sigma(l)} = \\epsilon(\\sigma) \\epsilon_{ij \\dots l} \\end{align*}\\] \\(\\epsilon\\) is totally antisymmetric. Proof. If \\(i, j, \\dots l\\) is not a permutation of \\(1, 2, \\dots, n\\) then \\(RHS = LHS = 0\\). If \\(i = \\rho(1), j = \\rho(2), \\dots, l = \\rho(n)\\) for some permutation \\(\\rho\\) then \\[\\begin{align*} RHS &amp;= \\epsilon(\\sigma) \\epsilon (\\rho) = \\epsilon (\\sigma \\rho) = LHS. \\end{align*}\\] 5.2.2 Alternating Forms and Linear (In)dependence Given \\(\\underline{v}_1, \\dots, \\underline{v}_n \\in \\mathbb{R}^n\\) or \\(\\mathbb{C}^n\\) (these are \\(n\\) vectors) the alternating form combines them to produce a scalar, defined by \\[\\begin{align*} [\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_n] &amp;= \\epsilon_{ij \\dots l}(\\underline{v}_1)_i (\\underline{v}_2)_j \\dots (\\underline{v}_n)_l \\ \\left(\\sum \\text{ convention}\\right) \\\\ &amp;= \\sum_\\sigma \\epsilon(\\sigma) (\\underline{v}_1)_{\\sigma(1)} (\\underline{v}_2)_{\\sigma(2)} \\dots (\\underline{v}_n)_{\\sigma(n)} \\ \\left[\\sum_\\sigma \\text{ means sum over all } \\sigma \\in S_n \\right] \\end{align*}\\] 5.2.2.1 Properties Multilinear \\[\\begin{align*} [\\underline{v}_1, \\dots, \\underline{v}_{p-1}, \\alpha \\underline{u} + \\beta \\underline{w}, \\underline{v}_{p+1}, \\underline{v}_n] &amp;= \\alpha [\\underline{v}_1, \\dots, \\underline{v}_{p-1}, \\underline{u}, \\underline{v}_{p+1}, \\underline{v}_n] + \\beta [\\underline{v}_1, \\dots, \\underline{w}, \\underline{v}_{p+1}, \\underline{v}_n] \\end{align*}\\] Totally antisymmetric \\[\\begin{align*} [\\underline{v}_{\\sigma(1)}, \\underline{v}_{\\sigma(2)}, \\dots \\underline{v}_{\\sigma(n)}] &amp;= \\epsilon (\\sigma) [\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_n] \\end{align*}\\] \\[\\begin{align*} [\\underline{e}_1, \\underline{e}_2, \\dots, \\underline{e}_n] = 1 \\end{align*}\\] for \\(\\underline{e}_i\\) standard basis vectors. Properties i, ii, iii fix the alternating form, and they also imply iv If \\(\\underline{v}_p = \\underline{v}_q\\) for some \\(p \\neq q\\) then \\[\\begin{align*} [\\underline{v}_1, \\dots, \\underline{v}_p, \\dots, \\underline{v}_q, \\dots, \\underline{v}_n] &amp;= 0 \\end{align*}\\] (from ii, exchanging \\(\\underline{v}_p \\leftrightarrow \\underline{v}_q\\) changes sign of alternating from). If \\(\\underline{v}_p = \\sum_{i \\neq p} \\lambda_i \\underline{v}\\) then \\[\\begin{align*} [\\underline{v}_1, \\dots, \\underline{v}_p, \\dots, \\underline{v}_n] &amp;= 0 \\end{align*}\\] (sub in and use i and iv). Example 5.2 \\[\\begin{align*} \\require{cancel} \\text{In } \\mathbb{C}^4, \\underline{v}_1 &amp;= \\begin{pmatrix}i \\\\0 \\\\0 \\\\2\\end{pmatrix}, \\underline{v}_2 = \\begin{pmatrix}0 \\\\0 \\\\5i \\\\0\\end{pmatrix}, \\\\ \\underline{v}_3 &amp;= \\begin{pmatrix}3 \\\\2i \\\\0 \\\\0\\end{pmatrix}, \\begin{pmatrix}0 \\\\0 \\\\-i \\\\1\\end{pmatrix} \\\\ \\implies [\\underline{v}_1, \\underline{v}_2, \\underline{v}_3, \\underline{v}_4] &amp;= 5i [\\underline{v}_1, \\underline{e}_3, \\underline{v}_3, \\underline{v}_4] \\\\ &amp;= 5i [i \\underline{e}_1 + \\cancel{2 \\underline{e}_4}, \\underline{e}_3, \\cancel{3 \\underline{e}_1} + 2i \\underline{e}_2, \\cancel{-i \\underline{e}_3} + \\underline{e}_4] \\\\ &amp;= 5i \\cdot i \\cdot 2i) [\\underline{e}_1, \\underline{e}_3, \\underline{e}_2, \\underline{e}_4] \\\\ &amp;= (- 10 i) \\cdot (-1) \\\\ &amp;= 10i \\end{align*}\\] (in cancelling, we first cancel \\(i \\underline{e}_3\\) as there is another lone \\(\\underline{e}_3\\), then the \\(2\\underline{e}_4\\) and finally the \\(3\\underline{e}_1\\)) Note: properties i and iii follow immediately from definition Proof (Property ii). \\[\\begin{align*} [\\underline{v}_{\\sigma(1)}, \\underline{v}_{\\sigma(2)}, \\dots \\underline{v}_{\\sigma(n)}] &amp;= \\sum_\\rho \\epsilon (\\rho) \\underbrace{[\\underline{v}_{\\sigma(1)}]_{\\rho(1)} \\dots [\\underline{v}_{\\sigma(n)}]_{\\rho(n)}}_\\text{each term can be re-written as: } \\text{ ($\\sigma$ fixed)} \\\\ &amp; [\\underline{v}_1]_{\\rho \\sigma^{-1} (1)} \\dots [\\underline{v}_n]_{\\rho \\sigma^{-1} (n)} \\\\ &amp;= \\sum_\\rho \\epsilon (\\sigma) \\epsilon(\\rho&#39;) [\\underline{v}_1]_{\\rho&#39;(1)} \\dots [\\underline{v}_n]_{\\rho&#39;(n)} \\text{ where } \\rho&#39; = \\rho \\sigma^{-1} \\\\ \\text{ and } \\sum_\\rho &amp;\\text{ is equivalent to } \\sum_{\\rho&#39;} \\\\ &amp;= \\epsilon(\\sigma) [\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_n] \\end{align*}\\] Proposition 5.1 \\[\\begin{align*} [\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_n] \\neq 0 \\iff \\underline{v}_1, \\dots, \\underline{v}_n \\text{ are linearly independent}. \\end{align*}\\] Proof. \\(\\implies\\): Use property v. If \\(\\underline{v}_1, \\dots \\underline{v}_n\\) are linearly dependent then \\(\\sum \\alpha_i \\underline{v}_i = \\underline{0}\\) where not all coefficients are zero. Suppose wlog that \\(\\alpha_p \\neq 0\\), then express \\(\\underline{v}_p\\) as a linear combination of \\(\\underline{v}_i (i \\neq p)\\) and so \\[\\begin{align*} [\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_n] = 0. \\end{align*}\\] \\(\\Longleftarrow\\): Note that \\(\\underline{v}_1, \\dots, \\underline{v}_n\\) being linearly independent means that they span (in \\(\\mathbb{R}^n\\) or \\(\\mathbb{C}^n\\)) so we can write the standard basis vectors as \\[\\begin{align*} \\underline{e}_i &amp;= A_{ai} \\underline{v}_a \\text{ for some } A_{ai} \\in \\mathbb{R} \\text{ or } \\mathbb{C}. \\\\ \\text{But then} &amp; \\\\ [\\underline{e_1}, \\dots, \\underline{e}_n] &amp;= [A_{a1} \\underline{v}_a, A_{b2} \\underline{v}_b, \\dots, A_{cn} \\underline{v}_c] \\\\ &amp;= A_{a1} A_{b2} \\dots A_{cn} [\\underline{v}_a, \\underline{v}_b, \\dots, \\underline{v}_c] \\\\ &amp;= A_{a1} A_{b2} \\dots A_{cn} \\epsilon_{ab \\dots c} [\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_n] \\text{ from property ii} \\\\ LHS &amp;= 1 \\implies [\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_n] \\neq 0. \\end{align*}\\] Example 5.2 are linearly independent. 5.3 Determinants in \\(\\mathbb{R}^n\\) and \\(\\mathbb{C}^n\\) 5.3.1 Definition Definition 5.1 (Determinant) For an \\(n \\times n\\) matrix \\(M\\) with columns \\[\\begin{align*} \\underline{C}_a &amp;= M \\underline{e}_a \\end{align*}\\] the determinant \\(\\det M\\) or \\(|M| \\in \\mathbb{R}\\) or \\(\\mathbb{C}\\) is defined by \\[\\begin{align*} \\det M &amp;= [\\underline{C}_1, \\underline{C}_2, \\dots, \\underline{C}_n] \\\\ &amp;= [M \\underline{e}_1, M \\underline{e}_2, \\dots, M \\underline{e}_n] \\\\ &amp;= \\epsilon_{ij \\dots l} M_{i1} M_{j2} \\dots M_{ln} \\\\ &amp;= \\sum_\\sigma \\epsilon(\\sigma) M_{\\sigma(1) 1} M_{\\sigma(2) 2} \\dots M_{\\sigma(n) n} \\end{align*}\\] Proposition 5.2 (Transpose Property) \\[\\begin{align*} \\det M &amp;= \\det M^T \\\\ \\text{so } \\det M &amp;= [\\underline{R}_1, \\underline{R}_2, \\dots, \\underline{R}_n] \\\\ &amp;= \\epsilon_{ij \\dots l} M_{1i} M_{2j} \\dots M_{nl} \\\\ &amp;= \\sum_\\sigma \\epsilon(\\sigma) M_{1 \\sigma(1)} M_{2 \\sigma(2)} \\dots M_{n \\sigma(n)} \\end{align*}\\] Example 5.3 In \\(\\mathbb{R}^3\\) or \\(\\mathbb{C}^3\\) \\[\\begin{align*} \\det M &amp;= \\epsilon_{ijk} M_{i1} M_{j2} M_{k3} \\\\ &amp;= M_{11} \\begin{bmatrix} M_{22} &amp; M_{23} \\\\ M_{32} &amp; M_{33} \\end{bmatrix} - M_{21} \\begin{bmatrix} M_{12} &amp; M_{13} \\\\ M_{32} &amp; M_{33} \\end{bmatrix} + M_{31} \\begin{bmatrix} M_{12} &amp; M_{13} \\\\ M_{22} &amp; M_{23} \\end{bmatrix} \\end{align*}\\] \\(\\det M\\) is a function of rows or columns of \\(M\\) that is multilinear totally anti-symmetric (or alternating) \\(\\det I = 1\\) Theorem 5.1 \\[\\begin{align*} \\det M \\neq 0 &amp;\\iff \\text{ columns of $M$ are linearly independent} \\\\ &amp;\\iff \\text{ row of $M$ are linearly independent} \\\\ &amp;\\iff \\operatorname{rank} M = n \\quad (M \\text{ is } n \\times n) \\\\ &amp;\\iff \\ker M = \\{ \\underline{0} \\} \\\\ &amp;\\iff M^{-1} \\text{ exists} \\end{align*}\\] Proof. All equivalences follow immediately from earlier results including the discussion in Introduction. Proof (Transpose property). Suffices to show \\[\\begin{align*} \\sum_\\sigma \\epsilon(\\sigma) M_{\\sigma(1) 1} \\dots M_{\\sigma(n) n} &amp;= \\sum_\\sigma \\epsilon(\\sigma) M_{1 \\sigma(1)} \\dots M_{n \\sigma(n)} \\end{align*}\\] But in a given term on LHS \\[\\begin{align*} M_{\\sigma(1) 1} \\dots M_{\\sigma(n) n} &amp;= M_{1 \\rho(1)} \\dots M_{n \\rho(n)} \\end{align*}\\] by reordering factors, where \\(\\rho = \\sigma^{-1}\\). Then \\(\\epsilon(\\sigma) = \\epsilon(\\rho)\\) and \\(\\sum_\\sigma\\) is equivalent to \\(\\sum_\\rho\\), so result follows. 5.3.2 Evaluating determinants: expanding by rows or columns For \\(M\\), \\(n \\times n\\), for each entry \\(M_{ia}\\) define the minor \\(M^{ia}\\) to be the determinant of \\((n - 1) \\times (n - 1)\\) matrix obtained from deleting row \\(i\\) and column \\(a\\) from \\(M\\). Proposition 5.3 \\[\\begin{align*} \\det M &amp;= \\sum_i (-1)^{i + a} M_{ia} M^{ia} \\hspace{.5cm} a \\text{ fixed} \\\\ &amp;= \\sum_a (-1)^{i + a} M_{ia} M^{ia} \\hspace{.5cm} i \\text{ fixed} \\end{align*}\\] called expanding by (or about) column \\(a\\) or row \\(i\\) respectively. Proof. See Cofactors and Determinants. Example 5.4 \\[\\begin{align*} M &amp;= \\begin{pmatrix} i &amp; 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 2i &amp; 0 \\\\ 0 &amp; 5i &amp; 0 &amp; -i \\\\ 2 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix}. \\end{align*}\\] Expand by row \\(3\\) to find \\[\\begin{align*} \\det M &amp;= \\sum_a (-1)^{3 + a} M_{3a} M^{3a} \\\\ M_{31} &amp;= M_{33} = 0 \\\\ M_{32} &amp;= 5i,\\ M^{32} = \\begin{vmatrix} i &amp; 3 &amp; 0 \\\\ 0 &amp; 2i &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\end{vmatrix} \\\\ M_{34} &amp;= -i, M^{34} = \\begin{vmatrix} i &amp; 0 &amp; 3 \\\\ 0 &amp; 0 &amp; 2i \\\\ 2 &amp; 0 &amp; 0 \\end{vmatrix} \\\\ M^{32} &amp;= i \\begin{vmatrix}2i &amp; 0 \\\\0 &amp; 1\\end{vmatrix} - 3 \\begin{vmatrix} 0 &amp; 0 \\\\ 2 &amp; 1 \\end{vmatrix} \\text{ (row 1)} \\\\ &amp;= i (2i) \\\\ &amp;= -2 \\\\ M_{34} &amp;= i \\begin{vmatrix} 0 &amp; 2i \\\\ 0 &amp; 0 \\end{vmatrix} + 3 \\begin{vmatrix} 0 &amp; 0 \\\\ 2 &amp; 0 \\end{vmatrix} \\text{ (row 1)}\\\\ &amp;= 0 \\\\ \\det M &amp;= (-1)^{3 + 2} 5i (-2) \\\\ &amp;= 10i \\\\ \\emph{or } \\text{expand by column 2}: \\\\ \\det M &amp;= \\sum_i (-1)^{2 + i} M_{i2} M^{i2} \\\\ &amp;= (-1)^{2 + 3} M_{32} M^{32} \\\\ &amp;= 10i. \\end{align*}\\] (Calculated this previously as example of alternating form in \\(\\mathbb{C}^4\\)) Lemma 5.2 If \\(M = \\left( \\begin{array}{c|c} A &amp; O \\\\ \\hline O &amp; I \\end{array} \\right)\\) block form with \\(A\\) a \\(r \\times r\\) matrix; \\(I \\ (n - r) \\times (n - r)\\) identity, then \\(\\det M = \\det A\\). Proof. For \\(r = n - 1\\), result follows by expanding about column \\(n\\) or row \\(n\\), and for \\(r &lt; n - 1\\), continue process. 5.3.3 Simplifying determinants: Row and Column Operations: From the definitions of \\(\\det M\\) in terms of columns (a) or rows (i) and the properties above (including Alternating Forms and Linear (In)dependence) we note the following 5.3.3.1 Row or column Scalings If \\(\\underline{R}_i \\mapsto \\lambda \\underline{R}_i\\) for some (fixed) \\(i\\) or \\(\\underline{C}_i \\mapsto \\lambda \\underline{C}_i\\) for some (fixed) \\(a\\) then \\(\\det M \\mapsto \\lambda \\det M\\). If all rows or columns are scaled \\[\\begin{align*} M &amp;\\mapsto \\lambda M, \\\\ \\text{then } \\det M &amp;\\mapsto \\lambda^n \\det M \\end{align*}\\] 5.3.3.2 Row or column Operations If \\(\\underline{R}_i \\mapsto \\underline{R}_i + \\lambda \\underline{R}_j\\) for \\(i \\neq j\\) or \\(\\underline{C}_a \\mapsto \\underline{C}_a + \\lambda \\underline{C}_b\\) for \\(a \\neq b\\) then \\(\\det M \\mapsto \\det M\\) as we can use multilinearity and then we have two rows being the same in one term so its 0. 5.3.3.3 Row or Column Exchanges \\[\\begin{align*} \\text{If } \\underline{R}_i &amp;\\leftrightarrow \\underline{R}_j \\hspace{0.5cm} \\text{for } i \\neq j \\\\ \\text{If } \\underline{C}_a &amp;\\leftrightarrow \\underline{C}_b \\hspace{0.5cm} \\text{for } a \\neq b \\\\ \\text{then } \\det M &amp;\\mapsto -\\det M. \\end{align*}\\] Example 5.5 \\[\\begin{align*} A &amp;= \\begin{pmatrix} 1 &amp; 1 &amp; a \\\\ a &amp; 1 &amp; 1 \\\\ 1 &amp; a &amp; 1 \\end{pmatrix},\\ a \\in \\mathbb{C} \\\\ \\underline{C}_1 &amp;\\mapsto \\underline{C}_1 - \\underline{C}_3 \\\\ \\det A &amp;= \\det \\begin{pmatrix} 1 - a &amp; 1 &amp; a \\\\ a - 1 &amp; 1 &amp; 1 \\\\ 0 &amp; a &amp; 1 \\end{pmatrix} \\\\ &amp;= (1 - a) \\det \\begin{pmatrix} 1 &amp; 1 &amp; a \\\\ -1 &amp; 1 &amp; 1 \\\\ 0 &amp; a &amp; 1 \\end{pmatrix} \\\\ \\underline{C}_2 &amp;\\mapsto \\underline{C}_2 - \\underline{C}_3 \\\\ \\det A &amp;= (1 - a) \\det \\begin{pmatrix} 1 &amp; 1-a &amp; a \\\\ -1 &amp; 0 &amp; 1 \\\\ 0 &amp; a-1 &amp; 1 \\end{pmatrix} \\\\ &amp;= (1 - a)^2 \\det \\begin{pmatrix} 1 &amp; 1 &amp; a \\\\ -1 &amp; 0 &amp; 1 \\\\ 0 &amp; -1 &amp; 1 \\end{pmatrix} \\\\ \\underline{R}_1 &amp;\\mapsto \\underline{R}_1 + \\underline{R}_2 + \\underline{R}_3 \\\\ \\det A &amp;= (1 - a)^2 \\det \\begin{pmatrix} 0 &amp; 0 &amp; a + 2 \\\\ -1 &amp; 0 &amp; 1 \\\\ 0 &amp; -1 &amp; 1 \\end{pmatrix} \\\\ &amp;= (1 - a)^2 (a + 2) \\begin{vmatrix} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{vmatrix} \\\\ &amp;= (1 - a)^2 (a + 2) \\end{align*}\\] 5.3.4 Multiplicative Property Theorem 5.2 For \\(n \\times n\\) matrices \\(M\\) and \\(N\\) \\[\\begin{align*} \\det (MN) = \\det M \\det N \\end{align*}\\] This is based on the following lemma Lemma 5.3 \\[\\begin{align*} \\epsilon_{i_1 \\dots i_n} M_{i_1 a_1} \\dots M_{i_n a_n} &amp;= (\\det M) \\epsilon_{a_1 \\dots a_n} \\end{align*}\\] Proof (Proof of Theorem). \\[\\begin{align*} \\det (MN) &amp;= \\epsilon_{i_1 \\dots i_n} (MN)_{i_1 1} \\dots (MN)_{i_n n} \\\\ &amp;= \\epsilon_{i_1 \\dots i_n} M_{i_1 k_1} N_{k_1 1} \\dots M_{i_n k_n} N_{k_n n} \\\\ &amp;= \\epsilon_{i_1 \\dots i_n} M_{i_1 k_1} \\dots M_{i_n k_n} N_{k_1 1} \\dots N_{k_n n} \\\\ &amp;= (\\det M) \\epsilon_{k_1 \\dots k_n} N_{k_1 1} \\dots N_{k_n n} \\text{by lemma } \\\\ &amp;= (\\det M) (\\det N) \\end{align*}\\] Proof (Proof of Lemma). Use total antisymmetry of LHS and RHS and then check taking \\(a_1 = 1, \\dots, a_n = n\\). Example 5.6 If \\(M = \\left( \\begin{array}{c|c} A &amp; O \\\\ \\hline O &amp; B \\end{array} \\right)\\) block form, with \\(A\\) \\(r \\times r\\) and \\(B\\) \\((n - r) \\times (n - r)\\) then \\(\\det M = \\det A \\det B\\). Since \\(\\left( \\begin{array}{c|c} A &amp; O \\\\ \\hline O &amp; B \\end{array} \\right) = \\left( \\begin{array}{c|c} A &amp; O \\\\ \\hline O &amp; I \\end{array} \\right) \\left( \\begin{array}{c|c} I &amp; O \\\\ \\hline O &amp; B \\end{array} \\right)\\) and we can use Lemma 5.2 above. \\[\\begin{align*} M^{-1}M &amp;= I \\implies \\det(M^{-1}) \\det M = \\det I = 1 \\\\ \\text{so } \\det(M^{-1}) &amp;= (\\det M)^{-1} \\end{align*}\\] For \\(R\\) real and orthogonal, \\[\\begin{align*} R^T R &amp;= I \\implies \\det(R^T) \\det R = (\\det R)^2 = 1 \\\\ \\implies \\det R &amp;= \\pm 1 \\end{align*}\\] For \\(U\\) complex and unitary, \\[\\begin{align*} U^\\dagger U = I &amp;\\implies \\det(U^\\dagger) \\det U \\\\ &amp;= \\overline{\\det(U)} \\det(U) \\\\ &amp;= |\\det U|^2 = 1 \\\\ \\implies |\\det U| &amp;= 1. \\end{align*}\\] 5.4 Minors, Cofactors and Inverses 5.4.1 Cofactors and Determinants Consider column \\(\\underline{C}_a\\) of matrix \\(M\\) (\\(a\\) fixed) and write \\(\\underline{C}_a = \\sum_i M_{ia} \\underline{e}_i\\) in definition of \\(\\det\\): \\[\\begin{align*} \\det M &amp;= [\\underline{C}_1, \\dots, \\underline{C}_{a - 1}, \\underline{C}_a, \\underline{C}_{a + 1}, \\dots, \\underline{C}_n] \\\\ &amp;= [\\underline{C}_1, \\dots, \\underline{C}_{a - 1}, \\sum_i M_{ia} \\underline{e}_i, \\underline{C}_{a + 1}, \\dots, \\underline{C}_n] \\\\ &amp;= \\sum_i M_{ia} \\Delta_{ia} \\hspace{0.5cm} \\text{(no sum over $a$)} \\\\ \\text{where the } &amp;\\emph{cofactor } \\Delta_{ia} \\text{ is defined by} \\\\ \\Delta_{ia} &amp;= [\\underline{C}_1, \\underline{C}_2, \\dots, \\underline{C}_{a- 1}, \\underline{e}_i, \\underline{C}_{a + 1}, \\dots, \\underline{C}_n] \\\\ &amp;= \\det \\left( \\begin{array}{c|c|c} A &amp; \\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array} &amp; B \\\\ \\hline \\begin{array}{ccc} 0 &amp; \\dots &amp; 0 \\end{array} &amp; 1 &amp; \\begin{array}{ccc} 0 &amp; \\dots &amp; 0 \\end{array} \\\\ \\hline C &amp; \\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array} &amp; D \\end{array} \\right) \\\\ &amp;= (-1)^{n - a} (-1)^{n - i} \\det \\left( \\begin{array}{c|c} \\begin{array}{c|c} A &amp; B \\\\ \\hline C &amp; D \\end{array} &amp; \\begin{array}{c} 0 \\\\ \\vdots \\\\ 0 \\end{array} \\\\ \\hline \\begin{array}{ccc} 0 &amp; \\dots &amp; 0 \\end{array} &amp; 1 \\end{array} \\right) \\\\ &amp;= (-1)^{a + i} M^{i a} \\\\ \\text{where } M^{ia} &amp;= \\det \\left( \\begin{array}{c|c} A &amp; B \\\\ \\hline C &amp; D \\end{array} \\right) \\text{by lemma } \\\\ \\therefore \\det M &amp;= \\sum_i M_{ia} \\Delta_{ia} \\\\ &amp;= \\sum_i M_{ia} (-1)^{i + a} M^{ia} \\end{align*}\\][Similarly, considering row \\(i\\), find other expression]. 5.2. To get \\(\\Delta_{ia}\\) we subtract column \\(a\\) with all other columns to get the row \\(i\\) as shown. 5.4.2 Adjugates and Inverses Reasoning as in Cofactors and Determinants with \\(\\underline{C}_b = \\sum_i M_{ib} \\underline{e}_i\\) \\[\\begin{align*} [\\underline{C}_1, \\dots, \\underline{C}_{a - 1}, \\underline{C}_b, C_\\underline{a + 1}, \\dots, \\underline{C_n}] &amp;= \\sum_i M_{ib} \\delta_{ia} \\\\ &amp;= \\begin{cases} \\det M &amp; \\text{if } a = b \\\\ 0 &amp; \\text{if } a \\neq b \\end{cases} \\\\ \\text{Hence } \\sum_i M_{ib} \\delta_{ia} &amp;= \\delta_{ab} \\det M \\\\ \\text{And similarly} \\sum_a M_{ja} \\delta_{ia} &amp;= \\delta_{ij} \\det M. \\end{align*}\\] Let \\(\\Delta\\) be the matrix of cofactors with entries \\(\\Delta_{ia}\\), and define the adjugate \\(\\widetilde{M} = \\operatorname{adj}(M) = \\Delta^T\\). Then the relations above become \\[\\begin{align*} \\Delta_{ia} M_{ib} &amp;= (\\Delta^T)_{ai} M_{ib} \\\\ &amp;= (\\Delta^T M)_{ab} \\\\ &amp;= (\\widetilde{M} M)_{ab} \\\\ &amp;= \\delta_{ab} \\det M \\\\ M_{ja} \\Delta_{ia} &amp;= (\\widetilde{M} M)_{ji} \\\\ &amp;= \\delta_{ij} \\det M. \\end{align*}\\] This justifies (5.1) with \\[\\begin{align*} \\widetilde{M} &amp;= \\Delta^T \\\\ \\Delta_{ia} &amp;= (-1)^{i + a} M^{ia} \\\\ \\text{we have } \\widetilde{M}M &amp;= M \\widetilde{M} = (\\det M) I. \\end{align*}\\]8 Hence if \\(\\det M \\neq 0\\) then \\(M\\) is invertible and \\[\\begin{align*} M^{-1} = \\frac{1}{\\det M} \\widetilde{M}. \\end{align*}\\] Example 5.7 Consider \\[\\begin{align*} A &amp;= \\begin{pmatrix} 1 &amp; 1 &amp; a \\\\ a &amp; 1 &amp; 1 \\\\ 1 &amp; a &amp; 1 \\end{pmatrix},\\ a \\in \\mathbb{C}, \\\\ \\end{align*}\\] previously found in Example 5.5 \\(\\det A = (1 - a)^2 (a + 2)\\). Hence \\(A^{-1}\\) exists if \\(a \\neq 1\\), \\(a \\neq - 2\\). Matrix of cofactors is \\[\\begin{align*} \\Delta &amp;= \\begin{pmatrix} 1 - a &amp; 1 - a &amp; a^2 - 1 \\\\ a^2 -1 &amp; 1 - a &amp; 1 - a \\\\ 1 - a &amp; a^2 - 1 &amp; 1 - a \\end{pmatrix} \\\\ \\text{e.g. } A^{12} &amp;= \\begin{vmatrix} a &amp; 1 \\\\ 1 &amp; 1 \\end{vmatrix} \\\\ &amp;= a - 1 \\\\ \\Delta_{12} &amp;= (-1)^{1 + 2} A^{12} \\\\ &amp;= 1 - a \\\\ \\widetilde{A} &amp;= \\Delta^T \\\\ A^{-1} &amp;= \\frac{1}{\\det A} \\widetilde{A} \\\\ &amp;= \\frac{1}{(1 -a) ( a + 2)} \\begin{pmatrix} 1 &amp; -(1 + a) &amp; 1 \\\\ 1 &amp; 1 &amp; - (1 + a) \\\\ - (1 + a) &amp; 1 &amp; 1 \\end{pmatrix}. \\end{align*}\\] 5.5 System of Linear Equations 5.5.1 Introduction and Nature of Solutions Consider a system of \\(n\\) linear equations in \\(n\\) unknowns \\(x_i\\) written in vector/ matrix form \\[\\begin{align*} A \\underline{x} &amp;= \\underline{b}, \\hspace{0.5cm} \\underline{x}, \\underline{b} \\in \\mathbb{R}^n \\text{ and } A \\ n \\times n \\text{ matrix} \\\\ \\text{i.e. } A_{11} x_1 + \\dots + A_{1n} x_n &amp;= b_1 \\\\ &amp;\\;\\;\\vdots \\\\ A_{n1} x_1 + \\dots + A_{nn} x_n &amp;= b_n \\end{align*}\\] There are three possibilities: \\(\\det A \\neq 0 \\implies A^{-1}\\) exists \\(\\implies\\) unique solution, \\(\\underline{x} = A^{-1} \\underline{b}\\) \\(\\det A = 0\\) and \\(\\underline{b} \\notin \\operatorname{Im} A \\implies\\) no solution. \\(\\det A = 0\\) and \\(\\underline{b} \\in \\operatorname{Im} A \\implies \\infty\\) many solutions. Elaboration: a solution exists iff \\(A \\underline{x}_0 = \\underline{b}\\) for some \\(\\underline{x}_0 \\iff \\underline{b} \\in \\operatorname{Im} A\\). Then \\(\\underline{x}\\) is also a solution iff \\(\\underline{u} = \\underline{x} - \\underline{x}_0\\) satisfies \\[\\begin{align*} A \\underline{u} &amp;= \\underline{0} \\emph{ homogenous problem}. \\text{Now } \\det A \\neq 0 &amp;\\iff \\operatorname{Im} A = \\mathbb{R}^n \\\\ &amp;\\iff \\ker A = \\{ \\underline{0} \\}. \\end{align*}\\] So in (i) there is a unique solutions and it can be found using \\(A^{-1}\\). \\[\\begin{align*} \\text{But } \\det A = 0 &amp;\\iff \\operatorname{rank} A &lt; n \\\\ &amp;\\iff \\operatorname{null} A &gt; 0 \\end{align*}\\] and then either \\(\\underline{b} \\notin \\operatorname{Im} A\\) as in case (ii) or \\(\\underline{b} \\in \\operatorname{Im} A\\) as in case (iii). If \\(\\underline{u}_1, \\dots, \\underline{u}_k\\) is a basis for \\(\\ker A\\) then general solution of homogenous problem is \\(\\underline{u} = \\sum_{i=1}^{k} \\lambda_i \\underline{u}_i\\). Example 5.8 \\(A \\underline{x} = \\underline{b}\\) with \\(A\\) as in Example 5.7 and \\(\\underline{b} = \\begin{pmatrix}1 \\\\c \\\\1\\end{pmatrix}\\), \\(a, c \\in \\mathbb{R}\\). \\(a \\neq 1, -2\\) Then \\(A^{-1}\\) exists and we have a solutions for any \\(c\\): \\[\\begin{align*} \\underline{x} &amp;= A^{-1} \\underline{b} \\\\ &amp;= \\frac{1}{(1- a)(a + 2)} \\begin{pmatrix}2 - c - ca \\\\c - a \\\\c - a \\end{pmatrix} \\end{align*}\\] \\(a = 1\\), \\[\\begin{align*} A &amp;= \\begin{pmatrix}1 &amp; 1 &amp; 1 \\\\1 &amp; 1 &amp; 1 \\\\1 &amp; 1 &amp; 1\\end{pmatrix} \\\\ \\operatorname{Im} A &amp;= \\left\\{ \\lambda \\begin{pmatrix}1 \\\\1 \\\\1\\end{pmatrix} \\right\\} \\\\ \\ker A &amp;= \\operatorname{span} \\left\\{ \\begin{pmatrix}-1 \\\\1 \\\\0\\end{pmatrix}, \\begin{pmatrix}-1 \\\\0 \\\\1\\end{pmatrix} \\right\\} \\\\ b &amp;\\in \\operatorname{Im} A \\iff c = 1; \\\\ \\text{particular solution } \\underline{x}_0 &amp;= \\begin{pmatrix}1 \\\\0 \\\\0\\end{pmatrix} \\\\ \\text{general solution } \\underline{x} &amp;= \\underline{x}_0 + \\underline{u} \\\\ &amp;= \\begin{pmatrix}1 - \\lambda - \\mu \\\\ \\lambda \\\\ \\mu \\end{pmatrix} \\\\ \\end{align*}\\] This is case (iii), and forms a plane of solutions. For \\(a = 1, c \\neq 1\\) we have no solutions - case (ii). \\(a = -2\\) \\[\\begin{align*} A &amp;= \\begin{pmatrix} 1 &amp; 1 &amp; -2 \\\\ -2 &amp; 1 &amp; 1 \\\\ 1 &amp; -2 &amp; 1 \\end{pmatrix} \\\\ \\operatorname{Im} A &amp;= \\operatorname{span} \\left\\{ \\begin{pmatrix}1 \\\\-2 \\\\1\\end{pmatrix}, \\begin{pmatrix}1 \\\\1 \\\\-2\\end{pmatrix} \\right\\} \\\\ \\ker A &amp;= \\left \\{ \\lambda \\begin{pmatrix}1 \\\\1 \\\\1\\end{pmatrix} \\right \\} \\\\ \\underline{b} &amp;\\in \\operatorname{Im} A \\iff c = 2, \\text{ (case (iii))} \\\\ \\text{particular solution } \\underline{x}_0 &amp;= \\begin{pmatrix}1 \\\\0 \\\\0\\end{pmatrix} \\\\ \\text{general solution } \\underline{x} &amp;= \\underline{x}_0 + \\underline{u} \\\\ &amp;= \\begin{pmatrix}1 + \\lambda \\\\ \\lambda \\\\ \\lambda \\end{pmatrix} \\\\ \\end{align*}\\] If \\(c \\neq 2\\), no solution, case (ii). 5.5.2 Geometric Interpretation in \\(\\mathbb{R}^3\\) Let \\(\\underline{R}_1, \\underline{R}_2, \\underline{R}_3\\) be rows of \\(A\\) (\\(3 \\times 3\\)). \\[\\begin{align*} A \\underline{u} = \\underline{0} \\iff \\begin{cases} \\underline{R}_1 \\cdot \\underline{u} = 0 \\\\ \\underline{R}_2 \\cdot \\underline{u} = 0 \\\\ \\underline{R}_3 \\cdot \\underline{u} = 0 \\end{cases} \\text{planes through $\\underline{0}$, normals $\\underline{R}_i (\\neq \\underline{0})$} \\end{align*}\\] So solutions of homogenous problem (finding \\(\\ker A\\)) is given by intersection of these planes. \\(\\operatorname{rank}(A) = 3 \\implies\\) normals are linearly independent and planes intersect in \\(\\underline{0}\\). \\(\\operatorname{rank}(A) = 3 \\implies 2\\) normals span a plane and planes intersect in a line and \\(\\dim \\ker A = 1\\). Figure 5.1: looking along line of intersection of planes \\(\\operatorname{rank}(A) = 1 \\implies\\) normals are parallel and planes coincide and \\(\\dim \\ker A = 2\\) Figure 5.2: Three identical planes Now consider instead \\[\\begin{align*} A \\underline{x} &amp;= \\underline{b} \\iff \\begin{cases} \\underline{R}_1 \\cdot \\underline{x} = b_1 \\\\ \\underline{R}_2 \\cdot \\underline{x} = b_2 \\\\ \\underline{R}_3 \\cdot \\underline{x} = b_3 \\end{cases} \\end{align*}\\] planes with normals \\(\\underline{R}_i\\) but not passing through \\(\\underline{0}\\) unless \\(b_i = 0\\). \\(\\operatorname{rank}(A) = 3 \\iff \\det A \\neq 0\\), normals are linearly independent; planes intersect in a point and get unique solution for any \\(b\\). \\(\\operatorname{rank}(A) = 2 \\implies\\) planes may intersect in a line (as in homogenous case) but they may not: Figure 5.3: No solutions \\(\\operatorname{rank}(A) = 1 \\implies\\) planes may coincide (as in homogenous case) but they may not: Figure 5.4: Three parallel or two identical and another parallel 5.5.3 Gaussian Elimination and Echelon Form Consider \\(A \\underline{x} = \\underline{b}\\) with \\(\\underline{x} \\in \\mathbb{R}^n\\), \\(\\underline{b} \\in R^m\\) and \\(A \\ m \\times n\\) matrix. Gaussian elimination is a direct approach to solving system of equations: \\[\\begin{align*} A_{11} x_1 + \\dots + A_{1n} x_n &amp;= b_1 \\\\ &amp;\\;\\;\\vdots \\\\ A_{m1} x_1 + \\dots + A_{mn} x_n &amp;= b_m \\end{align*}\\] Example \\[\\begin{align*} 3 x_1 + 2 x_2 + x_3 &amp;= b_1 \\tag{5.2} \\\\ 6 x_1 + 3 x_2 + 3 x_3 &amp;= b_2 \\tag{5.3} \\\\ 6 x_1 + 2 x_2 + 4 x_3 &amp;= b_3 \\tag{5.4} \\end{align*}\\] Steps subtract multiples of (5.2) from (5.3) and (5.4) to eliminate \\(x_1\\) \\[\\begin{align*} 0 - x_2 + x_3 &amp;= b_2 - 2 b_1 \\tag{5.5} \\\\ 0 - 2 x_2 + 2 x_3 &amp;= b_3 - 2 b_1 \\tag{5.6} \\end{align*}\\] repeat this using (5.5) to eliminate \\(x_2\\) from (5.6): \\[\\begin{align*} 0 + 0 + 0 &amp;= b_3 - 2 b_2 + 2 b_1 \\tag{5.7} \\end{align*}\\] Now consider new system (5.2), (5.5), (5.7). \\(b_3 - 2 b_2 + 2 b_1 \\neq 0 \\implies\\) no solutions \\(b_3 - 2 b_2 + 2 b_1 = 0 \\implies \\infty\\) solutions, \\(x_3\\) is arbitrary and then \\(x_2\\) and \\(x_1\\) determined from (5.2), (5.5). In general case we aim to carry out steps as in example until we obtain an equivalent system \\(M\\) is obtained from \\(A\\) by row operations including row exchanges and column exchanges which relabel variables \\(x_i\\). Note \\(x_{r + 1}, \\dots, x_n\\) are undetermined \\(d_{r + 1}, \\dots, d_m = 0\\) else no solution and if this is satisfied then \\(x_1, \\dots, x_r\\) can be determined successively. \\(r = \\operatorname{rank} M = \\operatorname{rank} A\\). If \\(n = m\\) then \\(\\det A = \\pm det M\\) and if \\(r = n = m\\) then \\(\\det M = M_{11} \\dots M_{rr} \\neq 0 \\implies A\\) and \\(M\\) are invertible. \\(M\\) as above is an example of echelon form. Would probably be guided through this proof in an exam and even then probably wouldnt come up. "],["eigenvalues-and-eigenvectors.html", "6 Eigenvalues and Eigenvectors 6.1 Introduction 6.2 Eigenspaces and Multiplicities 6.3 Diagonalisability and Similarity 6.4 Hermitian and Symmetric Matrices 6.5 Quadratic Forms 6.6 Cayley-Hamilton Theorem", " 6 Eigenvalues and Eigenvectors 6.1 Introduction 6.1.1 Definitions Definition 6.1 (Eigenvector and eigenvalue) For a linear map \\(T : V \\to V\\) (\\(V\\) a real vector or complex vector space) a vector \\(\\underline{v} \\in V\\) with \\(\\underline{v} \\neq \\underline{0}\\) is an eigenvector of \\(T\\) with eigenvalue \\(\\lambda\\) if \\[\\begin{align*} T(\\underline{v}) = \\lambda \\underline{v}. \\end{align*}\\] If \\(V = \\mathbb{R}^n\\) or \\(\\mathbb{C}^n\\) and \\(T\\) is given by an \\(n \\times n\\) matrix \\(A\\) then \\[\\begin{align*} A \\underline{v} = \\lambda \\underline{v} \\iff (A - \\lambda I) \\underline{v} = \\underline{0} \\end{align*}\\] and for a given \\(\\lambda\\) this holds for some \\(\\underline{v} \\neq 0 \\iff \\det(A - \\lambda I) = 0\\), the characteristic equation, i.e. \\(\\lambda\\) is an eigenvalue iff it is a root of \\(\\chi_A (t) = \\det (A - tI)\\), the characteristic polynomial. \\(\\chi_A (t)\\) is a polynomial of degree \\(n\\) for \\(A\\) (\\(n \\times n\\)). We find eigenvalues as roots of the characteristic equation/ polynomial and then determine corresponding eigenvectors. 6.1.2 Examples Example 6.1 \\[\\begin{align*} V &amp;= \\mathbb{C}^2 \\text{ and } A = \\begin{pmatrix} 2 &amp; i \\\\ -i &amp; 2 \\end{pmatrix} \\\\ \\det (A - \\lambda I) &amp;= \\begin{vmatrix} 2 - \\lambda &amp; i \\\\ -i &amp; 2 - \\lambda \\end{vmatrix} \\\\ &amp;= (2 - \\lambda)^2 - 1 \\\\ &amp;= 0 \\iff \\lambda = 1 \\text{ or } 3. \\end{align*}\\] To find eigenvectors \\(\\underline{v} = \\begin{pmatrix}v_1 \\\\v_2\\end{pmatrix}\\): \\(\\lambda = 1\\): \\[\\begin{align*} (A - I) \\underline{v} &amp;= \\begin{pmatrix} 1 &amp; i \\\\ -i &amp; 1 \\end{pmatrix} \\begin{pmatrix}v_1 \\\\v_2\\end{pmatrix} = \\underline{0} \\\\ \\implies \\underline{v} &amp;= \\alpha \\begin{pmatrix}1 \\\\i\\end{pmatrix} \\text{ any } \\alpha \\neq 0 \\end{align*}\\] \\(\\lambda = 3\\): \\[\\begin{align*} (A - 3I) \\underline{v} &amp;= \\begin{pmatrix} -1 &amp; i \\\\ -i &amp; -1 \\end{pmatrix} \\begin{pmatrix}v_1 \\\\v_2\\end{pmatrix} = \\underline{0} \\\\ \\implies \\underline{v} &amp;= \\beta \\begin{pmatrix}1 \\\\ -i\\end{pmatrix} \\text{ any } \\beta \\neq 0 \\end{align*}\\] Example 6.2 \\[\\begin{align*} V &amp;= \\mathbb{R}^2 \\text{ and } A = \\begin{pmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{pmatrix} \\\\ \\det (A - \\lambda I) &amp;= \\begin{vmatrix} 1 - \\lambda &amp; 1 \\\\ 0 &amp; 1 - \\lambda \\end{vmatrix} \\\\ &amp;= (1 - \\lambda)^2\\\\ &amp;= 0 \\iff \\lambda = 1. \\end{align*}\\] To find eigenvectors \\(\\underline{v} = \\begin{pmatrix}v_1 \\\\v_2\\end{pmatrix}\\): \\[\\begin{align*} (A - I) \\underline{v} &amp;= \\begin{pmatrix} 0 &amp; 1 \\\\ 0 &amp; 0 \\end{pmatrix} \\begin{pmatrix}v_1 \\\\v_2\\end{pmatrix} = \\underline{0} \\\\ \\implies \\underline{v} &amp;= \\alpha \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\text{ any } \\alpha \\neq 0 \\end{align*}\\] Example 6.3 \\[\\begin{align*} V &amp;= \\mathbb{R}^2 \\text{ or } \\mathbb{C}^2\\\\ U &amp;= \\begin{pmatrix} \\cos \\theta &amp; - \\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{pmatrix} \\\\ \\chi_U (t) &amp;= \\det (U - t I) \\\\ &amp;= t^2 - 2t \\cos \\theta + 1 \\\\ \\implies \\lambda &amp;= e^{\\pm i \\theta} \\\\ \\implies \\underline{v} &amp;= \\theta \\begin{pmatrix}1 \\\\ \\mp i\\end{pmatrix} \\ (\\alpha \\neq 0 ) \\end{align*}\\] 6.1.3 Deductions involving \\(\\chi_A(t)\\) For \\(A\\) \\(n \\times n\\), characteristic polynomial has degree \\(n\\) \\[\\begin{align*} \\chi_A(t) &amp;= \\det \\begin{pmatrix} A_{11} - t &amp; A_{12} &amp; \\dots &amp; A_{1n} \\\\ A_{21} &amp; A_{22} - t &amp; \\dots &amp; A_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ A_{n1} &amp; A_{n2} &amp; \\dots &amp; A_{nn} - t \\end{pmatrix} \\\\ &amp;= \\sum_{j=0}^{n} c_j t^j, \\text{ for some } c_j \\in \\mathbb{C} \\\\ &amp;= (-1)^n (t - \\lambda_1) \\dots (t - \\lambda_n) \\end{align*}\\] \\(\\exists\\) at least one eigenvalue (one root of \\(\\chi_A\\)); in fact \\(\\exists\\) \\(n\\) roots counted with multiplicity (FTA). \\(\\operatorname{tr}(A) = A_{ii} = \\sum_i \\lambda_i\\), sum of eigenvalues, by comparing terms of order \\(n - 1\\) in \\(t\\). \\(\\det A = \\chi_A(0) = \\Pi_i \\lambda_i\\), product of eigenvalues. If \\(A\\) is diagonal: \\[\\begin{align*} A = \\begin{pmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n \\end{pmatrix} \\end{align*}\\] with the diagonal entries being the eigenvalues; (ii) and (iii) are then immediate. If \\(A\\) is real, then coefficients \\(c_j\\) are real and \\(\\chi_A(\\lambda) = 0 \\iff \\chi_A(\\overline{\\lambda}) = 0\\): non-real roots occur in conjugate pairs. 6.2 Eigenspaces and Multiplicities 6.2.1 Definitions Definition 6.2 (Eigenspace) For an eigenvalue \\(\\lambda\\) of matrix \\(A\\), define the eigenspace \\[\\begin{align*} E_\\lambda = \\{ \\underline{v} : A \\underline{v} = \\lambda \\underline{v}\\} = \\ker (A - \\lambda I), \\end{align*}\\] this is the subspace consisting of the eigenvectors and \\(\\underline{0}\\). Definition 6.3 (Geometric multiplicity) The geometric multiplicity \\[\\begin{align*} m_\\lambda = \\dim E_\\lambda = \\operatorname{null}(A - \\lambda I), \\end{align*}\\] the number of linearly independent eigenvectors with eigenvalue \\(\\lambda\\). Definition 6.4 (Algebraic multiplicity) The algebraic multiplicity \\(M_\\Lambda\\) is the multiplicity of \\(\\lambda\\) as a root of \\(\\chi_A\\), i.e. \\(\\chi_A(t) = (t-\\lambda)^{M_\\lambda} f(t)\\) (with \\(f(\\lambda) \\neq 0\\)). Proposition 6.1 \\[\\begin{align*} M_\\lambda \\geq m_\\lambda \\end{align*}\\] Further discussion in 6.3. 6.2.2 Examples Example 6.4 \\[\\begin{align*} A &amp;= \\begin{pmatrix} -2 &amp; 2 &amp; -3 \\\\ 2 &amp; 1 &amp; -6 \\\\ -1 &amp; -2 &amp; 0 \\end{pmatrix} \\\\ \\chi_A(t) &amp;= \\det (A - tI) \\\\ &amp;= (5 - t) (t + 3)^2 \\\\ \\text{roots } \\lambda &amp;= 5, - 3 \\\\ M_5 &amp;= 1, M_{-3} = 2. \\\\ \\\\ \\underline{\\lambda = 5:} \\\\ (A - 5I)\\underline{x} &amp;= \\begin{pmatrix} -7 &amp; 2 &amp; -3 \\\\ 2 &amp; -4 &amp; -6 \\\\ -1 &amp; -2 &amp; -5 \\end{pmatrix}\\begin{pmatrix}x_1 \\\\x_2 \\\\x_3\\end{pmatrix} = \\underline{0} \\\\ \\implies E_5 &amp;= \\left\\{ \\alpha \\begin{pmatrix}1 \\\\2 \\\\-1\\end{pmatrix} \\right\\} \\\\ \\\\ \\underline{\\lambda = -3:} \\\\ (A + 3I)\\underline{x} &amp;= \\begin{pmatrix} 1 &amp; 2 &amp; -3 \\\\ 2 &amp; 4 &amp; -6 \\\\ -1 &amp; -2 &amp; 3 \\end{pmatrix}\\begin{pmatrix}x_1 \\\\x_2 \\\\x_3\\end{pmatrix} = \\underline{0} \\\\ \\text{Solve to find} \\\\ \\underline{x} &amp;= \\begin{pmatrix}-2 x_1 + 3 x_3 \\\\x_2 \\\\x_3\\end{pmatrix} \\\\ \\implies E_{-3} &amp;= \\left\\{ \\alpha \\begin{pmatrix}-2 \\\\1 \\\\0\\end{pmatrix} + \\beta \\begin{pmatrix}3 \\\\0 \\\\1\\end{pmatrix} \\right\\} \\\\ \\\\ m_5 &amp;= \\dim E_5 = 1 = M_5 \\\\ m_{-3} &amp;= \\dim E_{-3} = 2 = M_{-3} \\end{align*}\\] \\[\\begin{align*} A &amp;= \\begin{pmatrix} -3 &amp; -1 &amp; 1 \\\\ -1 &amp; -3 &amp; 1 \\\\ -2 &amp; -2 &amp; 0 \\end{pmatrix} \\\\ \\chi_A(t) &amp;= \\det (A - tI) \\\\ &amp;= - (t + 2)^3 \\\\ \\text{root } \\lambda &amp;= -2 \\\\ M_{-2} &amp;= 3 \\\\ \\text{To find eigenvectors:} \\\\ (A + 2I)\\underline{x} &amp;= \\begin{pmatrix} -1 &amp; -1 &amp; 1 \\\\ -1 &amp; -1 &amp; 1 \\\\ -2 &amp; -2 &amp; 2 \\end{pmatrix} \\begin{pmatrix}x_1 \\\\x_2 \\\\x_3\\end{pmatrix} = \\underline{0} \\\\ \\implies \\underline{x} &amp;= \\begin{pmatrix}- x_2 + x_3 \\\\ x_2\\\\ x_3\\end{pmatrix} \\\\ \\implies E_{-2} &amp;= \\left\\{ \\alpha \\begin{pmatrix}-1 \\\\1 \\\\0\\end{pmatrix} + \\beta \\begin{pmatrix}1 \\\\0 \\\\1\\end{pmatrix} \\right\\} \\\\ m_{-2} &amp;= \\dim E_{-2} = 2 \\\\ \\text{but } M_{-2} &amp;= 3 \\end{align*}\\] 6.2.3 Linear Independence of Eigenvectors Proposition 6.2 Let \\(\\underline{v}_1, \\dots, \\underline{v}_r\\) be eigenvectors of matrix \\(A\\) (\\(n \\times n\\)) with eigenvalues \\(\\lambda_1, \\dots, \\lambda_r\\). If the eigenvalues are distinct, \\(\\lambda_i \\neq \\lambda_j\\) for \\(i \\neq j\\), then the eigenvectors are linearly independent. With conditions as in (i), let \\(\\mathcal{B}_{\\lambda_i}\\) be a basis for \\(E_{\\lambda_i}\\), then \\[\\begin{align*} \\mathcal{B}_{\\lambda_1} \\cup \\mathcal{B}_{\\lambda_2} \\cup \\dots \\cup \\mathcal{B}_{\\lambda_r} \\end{align*}\\] is linearly independent. Proof. \\[\\begin{align*} \\text{Note } \\underline{w} &amp;= \\sum_{j=1}^{r} \\alpha_j \\underline{v}_j \\\\ \\implies (A - \\lambda I) \\underline{w} &amp;= \\sum_{j=1}^{r} \\alpha_j (\\lambda_j - \\lambda) \\underline{v}_j, \\text{ as } A \\underline{v}_j = \\lambda_j \\underline{v}_j \\\\ \\end{align*}\\] First, suppose the eigenvectors are linear dependent, so \\(\\exists\\) linear relations \\(\\underline{w} = 0\\) with a number of non-zero coefficients \\(p \\geq 2\\). Pick a \\(\\underline{w}\\) for which \\(p\\) is least and assume (wlog) \\(\\alpha_1 \\neq 0\\). Then \\((A - \\lambda_1 I) \\underline{w} = \\sum_{j&gt;1} \\alpha_j (\\lambda_j - \\lambda_1) \\underline{v}_j = \\underline{0}\\) is a linear relation with \\(p - 1\\) non-zero coefficients . Or secondly, \\[\\begin{align*} \\underline{w} &amp;= \\underline{0} \\implies \\Pi_{j \\neq k} (A - \\lambda_j I) \\underline{w} \\text{ for some chosen } k \\\\ &amp;= \\alpha_k (\\Pi_{j \\neq k} (\\lambda_k - \\lambda_j)) \\underline{v}_k = \\underline{0}, \\implies \\alpha_k &amp;= 0 \\end{align*}\\] hence eigenvectors are linearly independent. It suffices to show that if \\(\\underline{w} = \\underline{w}_1 + \\underline{w}_2 + \\dots + \\underline{w}_r = \\underline{0}\\) with \\(\\underline{w}_i \\in E_{\\lambda_i}\\) \\[\\begin{align*} \\implies \\underline{w}_i = \\underline{0}. \\end{align*}\\] This follows by same arguments as in (i). 6.3 Diagonalisability and Similarity 6.3.1 Introduction Proposition 6.3 For a \\(n \\times n\\) matrix \\(A\\) acting on \\(V = \\mathbb{R}^n\\) or \\(\\mathbb{C}^n\\), the following conditions are equivalent: There exists a basis of eigenvectors for \\(V\\), \\(\\underline{v}_1, \\underline{v}_2, \\dots, \\underline{v}_n\\) with \\(A \\underline{v}_i = \\lambda_i \\underline{v}_i\\) (not summation notation and \\(\\lambda_i\\) need not be distinct). There exists an \\(n \\times n\\) invertible matrix \\(P\\) with \\[\\begin{align*} P^{-1} A P = D = \\begin{pmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n \\end{pmatrix} \\end{align*}\\] If either of these conditions holds, \\(A\\) is diagonalisable. Proof. Note that for any matrix \\(P\\), \\(AP\\) has columns \\(A \\underline{C}_i(P)\\) and \\(PD\\) has columns \\(\\lambda_i \\underline{C}_i(P)\\) for each \\(i\\). Then (i) and (ii) are related by \\[\\begin{align*} \\underline{v}_i &amp;= \\underline{C}_i(P): \\\\ P^{-1} A P &amp;= D \\iff AP = PD \\iff A \\underline{v}_i = \\lambda_i \\underline{v}_i. \\end{align*}\\] Example 6.5 Refer to 6.3: Eigenvalues \\(e^{\\pm i \\theta}\\) and eigenvectors \\(\\begin{pmatrix}1 \\\\ \\mp i\\end{pmatrix}\\), the eigenvectors are linearly independent over \\(\\mathbb{C}\\). \\[\\begin{align*} P &amp;= \\begin{pmatrix} 1 &amp; 1 \\\\ -i &amp; i \\end{pmatrix} \\implies P^{-1} = \\frac{1}{2} \\begin{pmatrix} 1 &amp; i \\\\ 1 &amp; -i \\end{pmatrix} \\\\ P^{-1} U P &amp;= \\begin{pmatrix} e^{i\\theta} &amp; 0 \\\\ 0 &amp; e^{-i \\theta} \\end{pmatrix} \\end{align*}\\] \\(U\\) is diagonalisable over \\(\\mathbb{C}\\) but not over \\(\\mathbb{R}\\). 6.3.2 Criteria for Diagonalisability Theorem 6.1 Let \\(A\\) be a \\(n \\times n\\) matrix and \\(\\lambda_1, \\dots, \\lambda_r\\) all its distinct eigenvalues. A necessary and sufficient condition: \\(A\\) is diagonalisable iff \\[\\begin{align*} M_{\\lambda_i} = m_{\\lambda_i} \\text{ for } i = 1, \\dots, r \\end{align*}\\] A sufficient condition: \\(A\\) is diagonalisable if there are \\(n\\) distinct eigenvalues, i.e. \\(r = n\\). Proof. Use Proposition 6.2 If \\(r = n\\) we have \\(n\\) distinct eigenvalues and hence \\(n\\) linearly independent eigenvectors, which form a basis (for \\(\\mathbb{R}^n\\) or \\(\\mathbb{C}^n\\)). Choosing bases \\(\\mathcal{B}_{\\lambda_i}\\) for each eigenspace, \\[\\begin{align*} \\mathcal{B}_{\\lambda_1} \\cup \\mathcal{B}_{\\lambda_2} \\cup \\dots \\cup \\mathcal{B}_{\\lambda_r} \\end{align*}\\] is a linearly independent set of \\(m_{\\lambda_1} + m_{\\lambda_2} + \\dots + m_{\\lambda_r}\\) vectors. It is a basis (for \\(\\mathbb{R}^n\\) or \\(\\mathbb{C}^n\\)) iff we have \\(n\\) vectors. But \\[\\begin{align*} m_{\\lambda_i} &amp;\\leq M_{\\lambda_i} \\\\ \\text{and } M_{\\lambda_1} + M_{\\lambda_2} + \\dots + M_{\\lambda_r} &amp;= n. \\text{ (as there are n roots)} \\end{align*}\\] Hence we have a basis iff \\[\\begin{align*} M_{\\lambda_i} = m_{\\lambda_i} \\text{ for each } i. \\end{align*}\\] Refer to Examples i. \\[\\begin{align*} A &amp;= \\begin{pmatrix} -2 &amp; 2 &amp; -3 \\\\ 2 &amp; 1 &amp; -6 \\\\ -1 &amp; -2 &amp; 0 \\end{pmatrix} \\\\ \\lambda &amp;= 5, - 3 \\\\ m_5 &amp;= 1 = M_5 \\\\ m_{-3} &amp;= 2 = M_{-3} \\\\ \\implies A &amp;\\text{ is diagonalisable} \\\\ P &amp;= \\begin{pmatrix} 1 &amp; -2 &amp; 3 \\\\ 2 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 \\end{pmatrix}, P^{-1} = \\frac{1}{8} \\begin{pmatrix} 1 &amp; 2 &amp; -3 \\\\ -2 &amp; 4 &amp; 6 \\\\ 1 &amp; 2 &amp; 5 \\end{pmatrix} \\\\ P^{-1} A P &amp;= \\begin{pmatrix} 5 &amp; 0 &amp; 0 \\\\ 0 &amp; -3 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} \\text{ as expected}. \\end{align*}\\] \\[\\begin{align*} A &amp;= \\begin{pmatrix} -3 &amp; -1 &amp; 1 \\\\ -1 &amp; -3 &amp; 1 \\\\ -2 &amp; -2 &amp; 0 \\end{pmatrix} \\\\ \\lambda &amp;= -2 \\\\ M_{-2} &amp;= 3 &gt; m_{-2} = 2 \\\\ \\implies A &amp;\\text{ is not diagonalisable} \\\\ \\text{Check: if it was } &amp;P^{-1} A P = - 2 I \\\\ \\implies A &amp;= P (-2 I) P^{-1} = - 2I . \\end{align*}\\] 6.3.3 Similarity Matrices \\(A\\) and \\(B\\) (\\(n \\times n\\)) are similar if \\[\\begin{align*} B = P^{-1} A P \\end{align*}\\] for some invertible \\(P\\) (\\(n \\times n\\)). This is an equivalence relation. Proposition 6.4 If \\(A\\) and \\(B\\) are similar, then \\(B^r = P^{-1} A^r P\\) for \\(r \\geq 0\\) \\(B^{-1} = P^{-1} A^{-1} P\\) (if either \\(A\\) or \\(B\\) is invertible, so is the other) \\(\\operatorname{tr}(B) = \\operatorname{tr}(A)\\) \\(\\det B = \\det A\\) \\(\\chi_B(t) = \\chi_A(t)\\) Proof. is immediate. is immediate. \\[\\begin{align*} \\operatorname{tr}(B) &amp;= \\operatorname{tr}(P^{-1} A P) \\\\ &amp;= \\operatorname{tr}(APP^{-1}) \\\\ &amp;= \\operatorname{tr}(A). \\end{align*}\\] \\[\\begin{align*} \\det(B) &amp;= \\det(P^{-1} A P) \\\\ &amp;= \\det(P^{-1}) \\det(A) \\det(P) \\\\ &amp;= \\det A. \\end{align*}\\] \\[\\begin{align*} \\det(B - tI) &amp;= \\det(P^{-1}AP - tI) \\\\ &amp;= \\det(P^{-1} (A - tI) P) \\\\ &amp;= \\det(A - tI) \\text{ as in (iv)}. \\end{align*}\\] 6.4 Hermitian and Symmetric Matrices 6.4.1 Real Eigenvalues and Orthogonal Eigenvectors Recall: matrix \\(A\\) (\\(n \\times n\\)) is hermitian if \\[\\begin{align*} A^\\dagger = \\overline{A}^T = A \\text{ or } A_{ij} + \\overline{A}_{ji} \\end{align*}\\] special case: \\(A\\) is real and symmetric \\[\\begin{align*} \\overline{A} = A \\text{ and } A^T = A \\text{ or } \\begin{cases} A_{ij} &amp;= \\overline{A}_{ij} \\\\ A_{ij} &amp;= A_{ji} \\end{cases}. \\end{align*}\\] Recall: complex inner-product for \\(\\underline{v}, \\underline{w} \\in \\mathbb{C}^n\\) is \\[\\begin{align*} \\underline{v}^\\dagger \\underline{w} &amp;= \\sum_i \\overline{v}_i w_i \\end{align*}\\] and for \\(\\underline{v}, \\underline{w} \\in \\mathbb{R}^n\\) this reduces to \\[\\begin{align*} \\underline{v}^T \\underline{w} = \\underline{v} \\cdot \\underline{w} = \\sum_i v_i w_i \\end{align*}\\] Observation: If \\(A\\) is hermitian then \\[\\begin{align*} (A \\underline{v})^\\dagger \\underline{w} &amp;= \\underline{v}^\\dagger (A \\underline{w}) \\ \\forall \\; \\underline{v}, \\underline{w} \\in \\mathbb{C}^n \\\\ \\end{align*}\\] Proof. \\[\\begin{align*} (A \\underline{v})^\\dagger \\underline{w} &amp;= (\\underline{v}^\\dagger A^\\dagger) \\underline{w} \\\\ &amp;= \\underline{v}^\\dagger A \\underline{w} \\\\ &amp;= \\underline{v}^\\dagger (A \\underline{w}) \\end{align*}\\] Theorem 6.2 For a matrix \\(A\\) (\\(n \\times n\\)) that is hermitian Every eigenvalue \\(\\lambda\\) is real Eigenvectors \\(\\underline{v}, \\underline{w}\\) with distinct eigenvalues \\(\\lambda, \\mu\\) respectively (\\(\\lambda \\neq \\mu\\)) are orthogonal \\[\\begin{align*} \\underline{v}^\\dagger \\underline{w} = 0 \\end{align*}\\] If \\(A\\) is real and symmetric then for each \\(\\lambda\\) in (i) we can choose a real eigenvector \\(\\underline{v}\\) and (ii) becomes \\[\\begin{align*} \\underline{v}^T \\underline{w} = \\underline{v} \\cdot \\underline{w} = 0 \\end{align*}\\] Proof. \\[\\begin{align*} \\underline{v}^\\dagger (A \\underline{v}) &amp;= (A \\underline{v}^\\dagger) \\underline{v} \\\\ \\implies \\underline{v}^\\dagger (\\lambda \\underline{v}) &amp;= (\\lambda \\underline{v})^\\dagger \\underline{v} \\\\ \\implies \\lambda \\underline{v}^\\dagger \\underline{v} &amp;= \\overline{\\lambda} \\underline{v}^\\dagger \\underline{v} \\\\ \\end{align*}\\] for \\(\\underline{v}\\) an eigenvector with eigenvalue \\(\\lambda\\). But \\(\\underline{v} \\neq 0\\) so \\(\\underline{v}^\\dagger \\underline{v} \\neq 0\\) and so \\(\\lambda = \\overline{\\lambda}\\). \\[\\begin{align*} \\underline{v}^\\dagger (A \\underline{w}) &amp;= (A \\underline{v})^\\dagger \\underline{w} \\\\ \\implies \\underline{v}^\\dagger (\\mu \\underline{w}) &amp;= (\\lambda \\underline{v})^\\dagger \\underline{w} \\\\ \\implies \\mu \\underline{v}^\\dagger \\underline{w} &amp;= \\overline{\\lambda} \\underline{v}^\\dagger \\underline{w} \\\\ &amp;= \\lambda \\underline{v}^\\dagger \\underline{w} \\text{ (from (i))} \\end{align*}\\] But \\(\\mu \\neq \\lambda\\) so \\(\\underline{v}^\\dagger \\underline{w} = 0\\). Given \\(A \\underline{v} = \\lambda \\underline{v}\\) with \\(\\underline{v} \\in \\mathbb{C}^n\\) and \\(A, \\lambda\\) real, let \\(\\underline{v} = \\underline{u} + i \\underline{u&#39;}\\) with \\(\\underline{u}, \\underline{u&#39;} \\in \\mathbb{R}^n\\). Then \\(A \\underline{u} = \\lambda \\underline{u}\\) and \\(A \\underline{u&#39;} = \\lambda \\underline{u&#39;}\\) but \\(\\underline{v} \\neq \\underline{0} \\implies\\) one of \\(\\underline{u}, \\underline{u&#39;} \\neq 0\\) so there is at least one real eigenvector. 6.4.2 Unitary and Orthogonal Diagonalisation Theorem 6.3 Any \\(n \\times n\\) hermitian matrix \\(A\\) is diagonalisable (as in Introduction) \\(\\exists\\) a basis of eigenvectors \\(\\underline{u}_1, \\dots, \\underline{u}_n \\in \\mathbb{C}^n\\) with \\(A \\underline{u}_i = \\lambda_i \\underline{u}_i\\); equivalently \\(\\exists \\; n \\times n\\) invertible matrix \\(P\\) with \\(P^{-1} A P = D = \\begin{pmatrix}\\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n\\end{pmatrix}\\); columns of \\(P\\) are eigenvectors \\(\\underline{u}_i\\). In addition: these eigenvectors \\(\\underline{u}_i\\) can be chosen to be orthonormal \\(\\underline{u}_i^\\dagger \\underline{u}_j = \\delta_{ij}\\); equivalently the matrix \\(P\\) can be chosen to be unitary \\(P^\\dagger = P^{-1}\\) so \\(P^\\dagger A P = D\\). Special case: for \\(n \\times n\\) real symmetric \\(A\\), can choose eigenvectors \\(\\underline{u}_1, \\dots, \\underline{u}_n \\in \\mathbb{R}^n\\) with \\(\\underline{u}_i^T \\underline{u}_j = \\underline{u}_i \\underline{u}_j = \\delta_{ij}\\); equivalently the matrix \\(P\\) can be chosen to be orthogonal \\(P^T = P^{-1}\\) so \\(P^T A P = D\\). Proof of diagonalisability is not examinable and remaining statements follow by combining results of Eigenspaces and Multiplicities and Diagonalisability and Similarity and choosing orthonormal basis for each eigenspace. Example 6.6 \\(A = \\begin{pmatrix}2 &amp; i \\\\-i &amp; 2\\end{pmatrix}, A^\\dagger = A\\) so hermitian (as in Examples). \\(\\lambda_1 = 1, \\lambda_2 = 3\\) and choose \\[\\begin{align*} \\underline{u}_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}1 \\\\i\\end{pmatrix}, \\underline{u}_2 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}1 \\\\-i\\end{pmatrix} \\end{align*}\\] to ensure \\(\\underline{u}_1^\\dagger \\underline{u}_1 = \\underline{u}_2^\\dagger \\underline{u}_1 = 1\\) and note \\(\\underline{u}_1^\\dagger \\underline{u}_2 = \\frac{1}{2} (1 - i) \\begin{pmatrix}1 \\\\-i\\end{pmatrix} = 0\\). Let \\(P = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}1 &amp; 1 \\\\i &amp; -i\\end{pmatrix}\\) then \\(P^\\dagger = P^{-1}\\) so unitary and \\(P^\\dagger A P = \\begin{pmatrix}1 &amp; 0 \\\\0 &amp; 3\\end{pmatrix}\\). \\(A = \\begin{pmatrix}0 &amp; 1 &amp; 1 \\\\1 &amp; 0 &amp; 1 \\\\1 &amp; 1 &amp; 0\\end{pmatrix}, A^T = A\\) so real and symmetric. \\(\\lambda_1 = \\lambda_2 = -1, \\lambda_3 = 2\\) and can choose \\[\\begin{align*} \\underline{u}_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix}1 \\\\-1 \\\\0\\end{pmatrix}, \\underline{u}_2 = \\frac{1}{\\sqrt{6}} \\begin{pmatrix}1 \\\\1 \\\\-2\\end{pmatrix}, \\underline{u}_3 = \\frac{1}{\\sqrt{3}} \\begin{pmatrix}1 \\\\1 \\\\1\\end{pmatrix}. \\end{align*}\\]9 Let \\(P\\) be the matrix with columns \\(\\underline{u}_1, \\underline{u}_2, \\underline{u}_3\\) then \\(P^T = P^{-1}\\) so \\(P\\) is orthogonal and \\(P^T A P = \\begin{pmatrix}-1 &amp; 0 &amp; 0 \\\\0 &amp; -1 &amp; 0 \\\\0 &amp; 0 &amp; 2\\end{pmatrix}\\). 6.5 Quadratic Forms Consider \\(\\mathcal{F} : \\mathbb{R}^2 \\to \\mathbb{R}\\) defined by \\(\\mathcal{F}(\\underline{x}) = 2 x_1^2 - 4 x_1 x_2 + 5 x_2^2\\). This can be expressed as \\(\\mathcal{F}(\\underline{x}) = x_1&#39;^2 + 6 x_2&#39;^2\\) where \\[\\begin{align*} x_1&#39; &amp;= \\frac{1}{\\sqrt{5}} (2 x_1 + x_2) \\\\ x_2&#39; &amp;= \\frac{1}{\\sqrt{5}} (- x_1 + 2 x_2) \\end{align*}\\] with \\(x_1&#39;^2 + x_2&#39;^2 = x_1^2 + x_2^2\\). To understand this better, note \\(\\mathcal{F}(\\underline{x}) = \\underline{x}^T A \\underline{x}\\) where \\(A = \\begin{pmatrix}2 &amp; -2 \\\\-2 &amp; 5\\end{pmatrix}\\) and we can diagonalise \\(A\\): \\(\\lambda_1 = 1, \\lambda_2 = 6\\) and orthonormal eigenvectors \\[\\begin{align*} \\underline{u}_1 &amp;= \\frac{1}{\\sqrt{5}} \\begin{pmatrix}2 \\\\1\\end{pmatrix}, \\underline{u}_2 = \\frac{1}{\\sqrt{5}} \\begin{pmatrix}-1 \\\\2\\end{pmatrix} \\\\ \\text{Then } x_1&#39; &amp;= \\underline{u}_1 \\cdot \\underline{x} \\\\ x_2&#39; &amp;= \\underline{u}_2 \\cdot \\underline{x} \\end{align*}\\] give the simplified form for \\(\\mathcal{F}\\). In general, a quadratic form is a function \\[\\begin{align*} \\mathcal{F} : \\mathbb{R}^n &amp;\\to \\mathbb{R} \\\\ \\underline{x} &amp;\\mapsto \\underline{x}^T A \\underline{x} = x_i A_{ij} x_j \\end{align*}\\] where \\(A\\) is a \\(n \\times n\\) real symmetric matrix. From Hermitian and Symmetric Matrices \\[\\begin{align*} P^T A P &amp;= D = \\begin{pmatrix}\\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n\\end{pmatrix} \\end{align*}\\] where \\(\\lambda_i\\) are the eigenvalues of \\(A\\) and \\(P\\) is orthogonal with columns \\(\\underline{u}_i\\), the orthonormal eigenvectors. Let \\(\\underline{x}&#39; = P^T \\underline{x}\\) or \\(\\underline{x} = P \\underline{x}&#39;\\), then \\[\\begin{align*} \\mathcal{F}(\\underline{x}) &amp;= \\underline{x}^T A \\underline{x} \\\\ &amp;= (P \\underline{x}&#39;)^T A (P \\underline{x}&#39;) \\\\ &amp;= (\\underline{x}&#39;)^T (P^T A P) \\underline{x}&#39; \\\\ &amp;= (\\underline{x}&#39;)^T D \\underline{x}&#39; \\\\ &amp;= \\sum_i \\lambda_i x_i&#39;^2 \\\\ &amp;= \\lambda_1 x_1&#39;^2 + \\dots + \\lambda_n x_n&#39;^2. \\end{align*}\\] \\(\\mathcal{F}\\) has been diagonalised. \\[\\begin{align*} \\text{Now } \\underline{x}&#39; &amp;= x_1&#39; \\underline{e}_1 + \\dots + x_n&#39; \\underline{e}_n \\\\ \\underline{x} &amp;= x_1 \\underline{e}_1 + \\dots + x_n \\underline{e}_n \\\\ &amp;= x_1&#39; \\underline{u}_1 + \\dots + x_n&#39; \\underline{u}_n \\\\ \\text{since } x_i&#39; &amp;= \\underline{u}_i \\cdot \\underline{x} \\iff \\underline{x}&#39; = P^T \\underline{x} \\end{align*}\\] Thus, \\(x_i&#39;\\) are coordinates w.r.t new axes given by orthonormal basis vectors \\(\\underline{u}_i\\), these are called the principal axes of \\(\\mathcal{F}\\). Relation to original axes along standard basis vectors \\(\\underline{e}_i\\) and coordinates \\(x_i\\) is given by an orthogonal transformations \\[\\begin{align*} |\\underline{x}|^2 &amp;= x_i x_i = x_i&#39; x_i&#39;. \\end{align*}\\] 6.5.1 Examples in \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) 6.5.1.1 In \\(\\mathbb{R}^2\\) \\[\\begin{align*} \\mathcal{F}(\\underline{x}) &amp;= \\underline{x}^T A \\underline{x} \\\\ \\text{with } A &amp;= \\begin{pmatrix}\\alpha &amp; \\beta \\\\\\beta &amp; \\alpha\\end{pmatrix} \\\\ \\text{Eigenvalues: } \\lambda_1 &amp;= \\alpha + \\beta, \\lambda_2 &amp;= \\alpha - \\beta \\\\ \\text{Eigenvectors: } \\underline{u}_1 &amp;= \\frac{1}{\\sqrt{2}} \\begin{pmatrix}1 \\\\1\\end{pmatrix}, \\underline{u}_2 = \\begin{pmatrix}-1 \\\\1\\end{pmatrix} \\\\ \\mathcal{F}(\\underline{x}) &amp;= \\alpha x_1^2 + 2 \\beta x_1 x_2 + \\alpha x_2^2 \\\\ &amp;= (\\alpha + \\beta) x_1&#39;^2 + (\\alpha - \\beta) x_2&#39;^2 \\\\ \\text{with } x_1&#39; &amp;= \\frac{1}{\\sqrt{2}} (x_1 + x_2) \\\\ x_2&#39; &amp;= \\frac{1}{\\sqrt{2}} (-x_1 + x_2) \\end{align*}\\] \\[\\begin{align*} \\alpha &amp;= \\frac{3}{2}, \\beta = - \\frac{1}{2} \\\\ \\implies \\lambda_1 &amp;= 1, \\lambda_2 = 2 \\\\ \\mathcal{F}(x) &amp;= x_1&#39;^2 + 2 x_2&#39;^2 = 1 \\end{align*}\\] defines an ellipse \\[\\begin{align*} \\alpha &amp;= -\\frac{1}{2}, \\beta = \\frac{3}{2} \\\\ \\implies \\lambda_1 &amp;= 1, \\lambda_2 = - 2 \\\\ \\mathcal{F}(x) &amp;= x_1&#39;^2 - 2 x_2&#39;^2 = 1 \\end{align*}\\] defines an hyperbola 6.5.1.2 In \\(\\mathbb{R}^3\\) \\(\\mathcal{F}(x) = \\underline{x}^T A \\underline{x} = \\lambda_1 x_1&#39;^2 + \\lambda_2 x_2&#39;^2 + \\lambda_3 x_3&#39;\\) after diagonalisation. If \\(A\\) has eigenvalues \\(\\lambda_1, \\lambda_2, \\lambda_3 &gt; 0\\) then \\(\\mathcal{F} = 1\\) defines an ellipsoid. \\(A = \\begin{pmatrix}0 &amp; 1 &amp; 1 \\\\1 &amp; 0 &amp; 1 \\\\1 &amp; 1 &amp; 0\\end{pmatrix}\\) (from example 6.6), \\(\\lambda_1 = \\lambda_2 = -1, \\lambda_3 = 2\\). Hence \\[\\begin{align*} \\mathcal{F} &amp;= 2 x_1 x_2 + 2 x_2 x_3 + 2 x_3 x_1 \\\\ &amp;= - x_1&#39;^2 - x_2&#39;^2 + 2 x_3&#39;^2 \\\\ \\mathcal{F} &amp;= 1 \\iff 2 x_3&#39;^2 = 1 + x_1&#39;^2 + x_2&#39;^2 \\end{align*}\\] defines a hyperboloid (2 sheeted) \\(\\mathcal{F} = -1 \\iff x_1&#39;^2 + x_2&#39;^2 = 1 + 2 x_3&#39;^2\\) defined a hyperboloid (2 sheeted). 6.6 Cayley-Hamilton Theorem If \\(A\\) is a \\(n \\times n\\) complex matrix and \\(f(t) = c_0 + c_1 t + \\dots + c_k t^k\\) is a poly of degree k, then \\[\\begin{align*} f(A) = c_0 I + c_1 A + \\dots + c_k A^k. \\end{align*}\\] We can also define power series of matrices subject to convergence \\[\\begin{align*} \\text{e.g. } \\exp A = I + A + \\dots + \\frac{1}{r!} A^r + \\dots \\end{align*}\\] converges for any \\(A\\). Note If \\(D = \\begin{pmatrix}\\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n\\end{pmatrix}\\) is diagonal then \\(D^r = \\begin{pmatrix}\\lambda_1^r &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n^r\\end{pmatrix}\\) and \\(f(D) = \\begin{pmatrix}f(\\lambda_1) &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; f(\\lambda_n)\\end{pmatrix}\\). If \\(B = P^{-1} A P\\) for invertible \\(P\\), i.e. \\(A\\) and \\(B\\) are similar then \\(B^r = P^{-1} A^r P\\) and \\(f(B) = f(P^{-1} A P) = P^{-1} f(A) P\\). Recall the characteristic polynomial is \\(\\chi_A(t) = \\det (A - tI) = c_0 + c_1 t + \\dots + c_n t^n\\) where \\(c_0 = \\det A\\) and \\(c_n = (-1)^n\\), Deductions involving \\(\\chi_A(t)\\). Theorem 6.4 (Cayley-Hamilton) \\[\\begin{align*} \\chi_A(A) &amp;= c_0 I + c_1 A + \\dots + c_n A^n = 0 \\end{align*}\\] a matrix satisfies its own characteristic equation Note Cayley-Hamilton, 6.4, implies \\[\\begin{align*} c_0 I = - A \\left(c_1 I + \\dots + c_n A^{n-1} \\right) \\end{align*}\\] and if \\(c_0 = \\det A \\neq 0\\) then \\[\\begin{align*} A^{-1} &amp;= - \\frac{1}{c_0} \\left(c_1 I + \\dots + c_n A^{n-1} \\right) \\end{align*}\\] Proof. General \\(2 \\times 2\\) matrix \\[\\begin{align*} A = \\begin{pmatrix}a &amp; b \\\\c &amp; d\\end{pmatrix} \\implies \\chi_A(t) = t^2 - (a + d)t + (ad - bc) \\end{align*}\\] then check by substitution that \\(\\chi_A(A) = 0\\). (Sheet 4) Diagonalisable \\(n \\times n\\) matrix: consider \\(A\\) with evals \\(\\lambda_i\\) and invertible \\(P\\) s.t. \\[\\begin{align*} PAP^{-1} &amp;= D = \\begin{pmatrix}\\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n\\end{pmatrix}.\\\\ \\text{Now } \\chi_A(D) &amp;= \\begin{pmatrix}\\chi_A(\\lambda_1) &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\chi_A(\\lambda_n) \\end{pmatrix} = 0 \\quad \\text{since $\\chi_A$ is poly and $\\lambda_i$ evals} \\\\ \\text{Then } \\chi_A(A) &amp;= \\chi_A(P^{-1} D P) \\\\ &amp;= P^{-1} \\chi_A(D) P \\\\ &amp;= 0 \\end{align*}\\] For symmetric \\(3 \\times 3\\) matrices, we may guess that \\(\\begin{pmatrix}1 &amp; 1 &amp; 1\\end{pmatrix}^T\\) is an eigenvector. "],["changing-bases-canonical-forms-and-symmetries.html", "7 Changing Bases, Canonical Forms and Symmetries 7.1 Changing Bases in General 7.2 Jordan Canonical/ Normal Form 7.3 Conics and Quadrics 7.4 Symmetries and Transformation Groups.", " 7 Changing Bases, Canonical Forms and Symmetries 7.1 Changing Bases in General 7.1.1 Definition and Proposition Recall Matrices for Linear Maps in General given linear map \\(T : V \\to W\\) (real or complex vector spaces) and choice of bases \\[\\begin{align*} \\{\\underline{e}_i\\} \\hspace{0.5cm} i &amp;= 1, \\dots, n \\text{ for } V \\\\ \\{\\underline{f}_a\\} \\hspace{0.5cm} a &amp;= 1, \\dots, m \\text{ for } W \\end{align*}\\] the matrix \\(A\\) (\\(m \\times n\\)) w.r.t these bases is defined by \\[\\begin{align*} T(\\underline{e}_i) &amp;= \\sum_a \\underline{f}_a A_{ai}. \\end{align*}\\] This definition is chosen to ensure \\[\\begin{align*} \\underline{y} = T(\\underline{x}) \\iff y_a &amp;= \\sum_i A_{ai} x_i \\\\ &amp;= A_{ai} x_i \\ (\\sum \\text{ convention}) \\\\ \\text{where } \\underline{x} = \\sum_i x_i \\underline{e}_i,\\ \\underline{y} &amp;= \\sum_a y_a \\underline{f}_a \\\\ \\text{which holds since } T\\left(\\sum_i x_i \\underline{e}_i \\right) &amp;= \\sum_i x_i T(\\underline{e}_i) \\\\ &amp;= \\sum_i x_i \\left( \\sum_a \\underline{f}_a A_{ai} \\right) \\\\ &amp;= \\sum_a \\underbrace{\\left( \\sum_i A_{ai} x_i \\right)}_{y_a} \\underline{f}_a. \\end{align*}\\] The same linear map \\(T\\) has matrix \\(A&#39;\\) w.r.t bases \\[\\begin{align*} \\{\\underline{e}&#39;_i\\} \\hspace{0.5cm} i &amp;= 1, \\dots, n \\text{ for } V \\\\ \\{\\underline{f}&#39;_a\\} \\hspace{0.5cm} a &amp;= 1, \\dots, m \\text{ for } W \\end{align*}\\] defined by \\[\\begin{align*} T(\\underline{e}&#39;_i) &amp;= \\sum_a \\underline{f}&#39;_a A&#39;_{ai}. \\end{align*}\\] To relate \\(A\\) and \\(A&#39;\\) we need to say how bases are related, and change of base matrices \\(P\\) (\\(n \\times n\\)) and \\(Q\\) (\\(m \\times m\\)) are defined by \\[\\begin{align*} \\underline{e}&#39;_i &amp;= \\sum_j \\underline{e}_j P_{ji},\\ \\underline{f}&#39;_a = \\sum_b \\underline{f}_b Q_{ba} \\end{align*}\\] Note: \\(P\\) and \\(Q\\) are invertible; in relation above we can exchange \\(\\underline{e}_i\\) and \\(\\underline{e}&#39;_i\\) with \\(P \\to P^{-1}\\) and similarly for \\(Q\\). Proposition 7.1 With definitions as above \\[\\begin{align*} A&#39; = Q^{-1} A P \\end{align*}\\] which is the change of basis formula for matrix of a linear map. Example 7.1 \\(n = 2,\\ m = 3\\) \\[\\begin{align*} T(\\underline{e}_1) &amp;= \\underline{f_1} + 2 \\underline{f}_2 - \\underline{f}_3 = \\sum_a \\underline{f}_a A_{a 1} \\\\ T(\\underline{e}_2) &amp;= -\\underline{f_1} + 2 \\underline{f}_2 + \\underline{f}_3 = \\sum_a \\underline{f}_a A_{a 2} \\\\ \\implies A &amp;= \\begin{pmatrix}1 &amp; -1 \\\\2 &amp; 2 \\\\-1 &amp; 1\\end{pmatrix} \\end{align*}\\] New basis for \\(V\\) \\[\\begin{align*} \\underline{e}&#39;_1 &amp;= \\underline{e}_1 - \\underline{e}_2 = \\sum_i \\underline{e}_i P_{i 1} \\\\ \\underline{e}&#39;_2 &amp;= \\underline{e}_1 + \\underline{e}_2 = \\sum_i \\underline{e}_i P_{i 2} \\\\ \\implies P &amp;= \\begin{pmatrix}1 &amp; 1 \\\\-1 &amp; 1\\end{pmatrix} \\end{align*}\\] New basis for \\(W\\) \\[\\begin{align*} \\underline{f}&#39;_1 &amp;= \\underline{f}_1 - \\underline{f}_3 = \\sum_i \\underline{e}_i P_{i 1} \\\\ \\underline{f}&#39;_2 &amp;= \\underline{f}_2 = \\sum_i \\underline{e}_i P_{i 2} \\\\ \\underline{f}&#39;_3 &amp;= \\underline{f}_1 + \\underline{f}_3 = \\sum_i \\underline{e}_i P_{i 3} \\\\ \\implies Q &amp;= \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 \\end{pmatrix} \\end{align*}\\] Change of basis formula: \\[\\begin{align*} A&#39; &amp;= Q^{-1} A P \\\\ &amp;= \\begin{pmatrix} \\frac{1}{2} &amp; 0 &amp; -\\frac{1}{2} \\\\ 0 &amp; 1 &amp; 0 \\\\ \\frac{1}{2} &amp; 0 &amp; \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix}1 &amp; -1 \\\\2 &amp; 2 \\\\-1 &amp; 1\\end{pmatrix} \\begin{pmatrix}1 &amp; 1 \\\\-1 &amp; 1\\end{pmatrix} \\\\ &amp;= \\begin{pmatrix}2 &amp; 0 \\\\4 &amp; 0 \\\\0 &amp; 0\\end{pmatrix}. \\end{align*}\\] Direct check: \\[\\begin{align*} T(\\underline{e}&#39;_1) &amp;= 2 \\underline{f}&#39;_1 \\\\ T(\\underline{e}&#39;_2) &amp;= 4 \\underline{f}&#39;_2 \\ \\checkmark. \\end{align*}\\] 7.1.2 Proof of proposition Proof. \\[\\begin{align*} T(\\underline{e}&#39;_i) &amp;= T\\left( \\sum_j \\underline{e}_j P_{ji} \\right) \\ \\text{ definition of $P$} \\\\ &amp;= \\sum_j T(\\underline{e}_j) P_{ji} \\ \\text{ linearity of $T$} \\\\ &amp;= \\sum_j \\sum_a \\underline{f}_a A_{aj} P_{ji} \\ \\text{ definition of $A$} \\\\ T(\\underline{e}&#39;_i) &amp;= \\sum_b \\underline{f}&#39;_b A&#39;_{bi} \\ \\text{definition of $A&#39;$} \\\\ &amp;= \\sum_b \\sum_a \\underline{f}_a Q_{ab} A&#39;_{bi} \\ \\text{ definition of $Q$} \\end{align*}\\] Comparing coefficients of \\(\\underline{f}_a\\) as it is a basis \\[\\begin{align*} \\sum_j A_{aj} P_{ji} = \\sum_b Q_{ab} A&#39;_{bi} \\\\ \\text{or } AP = QA&#39;. \\end{align*}\\] 7.1.3 Approach using vector components Consider \\[\\begin{align*} \\underline{x} &amp;= \\sum_j x_j \\underline{e}_j \\\\ &amp;= \\sum_i x&#39;_i \\underline{e}&#39;_i \\\\ &amp;= \\sum_j \\left( \\sum_i P_{ji} x&#39;_i \\right) \\underline{e}_j \\\\ \\implies x_j &amp;= P_{ji} x&#39;_i \\\\ \\text{Write } X &amp;= \\begin{pmatrix}x_1 \\\\\\vdots \\\\x_n\\end{pmatrix},\\ X&#39; = \\begin{pmatrix}x&#39;_1 \\\\ \\vdots \\\\ x&#39;_n \\end{pmatrix} \\\\ \\text{then } X &amp;= PX&#39; \\text{ or } X&#39; = P^{-1} X \\end{align*}\\] Note: some care needed if \\(V = \\mathbb{R}^n\\), e.g. \\(n = 2\\) with \\(\\underline{e}_1 = \\begin{pmatrix}1 \\\\1\\end{pmatrix},\\ \\underline{e}_2 = \\begin{pmatrix}1 \\\\-1\\end{pmatrix}\\) then \\(\\underline{x} = \\begin{pmatrix}5 \\\\1\\end{pmatrix} \\in \\mathbb{R}^2\\) has \\(\\underline{x} = 3 \\underline{e}_1 + 2 \\underline{e}_2\\) so \\(X = \\begin{pmatrix}3 \\\\2\\end{pmatrix}\\). Similarly \\[\\begin{align*} \\underline{y} &amp;= \\sum_b y_j \\underline{f}_b \\\\ &amp;= \\sum_a y&#39;_a \\underline{f}&#39;_a \\\\ \\implies y_b &amp;= Q_{ba} y&#39;_a \\\\ \\text{Then } Y &amp;= QY&#39; \\text{ or } Y&#39; = Q^{-1} Y \\\\ \\text{where } Y &amp;= \\begin{pmatrix}y_1 \\\\\\vdots \\\\y_m \\end{pmatrix},\\ Y&#39; = \\begin{pmatrix}y&#39;_1 \\\\ \\vdots \\\\ y&#39;_m \\end{pmatrix} \\end{align*}\\] Now, matrices \\(A, A&#39;\\) are defined to ensure \\(Y = AX\\) and \\(Y&#39; = A&#39; X&#39;\\). \\[\\begin{align*} \\text{But } Y&#39; &amp;= Q^{-1} Y = Q^{-1} A X \\\\ &amp;= (Q^{-1} A P) X&#39; \\\\ &amp;= A&#39; X&#39; \\end{align*}\\] which is true \\(\\forall \\; \\underline{x}\\), so \\(A&#39; = Q^{-1} A P\\). 7.1.4 Comments Definition of matrix \\(A\\) for \\(T : V \\to W\\) w.r.t. bases \\(\\{ \\underline{e}_i \\}\\) and \\(\\{ \\underline{f}_a \\}\\) can be expressed: column \\(i\\) of \\(A\\) consists of components of \\(T(\\underline{e}_i)\\) w.r.t. basis \\(\\{ \\underline{f}_a \\}\\) [For \\(T : \\mathbb{R}^n \\to \\mathbb{R}^m\\) with standard bases, columns of \\(A\\) are images of standard basis vectors]. Similarly, definitions of \\(P\\) and \\(Q\\) say: columns consist of components of new basis vectors w.r.t. old. With \\(V = W\\) and same bases and \\(\\underline{e}_i = \\underline{f}_i,\\ \\underline{e}&#39;_i = \\underline{f}&#39;_i\\) we have \\(P = Q\\) and \\(A&#39; = P^{-1} A P\\). Matrices representing the same linear map w.r.t. different bases are similar; conversely if \\(A\\) and \\(A&#39;\\) are similar then we can regard them as representing the same linear map with \\(P\\) defining the change of basis. In Diagonalisability and Similarity we observed \\[\\begin{align*} \\operatorname{tr}(A&#39;) &amp;= \\operatorname{tr}(A) \\\\ \\det(A&#39;) &amp;= \\det A \\\\ \\chi_{A&#39;}(t) &amp;= \\chi_A(t) \\end{align*}\\] so these are properties of the linear map. \\(V = W = \\mathbb{R}^n\\) or \\(\\mathbb{C}^n\\), with \\(\\underline{e}_i\\) the standard basis - matrix \\(A\\) is diagonalisable iff \\(\\exists\\) basis of eigenvectors \\(\\underline{e}&#39;_i = \\underline{v}_i\\) with \\(A \\underline{v}_i = \\lambda_i \\underline{v}_i\\) (not \\(\\sum\\)) and then \\(A&#39; = P^{-1} A P = D = \\begin{pmatrix}\\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_n\\end{pmatrix}\\) and \\(\\underline{v}_i = \\sum_j \\underline{e}_j P_{ji}\\) so the eigenvectors are the columns of \\(P\\). Specialising further \\(A^\\dagger = A \\implies \\exists\\) basis of orthonormal eigenvectors \\(\\underline{e}&#39;_i = \\underline{u}_i\\) and \\(P^\\dagger = P^{-1}\\). 7.2 Jordan Canonical/ Normal Form This results classifies \\(n \\times n\\) complex matrices up to similarity. Proposition 7.2 Any \\(2 \\times 2\\) complex matrix \\(A\\) is similar to one of the following: \\(A&#39; = \\begin{pmatrix}\\lambda_1 &amp; 0 \\\\0 &amp; \\lambda_2\\end{pmatrix}\\) with \\(\\lambda_1 \\neq \\lambda_2,\\ \\chi_A = (t - \\lambda_1) (t - \\lambda_2)\\). \\(A&#39; = \\begin{pmatrix}\\lambda &amp; 0 \\\\0 &amp; \\lambda\\end{pmatrix}\\) with \\(\\chi_A = (t - \\lambda)^2\\). \\(A&#39; = \\begin{pmatrix}\\lambda &amp; 1 \\\\0 &amp; \\lambda\\end{pmatrix}\\) with \\(\\chi_A = (t - \\lambda)^2\\). Proof. \\(\\chi_A(t)\\) has 2 roots over \\(\\mathbb{C}\\). For distinct roots/ evals \\(\\lambda_1, \\lambda_2\\) we have \\(M_1 = m_1 = M_2 = m_2 = 1\\) and eigenvectors \\(\\underline{v}_1, \\underline{v}_2\\) provide a basis. Hence \\(A&#39; = P^{-1} A P\\) where eigenvectors are columns of \\(P\\). For repeated root/ eval \\(\\lambda\\), if \\(M_\\lambda = m_\\lambda = 2\\), then same argument applies. For repeated root/ eval \\(\\lambda\\), with \\(M_\\lambda = 2\\) and \\(m_\\lambda = 1\\), let \\(\\underline{v}\\) be an eigenvector for \\(\\lambda\\) and \\(\\underline{w}\\) any linearly independent vector. Then \\[\\begin{align*} A \\underline{v} &amp;= \\lambda \\underline{v} \\\\ A \\underline{w} &amp;= \\alpha \\underline{v} + \\beta \\underline{w}. \\end{align*}\\] Matrix of map w.r.t basis \\(\\{\\underline{v}, \\underline{w}\\}\\) is \\[\\begin{align*} \\begin{pmatrix}\\lambda &amp; \\alpha \\\\0 &amp; \\beta\\end{pmatrix}. \\end{align*}\\] But \\(\\beta = \\lambda\\) (otherwise we get case (i) with evals \\(\\lambda, \\beta\\)) and \\(\\alpha \\neq 0\\) (otherwise case (ii)). Now set \\(\\underline{u} = \\alpha \\underline{v}\\) and note \\[\\begin{align*} A \\underline{u} = \\lambda \\underline{u} \\\\ A \\underline{w} = \\underline{u} + \\lambda \\underline{w} \\end{align*}\\] so w.r.t. basis \\(\\{ \\underline{u}, \\underline{w}\\}\\) matrix is \\[\\begin{align*} A&#39; = \\begin{pmatrix}\\lambda &amp; 1 \\\\0 &amp; \\lambda\\end{pmatrix}. \\end{align*}\\] Example 7.2 (using a slightly different approach) \\[\\begin{align*} A &amp;= \\begin{pmatrix}1 &amp; 4 \\\\-1 &amp; 5\\end{pmatrix} \\\\ \\implies \\chi_A(t) &amp;= (t - 3)^2 \\\\ A - 3 I &amp;= \\begin{pmatrix}-2 &amp; 4 \\\\-1 &amp; 2\\end{pmatrix}. \\end{align*}\\] Choose \\(\\underline{w} = \\begin{pmatrix}1 \\\\0\\end{pmatrix}\\) which is not an eigenvector and then \\(\\underline{u} = (A - 3I) \\underline{w} = \\begin{pmatrix}-2 \\\\-1\\end{pmatrix}\\). But \\((A - 3I)^2 = 0\\) by Cayley-Hamilton Theorem, 6.4, so \\((A - 3I)^2 \\underline{w} = (A - 3I) \\underline{u} = 0\\) so \\[\\begin{align*} A \\underline{u} &amp;= 3 \\underline{u} \\\\ A \\underline{w} &amp;= \\underline{u} + 3 \\underline{w} \\end{align*}\\] so basis \\(\\{ \\underline{u}, \\underline{w}\\}\\) gives Jordan Canonical Form. Check: \\(P = \\begin{pmatrix}-2 &amp; 1 \\\\-1 &amp; 0\\end{pmatrix} \\implies P^{-1} = \\begin{pmatrix}0 &amp; -1 \\\\1 &amp; -2\\end{pmatrix}\\) so \\(P^{-1} A P = \\begin{pmatrix}3 &amp; 1 \\\\0 &amp; 3\\end{pmatrix}\\). Generalisation to larger matrices can be considered, starting with \\(N = \\begin{pmatrix}0 &amp; 1 &amp; &amp; \\\\ &amp; 0 &amp; \\ddots &amp; \\\\ &amp; &amp; \\ddots &amp; 1 \\\\ &amp; &amp; &amp; 0\\end{pmatrix}\\) (\\(n \\times n\\)) when applied to standard basis vectors gives \\(\\underline{e}_n \\mapsto \\underline{e}_{n-1} \\mapsto \\dots \\mapsto \\underline{e}_1 \\mapsto \\underline{0}\\). Note \\(J = \\lambda I + N\\) then \\(\\chi_J(t) = (\\lambda - t)^n\\) but \\(m_\\lambda = 1\\) (\\(M_\\lambda = n\\)). Theorem 7.1 Any \\(n \\times n\\) complex matrix \\(A\\) is similar to a matrix \\(A&#39;\\) with block form \\[\\begin{align*} A&#39; &amp;= \\begin{pmatrix} \\boxed{J_{n_1}(\\lambda_1)} &amp; &amp; &amp; \\\\ &amp; \\boxed{J_{n_2}(\\lambda_2)} &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\boxed{J_{n_r}(\\lambda_r)} \\end{pmatrix} \\end{align*}\\] where each diagonal block is a Jordan block with form \\[\\begin{align*} J_p(\\lambda) &amp;= \\begin{pmatrix} \\lambda &amp; 1 &amp; &amp; &amp; \\\\ &amp; \\lambda &amp; 1 &amp; &amp; \\\\ &amp; &amp; \\lambda &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\ddots &amp; 1 \\\\ &amp; &amp; &amp; &amp; \\lambda \\end{pmatrix} \\end{align*}\\] (\\(p \\times p\\)) with \\(n_1 + n_2 + \\dots + n_r = n\\), \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_r\\) are the eigenvalues of \\(A\\) and \\(A&#39;\\) (same eigenvalue may appear in more than one block). \\(A\\) is diagonalisable iff \\(A&#39;\\) consists of \\(1 \\times 1\\) Jordan blocks only. Proof. See Linear Algebra and GRM in Part IB. 7.3 Conics and Quadrics 7.3.1 Quadrics in General Definition 7.1 (Quadric) A quadric in \\(\\mathbb{R}^n\\) is a hypersurface defined by \\[\\begin{align*} Q(\\underline{x}) = \\underline{x}^T A \\underline{x} + \\underline{b}^T \\underline{x} + c = 0 \\end{align*}\\] for some \\(A\\), \\(n \\times n\\) real symmetric, non-zero matrix, \\(\\underline{b} \\in \\mathbb{R}^n\\), \\(c \\in \\mathbb{R}\\). So \\(Q(\\underline{x}) = A_{ij} x_i x_j + b_i x_i + c = 0\\). Consider classifying solutions up to geometrical equivalence : no distinction between solutions related by isometries in \\(\\mathbb{R}^n\\), i.e. related by translation - change in origin orthogonal transformation about origin - change in the axes If \\(A\\) is invertible (no zero eigenvalues) then by setting \\(\\underline{y} = \\underline{x} + \\frac{1}{2} A^{-1} \\underline{b}\\) we have \\[\\begin{align*} \\underline{y}^T A \\underline{y} &amp;= (\\underline{x} + \\frac{1}{2}A^{-1} \\underline{b})^T A (\\underline{x} + \\frac{1}{2}A^{-1} \\underline{b}) \\\\ &amp;= \\underline{x}^T A \\underline{x} + \\underline{b}^T \\underline{x} + \\frac{1}{4} \\underline{b}^T A^{-1} \\underline{b} \\end{align*}\\] since \\((A^{-1} \\underline{b})^T = \\underline{b} A^{-1}\\) as \\(A\\) is symmetric so \\(\\frac{1}{2} (A^{-1} \\underline{b})^T A \\frac{1}{2} (A^{-1} \\underline{b})\\) is the last term. Then \\(Q(\\underline{x}) = 0 \\iff \\mathcal{F}(\\underline{y}) = k\\) with \\(\\mathcal{F}(\\underline{y}) = \\underline{y}^T A \\underline{y}\\) (a quadratic form w.r.t. new origin \\(\\underline{y} = 0\\)) and \\(k = \\frac{1}{4} \\underline{b}^T A^{-1} \\underline{b} - c\\). Diagonalise \\(\\mathcal{F}\\) as in Quadratic Forms: orthonormal eigenvectors give principal axes; eigenvalues of \\(A\\) and value of \\(k\\) determine nature of this quadric. Examples in \\(\\mathbb{R}^3\\) given in In \\(\\mathbb{R}^3\\) eigenvalues \\(&gt; 0\\) and \\(k &gt; 0\\) give ellipsoid eigenvalues have different signs and \\(k \\neq 0\\) gives a hyperboloid. If \\(A\\) has one or more zero eigenvalues then analysis changes and simplest standard form may have both linear and quadratic terms. 7.3.2 Conics Quadrics in \\(\\mathbb{R}^2\\) are curves, *conics. If \\(\\det A \\neq 0\\) By completing the square and diagonalising we get a standard form \\[\\begin{align*} \\lambda_1 x_1&#39;^2 + \\lambda_2 x_2&#39;^2 &amp;= k \\end{align*}\\] \\(\\lambda_1, \\lambda_2 &gt; 0 \\implies\\) ellipse for \\(k &gt; 0\\), point for \\(k = 0\\) and no soln for \\(k &lt; 0\\). \\(\\lambda_1 &gt; 0, \\lambda_2 &lt; 0 \\implies\\) hyperbola for \\(k &gt; 0\\) or \\(k &lt; 0\\) and a pair of lines for \\(k = 0\\). e.g. \\(x_1&#39;^2 - x_2&#39;^2 = (x_1 - x_2) (x_1 + x_2) = 0\\). If \\(\\det A = 0\\) Suppose \\(\\lambda_1 &gt; 0\\) and \\(\\lambda_2 = 0\\); diagonalise \\(A\\) in original formula to get \\[\\begin{align*} \\lambda_1 x_1&#39;^2 + b_1&#39; x_1&#39; + b_2&#39; x_2&#39; + c &amp;= 0 \\\\ \\iff \\lambda_1 x_1&#39;&#39;^2 + b_2&#39; x_2&#39; + c&#39; &amp;= 0 \\end{align*}\\] where \\(x_1&#39;&#39; = x_1&#39; + \\frac{1}{2 \\lambda_1} b_1&#39;\\) and \\(c&#39; = c - \\frac{b_1&#39;^2}{4 \\lambda_1^2}\\) If \\(b_2&#39; = 0\\) we get a pair of lines for \\(c&#39; &lt; 0\\), a single line for \\(c&#39; = 0\\) and no soln for \\(c&#39; &gt; 0\\) If \\(b_2&#39; \\neq 0\\) equations becomes \\(\\lambda_1 x_1&#39;&#39;^2 + b_2 x_2&#39;&#39; = 0\\), a parabola, where \\(x_2&#39;&#39; = x_2 + \\frac{1}{b_2&#39;}c&#39;\\). 7.4 Symmetries and Transformation Groups. 7.4.1 Orthogonal Transformations and Rotations in \\(\\mathbb{R}^n\\) \\[\\begin{align*} \\underset{n \\times n}{R} \\text{ is orthogonal} &amp;\\iff R^T R = R R^T = I \\\\ &amp;\\iff (R \\underline{x}) \\cdot (R \\underline{y}) = \\underline{x} \\cdot \\underline{y} \\ \\forall \\; \\underline{x}, \\underline{y} \\\\ &amp;\\iff \\text{ cols or rows of $R$ are orthonormal vectors}. \\end{align*}\\] The set of such matrices forms the orthogonal group \\(O(n)\\). \\[\\begin{align*} R \\in O(n) \\implies \\det R = \\pm 1. \\end{align*}\\] (\\(\\det (R^T) \\det R = (\\det R)^2 = 1\\)). \\(SO(n) = \\{R \\in O(n) : \\det R = + 1\\}\\) is a subgroup, the special orthogonal group. \\(R \\in O(n) \\implies\\) R preserves lengths and |n-dim volume|. \\(R \\in SO(n) \\implies\\) R also preserves orientation. \\(SO(n)\\) consists of all rotations in \\(\\mathbb{R}^n\\) Reflections belong to \\(O(n)\\) but not \\(SO(n)\\). For a specific \\(H \\in O(n) \\setminus SO(n)\\), any element of \\(O(n)\\) is of teh form \\(R\\) or \\(RH\\) with \\(R \\in SO(n)\\), e.g. if \\(n\\) is odd, we can choose \\(H = -I\\). 7.4.1.1 Active and Passive points of View For a rotation \\(R\\) (matrix), the transformation \\[\\begin{align*} x_i&#39; = R_{ij} x_j \\end{align*}\\] can be viewed in two ways. Active view point: rotation transforms vectors, i.e. \\(x_i&#39;\\) are the components of the new vector so \\(\\underline{x}&#39; = R \\underline{x}\\) w.r.t. standard basis \\(\\{ \\underline{e}_i \\}\\). Example 7.3 ($\\mathbb{R}^2$) \\(|\\underline{x}&#39;|^2 = |\\underline{x}|^2\\) Passive view point: rotation changes basis, i.e. \\(x_i&#39;\\) are the components of same vector \\(\\underline{x}\\) but w.r.t. new basis \\(\\{\\underline{u}_i\\}\\). Example 7.4 ($\\mathbb{R}^2$) \\[\\begin{align*} \\underline{u}_i &amp;= \\sum_j R_{ij} \\underline{e}_j \\\\ &amp;= \\sum_J \\underline{e}_j (R^T)_{ji} \\\\ &amp;= \\sum_J \\underline{e}_j (R^{-1})_{ji} \\end{align*}\\] (compare to Quadratic Forms : \\(P = R^{-1}\\)). 7.4.2 2d Minkowski Space and Lorentz Transformations Define a new inner product on \\(\\mathbb{R}^2\\) by \\[\\begin{align*} (\\underline{x}, \\underline{y}) = \\underline{x}^T J \\underline{y} \\quad \\text{where } J = \\begin{pmatrix}1 &amp; 0 \\\\0 &amp; -1\\end{pmatrix} \\\\ \\therefore \\left( \\begin{pmatrix} x_0 \\\\ x_1 \\end{pmatrix}, \\begin{pmatrix} y_0 \\\\ y_1 \\end{pmatrix} \\right) = x_0 y_0 - x_1 y_1 \\end{align*}\\] This is not positive definite since \\((\\underline{x}, \\underline{x}) = \\underline{x}^T J \\underline{x} = x_0^2 - x_1^2\\) but still bilinear and symmetric. Standard basis vectors are orthonormal in generalised sense \\[\\begin{align*} \\underline{e}_0 = \\begin{pmatrix}1 \\\\0\\end{pmatrix} &amp;\\text{ and } \\underline{e}_1 = \\begin{pmatrix}0 \\\\1\\end{pmatrix} \\\\ \\text{obey } (\\underline{e}_0, \\underline{e}_0) &amp;= 1 \\\\ (\\underline{e}_1, \\underline{e}_1) &amp;= -1 \\\\ (\\underline{e}_0, \\underline{e}_0) &amp;= 0 \\end{align*}\\] This new inner product is called the Minkowski metric and \\(\\mathbb{R}^2\\) equipped with it is called Minkowski space. Consider \\(M = \\begin{pmatrix}M_{00} &amp; M_{01} \\\\M_{10} &amp; M_{11}\\end{pmatrix}\\) giving a linear map \\(\\mathbb{R}^2 \\to \\mathbb{R}^2\\). This preserves the Minkowski metric iff \\[\\begin{align*} (M \\underline{x}, M \\underline{y}) &amp;= (\\underline{x}, \\underline{y}) \\quad \\forall \\; x, y \\in \\mathbb{R}^2 \\\\ \\iff (M \\underline{x})^T J (M \\underline{y}) &amp;= \\underline{x}^T (M^T J M) \\underline{y} \\\\ &amp;= \\underline{x}^T J \\underline{y} \\quad \\forall \\; \\underline{x}, \\underline{y} \\in \\mathbb{R}^2 \\\\ \\iff M^T J M &amp;= J. \\end{align*}\\] The set of such matrices form a group. Now \\[\\begin{align*} \\det (M^T J M) &amp;= \\det (M^T) \\det J \\det M \\\\ &amp;= \\det J \\\\ \\implies (\\det M)^2 &amp;= 1 \\implies \\det M = \\pm 1 \\end{align*}\\] Furthermore, \\(|M_{00}| \\geq 1\\), so \\(M_{00} \\geq 1\\) or \\(M_{00} \\leq -1\\). The subgroup with \\(\\det M = + 1\\) and \\(M_{00} \\geq 1\\) is the Lorentz group in 2d. General form for \\(M\\): require cols \\(M \\underline{e}_0, M \\underline{e}_1\\) to be orthonormal w.r.t Minkowski metric, like \\(\\underline{e}_0, \\underline{e}_1\\). \\[\\begin{align*} (M \\underline{e_0}, M \\underline{e_0}) = M_{00}^2 - M_{10}^2 = (\\underline{e_0}, \\underline{e_0}) = 1 \\quad (\\text{hence } |M_{00}|^2 \\geq 1) \\end{align*}\\] Taking \\(M_{00} \\geq 1\\), we can write \\[\\begin{align*} M \\underline{e_0} = \\begin{pmatrix} \\cosh \\theta \\\\ \\sinh \\theta \\end{pmatrix} \\end{align*}\\] for some real value \\(\\theta\\). For the other column, \\[\\begin{align*} (M \\underline{e_0}, M \\underline{e_1}) = 0;\\; (M \\underline{e_1}, M \\underline{e_1}) = -1 \\implies M \\underline{e_1} = \\pm \\begin{pmatrix} \\sinh \\theta \\\\ \\cosh \\theta \\end{pmatrix} \\end{align*}\\]10 The sign is fixed to be positive by the condition that \\(\\det M = +1\\). \\[\\begin{align*} M(\\theta) = \\begin{pmatrix} \\cosh \\theta &amp; \\sinh \\theta \\\\ \\sinh \\theta &amp; \\cosh \\theta \\end{pmatrix} \\end{align*}\\] For such matrices \\[\\begin{align*} M(\\theta_1) M(\\theta_2) &amp;= M(\\theta_1 + M \\theta_2). \\end{align*}\\] This confirms that they form a group. The curves defined by \\((\\underline{x}, \\underline{x}) = k\\) where \\(k\\) is a constant are hyperbolas, as shown. This is analogous to how the curves defined by \\(\\underline{x} \\cdot \\underline{x} = k\\) are circles. So applying \\(M\\) to any vector on a given branch of a hyperbola, the resultant vector remains on the hyperbola. Allowing \\(|M_{00}| \\geq 1\\) we would get transformations that could map \\(\\underline{x}\\) to the dotted branch below. 7.4.3 Application to special relativity Set \\[\\begin{align*} M(\\theta) &amp;= \\gamma(v) \\begin{pmatrix} 1 &amp; v \\\\ v &amp; 1 \\end{pmatrix} \\\\ v &amp;= \\tanh \\theta,\\quad |v| &lt; 1 \\\\ \\gamma(v) &amp;= (1 - v^2)^{-\\frac{1}{2}} \\end{align*}\\] We will rename \\(x_0 \\to t\\), which is now our time coordinate. \\(x_1 \\to x\\), our one-dimensional space coordinate. Then, \\[ \\underline{x}&#39; = M \\underline{x} \\iff \\begin{cases} t&#39; &amp; = \\gamma(v) (t + vx) \\\\ x&#39; &amp; = \\gamma(v) (x + vt) \\end{cases} \\] This is a Lorentz transformation or boost relating the time and space coordinates for observers moving with relative velocity \\(v\\) in Special Relativity, in units where the speed of light \\(c\\) is taken to be 1. The factor \\(\\gamma\\) gives rise to effects such as time dilation and length contraction. The group property \\(M(\\theta_3) = M(\\theta_1)M(\\theta_2)\\) with \\(\\theta_3 = \\theta_1 + \\theta_2\\) corresponds to the velocities \\[ v_i = \\tanh \\theta_i \\implies v_3 = \\frac{v_1 + v_2}{1 + v_1 v_2} \\] This is consistent with the fact that all velocities are less than the speed of light, 1. for standard inner product to get normal vector swap \\(x\\) and \\(y\\) values and make one negative, here we dont need to make one component negative. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
