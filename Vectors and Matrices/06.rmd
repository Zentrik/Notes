# Eigenvalues and Eigenvectors

## Introduction

### Definitions

::: {.definition name="Eigenvector and eigenvalue"}
For a linear map $T : V \to V$ ($V$ a real vector or complex vector space) a vector $\underline{v} \in V$ with $\underline{v} \neq \underline{0}$ is an *eigenvector* of $T$ with eigenvalue $\lambda$ if \begin{align*}
    T(\underline{v}) = \lambda \underline{v}.
\end{align*} 
:::

If $V = \mathbb{R}^n$ or $\mathbb{C}^n$ and $T$ is given by an $n \times n$ matrix $A$ then
\begin{align*}
    A \underline{v} = \lambda \underline{v} \iff  (A - \lambda I) \underline{v} = \underline{0}
\end{align*} and for a given $\lambda$ this holds for some $\underline{v} \neq 0 \iff \det(A - \lambda I) = 0$, the *characteristic equation*, i.e. $\lambda$ is an eigenvalue iff it is a root of $\chi_A (t) = \det (A - tI)$, the *characteristic polynomial*.
$\chi_A (t)$ is a polynomial of degree $n$ for $A$ ($n \times n$).
We find eigenvalues as roots of the characteristic equation/ polynomial and then determine corresponding eigenvectors.

### Examples

::: {.example}
\begin{align*}
    V &= \mathbb{C}^2 \text{ and } A = \begin{pmatrix}
    2 & i \\
    -i & 2
    \end{pmatrix} \\
    \det (A - \lambda I) &= \begin{vmatrix}
    2 - \lambda & i \\
    -i & 2 - \lambda
    \end{vmatrix} \\
    &= (2 - \lambda)^2 - 1 \\
    &= 0 \iff \lambda = 1 \text{ or } 3.
\end{align*} 
To find eigenvectors $\underline{v} = \begin{pmatrix}v_1 \\v_2\end{pmatrix}$:

- $\lambda = 1$:
\begin{align*}
    (A - I) \underline{v} &= \begin{pmatrix}
        1 & i \\
        -i & 1
        \end{pmatrix} \begin{pmatrix}v_1 \\v_2\end{pmatrix} = \underline{0} \\
    \implies \underline{v} &= \alpha \begin{pmatrix}1 \\i\end{pmatrix} \text{ any } \alpha \neq 0 
\end{align*} 

- $\lambda = 3$:
\begin{align*}
    (A - 3I) \underline{v} &= \begin{pmatrix}
        -1 & i \\
        -i & -1
        \end{pmatrix} \begin{pmatrix}v_1 \\v_2\end{pmatrix} = \underline{0} \\
    \implies \underline{v} &= \beta \begin{pmatrix}1 \\ -i\end{pmatrix} \text{ any } \beta \neq 0 
\end{align*} 
:::

::: {.example}
\begin{align*}
    V &= \mathbb{R}^2 \text{ and } A = \begin{pmatrix}
    1 & 1 \\
    0 & 1
    \end{pmatrix} \\
    \det (A - \lambda I) &= \begin{vmatrix}
    1 - \lambda & 1 \\
    0 & 1 - \lambda
    \end{vmatrix} \\
    &= (1 - \lambda)^2\\
    &= 0 \iff \lambda = 1.
\end{align*} 
To find eigenvectors $\underline{v} = \begin{pmatrix}v_1 \\v_2\end{pmatrix}$:

\begin{align*}
    (A - I) \underline{v} &= \begin{pmatrix}
        0 & 1 \\
        0 & 0
        \end{pmatrix} \begin{pmatrix}v_1 \\v_2\end{pmatrix} = \underline{0} \\
    \implies \underline{v} &= \alpha \begin{pmatrix}1 \\ 0\end{pmatrix} \text{ any } \alpha \neq 0 
\end{align*} 
:::

::: {.example}
\begin{align*}
    V &= \mathbb{R}^2 \text{ or } \mathbb{C}^2\\
    U &= \begin{pmatrix}
    \cos \theta & - \sin \theta \\
    \sin \theta & \cos \theta
    \end{pmatrix} \\
    \chi_U (t) &= \det (U - t I) \\
    &= t^2 - 2t \cos \theta + 1 \\
    \implies \lambda &= e^{\pm i \theta} \\
    \implies \underline{v} &= \theta \begin{pmatrix}1 \\ \mp i\end{pmatrix} \ (\alpha \neq 0 )
\end{align*} 
:::

### Deductions involving $\chi_A(t)$

For $A$ $n \times n$, characteristic polynomial has degree $n$
\begin{align*}
    \chi_A(t) &= \det \begin{pmatrix}
    A_{11} - t & A_{12} & \ldots & A_{1n} \\
    A_{21} & A_{22} - t & \ldots & A_{2n} \\
    \vdots & \vdots &  & \vdots \\
    A_{n1} & A_{n2} & \ldots & A_{nn} - t 
    \end{pmatrix} \\
    &= \sum_{j=0}^{n} c_j t^j, \text{ for some } c_j \in \mathbb{C} \\
    &= (-1)^n (t - \lambda_1) \ldots (t - \lambda_n)
\end{align*} 

i. $\exists$ at least one eigenvalue (one root of $\chi_A$); in fact $\exists$ $n$ roots counted with multiplicity (FTA).

ii. $\operatorname{tr}(A) = A_{ii} = \sum_i \lambda_i$, sum of eigenvalues, by comparing terms of order $n - 1$ in $t$. (???)

iii. $\det A = \chi_A(0) = \Pi_i \lambda_i$, product of eigenvalues.

iv. If $A$ is diagonal:
\begin{align*}
    A = \begin{pmatrix}
    \lambda_1 & & \\
    & \ddots & \\
    & & \lambda_n
    \end{pmatrix}
\end{align*} with the diagonal entries being the eigenvalues; (ii) and (iii) are then immediate.

v. If $A$ is real, then coefficients $c_j$ are real and $\chi_A(\lambda) = 0 \iff \chi_A(\overline{\lambda}) = 0$: non-real roots occur in conjugate pairs.

## Eigenspaces and Multiplicities

### Definitions

::: {.definition name="Eigenspace"}
For an eigenvalue $\lambda$ of matrix $A$, define the *eigenspace*
\begin{align*}
    E_\lambda = \{ \underline{v} : A \underline{v} = \lambda \underline{v}\} = \ker (A - \lambda I),
\end{align*} this is the subspace consisting of the eigenvectors and $\underline{0}$.
:::

::: {.definition name="Geometric multiplicity"}
The *geometric multiplicity*
\begin{align*}
    m_\lambda = \dim E_\lambda = \operatorname{null}(A - \lambda I),
\end{align*} the number of linearly independent eigenvectors with eigenvalue $\lambda$.
:::

::: {.definition name="Algebraic multiplicity"}
The *algebraic multiplicity* $M_\Lambda$ is the multiplicity of $\lambda$ as a root of $\chi_A$, i.e. $\chi_A(t) = (t-\lambda)^{M_\lambda} f(t)$ (with $f(\lambda) \neq 0$).
:::

::: {.proposition}
\begin{align*}
    M_\lambda \geq m_\lambda
\end{align*} 
Further discussion in 6.3.
:::

### Examples

:::{.example}

\

i. \begin{align*}
    A &= \begin{pmatrix}
    -2 & 2 & -3 \\
    2 & 1 & -6 \\
    -1 & -2 & 0
    \end{pmatrix} \\
    \chi_A(t) &= \det (A - tI) \\
    &= (5 - t) (t + 3)^2 \\
    \text{roots } \lambda &= 5, - 3 \\
    M_5 &= 1, M_{-3} = 2. \\ \\
    \underline{\lambda = 5:} \\
    (A - 5I)\underline{x} &= \begin{pmatrix}
    -7 & 2 & -3 \\
    2 & -4 & -6 \\
    -1 & -2 & -5
    \end{pmatrix}\begin{pmatrix}x_1 \\x_2 \\x_3\end{pmatrix} = \underline{0} \\
    \implies E_5 &= \left\{ \alpha \begin{pmatrix}1 \\2 \\-1\end{pmatrix} \right\} \\ \\
    \underline{\lambda = -3:} \\
    (A + 3I)\underline{x} &= \begin{pmatrix}
    1 & 2 & -3 \\
    2 & 4 & -6 \\
    -1 & -2 & 3
    \end{pmatrix}\begin{pmatrix}x_1 \\x_2 \\x_3\end{pmatrix} = \underline{0} \\
    \text{Solve to find} \\
    \underline{x} &= \begin{pmatrix}-2 x_1 + 3 x_3 \\x_2 \\x_3\end{pmatrix} \\
    \implies E_{-3} &= \left\{ \alpha \begin{pmatrix}-2 \\1 \\0\end{pmatrix} + \beta \begin{pmatrix}3 \\0 \\1\end{pmatrix} \right\} \\ \\
    m_6 &= \dim E_5 = 1 = M_5 \\
    m_{-3} &= \dim E_{-3} = 2 = M_{-3}
\end{align*} 

ii. \begin{align*}
    A &= \begin{pmatrix}
    -3 & -1 & 1 \\
    -1 & -3 & 1 \\
    -2 & -2 & 0
    \end{pmatrix} \\
    \chi_A(t) &= \det (A - tI) \\
    &= - (t + 2)^3 \\
    \text{root } \lambda &= -2 \\
    M_{-2} &= 3 \\
    \text{To find eigenvectors:} \\
    (A + 2I)\underline{x} &= \begin{pmatrix}
    -1 & -1 & 1 \\
    -1 & -1 & 1 \\
    -2 & -2 & 2
    \end{pmatrix} \begin{pmatrix}x_1 \\x_2 \\x_3\end{pmatrix} = \underline{0} \\
    \implies \underline{x} &= \begin{pmatrix}- x_2 + x_3 \\ x_2\\ x_3\end{pmatrix} \\
    \implies E_{-2} &= \left\{ \alpha \begin{pmatrix}-1 \\1 \\0\end{pmatrix} + \beta \begin{pmatrix}1 \\0 \\1\end{pmatrix} \right\} \\
    m_{-2} &= \dim E_{-2} = 2 \\
    \text{but } M_{-2} &= 3
\end{align*} 
:::

### Linear Independence of Eigenvectors

::: {.proposition}

\

i. Let $\underline{v}_1, \ldots, \underline{v}_r$ be eigenvectors of matrix $A$ ($n \times n$) with eigenvalues $\lambda_1, \ldots, \lambda_r$.
If the eigenvalues are distinct, $\lambda_i \neq \lambda_j$ for $i \neq j$, then the eigenvectors are linearly independent.

ii. With conditions as in (i), let $\mathcal{B}_{\lambda_i}$ be a basis for $E_{\lambda_i}$, then 
\begin{align*}
    \mathcal{B}_{\lambda_1} \cup \mathcal{B}_{\lambda_2} \cup \ldots \cup \mathcal{B}_{\lambda_r}
\end{align*} is linearly independent.
:::

::: {.proof}

\

i. 
\begin{align*}
    \text{Note } \underline{w} &= \sum_{j=1}^{r} \alpha_j \underline{v}_j \\
    \implies (A - \lambda I) \underline{w} &= \sum_{j=1}^{r} \alpha_j (\lambda_j - \lambda) \underline{v}_j, \text{ as } A \underline{v}_j = \lambda_j \underline{v}_j \\
\end{align*} 

First, suppose the eigenvectors are linear dependent, so $\exists$ linear relations $\underline{w} = 0$ with a number of non-zero coefficients $p \geq 2$.
Pick a $\underline{w}$ for which $p$ is least and assume (wlog) $\alpha_1 \neq 0$.
Then $(A - \lambda_1 I) \underline{w} = \sum_{j>1} \alpha_j (\lambda_j - \lambda_1) \underline{v}_j = \underline{0}$ is a linear relation with $p - 1$ non-zero coefficients â¨³.

Or secondly, 
\begin{align*}
    \underline{w} &= \underline{0} \implies \Pi_{j \neq k} (A - \lambda_j I) \underline{w} \text{ for some chosen } k \\
    &= \alpha_k (\Pi_{j \neq k} (\lambda_k - \lambda_j)) \underline{v}_k = \underline{0},
    \implies \alpha_k &= 0
\end{align*} hence eigenvectors are linearly independent.

ii. It suffices to show that if $\underline{w} = \underline{w}_1 + \underline{w}_2 + \ldots + \underline{w}_r = \underline{0}$ with $\underline{w}_i \in E_{\lambda_i}$
\begin{align*}
    \implies \underline{w}_i = \underline{0}.
\end{align*} 
This follows by same arguments as in (i).
:::

## Diagonalisability and Similarity

### Introduction

::: {.proposition}
For a $n \times n$ matrix $A$ acting on $V = \mathbb{R}^n$ or $\mathbb{C}^n$, the following conditions are equivalent:

i. There exists a basis of eigenvectors for $V$, $\underline{v}_1, \underline{v}_2, \ldots, \underline{v}_n$ with $A \underline{v}_i = \lambda_i \underline{v}_i$ (not $\sum_i$).

ii. There exists an $n \times n$ invertible matrix $P$ with 
\begin{align*}
    P^{-1} A P = D = \begin{pmatrix}
    \lambda_1 &  &  \\
     & \ddots &  \\
     &  & \lambda_n
    \end{pmatrix}
\end{align*} 

If either of these conditions holds, $A$ is *diagonalisable*.
:::

::: {.proof}
Note that for any matrix $P$, $AP$ has columns $A \underline{C}_i(P)$ and $PD$ has columns $\lambda_i \underline{C}_i(P)$ for each $i$.
Then (i) and (ii) are related by 
\begin{align*}
    \underline{v}_i &= \underline{C}_i(P): \\
    P^{-1} A P &= D \iff AP = PD \iff A \underline{v}_i = \lambda_i \underline{v}_i.
\end{align*} 
:::