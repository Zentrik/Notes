# Eigenvalues and Eigenvectors

## Introduction

### Definitions

::: {.definition name="Eigenvector and eigenvalue"}
For a linear map $T : V \to V$ ($V$ a real vector or complex vector space) a vector $\underline{v} \in V$ with $\underline{v} \neq \underline{0}$ is an *eigenvector* of $T$ with eigenvalue $\lambda$ if \begin{align*}
    T(\underline{v}) = \lambda \underline{v}.
\end{align*} 
:::

If $V = \mathbb{R}^n$ or $\mathbb{C}^n$ and $T$ is given by an $n \times n$ matrix $A$ then
\begin{align*}
    A \underline{v} = \lambda \underline{v} \iff  (A - \lambda I) \underline{v} = \underline{0}
\end{align*} and for a given $\lambda$ this holds for some $\underline{v} \neq 0 \iff \det(A - \lambda I) = 0$, the *characteristic equation*, i.e. $\lambda$ is an eigenvalue iff it is a root of $\chi_A (t) = \det (A - tI)$, the *characteristic polynomial*.
$\chi_A (t)$ is a polynomial of degree $n$ for $A$ ($n \times n$).
We find eigenvalues as roots of the characteristic equation/ polynomial and then determine corresponding eigenvectors.

### Examples

::: {.example}
\begin{align*}
    V &= \mathbb{C}^2 \text{ and } A = \begin{pmatrix}
    2 & i \\
    -i & 2
    \end{pmatrix} \\
    \det (A - \lambda I) &= \begin{vmatrix}
    2 - \lambda & i \\
    -i & 2 - \lambda
    \end{vmatrix} \\
    &= (2 - \lambda)^2 - 1 \\
    &= 0 \iff \lambda = 1 \text{ or } 3.
\end{align*} 
To find eigenvectors $\underline{v} = \begin{pmatrix}v_1 \\v_2\end{pmatrix}$:

- $\lambda = 1$:
\begin{align*}
    (A - I) \underline{v} &= \begin{pmatrix}
        1 & i \\
        -i & 1
        \end{pmatrix} \begin{pmatrix}v_1 \\v_2\end{pmatrix} = \underline{0} \\
    \implies \underline{v} &= \alpha \begin{pmatrix}1 \\i\end{pmatrix} \text{ any } \alpha \neq 0 
\end{align*} 

- $\lambda = 3$:
\begin{align*}
    (A - 3I) \underline{v} &= \begin{pmatrix}
        -1 & i \\
        -i & -1
        \end{pmatrix} \begin{pmatrix}v_1 \\v_2\end{pmatrix} = \underline{0} \\
    \implies \underline{v} &= \beta \begin{pmatrix}1 \\ -i\end{pmatrix} \text{ any } \beta \neq 0 
\end{align*} 
:::

::: {.example}
\begin{align*}
    V &= \mathbb{R}^2 \text{ and } A = \begin{pmatrix}
    1 & 1 \\
    0 & 1
    \end{pmatrix} \\
    \det (A - \lambda I) &= \begin{vmatrix}
    1 - \lambda & 1 \\
    0 & 1 - \lambda
    \end{vmatrix} \\
    &= (1 - \lambda)^2\\
    &= 0 \iff \lambda = 1.
\end{align*} 
To find eigenvectors $\underline{v} = \begin{pmatrix}v_1 \\v_2\end{pmatrix}$:

\begin{align*}
    (A - I) \underline{v} &= \begin{pmatrix}
        0 & 1 \\
        0 & 0
        \end{pmatrix} \begin{pmatrix}v_1 \\v_2\end{pmatrix} = \underline{0} \\
    \implies \underline{v} &= \alpha \begin{pmatrix}1 \\ 0\end{pmatrix} \text{ any } \alpha \neq 0 
\end{align*} 
:::

::: {.example #sone}
\begin{align*}
    V &= \mathbb{R}^2 \text{ or } \mathbb{C}^2\\
    U &= \begin{pmatrix}
    \cos \theta & - \sin \theta \\
    \sin \theta & \cos \theta
    \end{pmatrix} \\
    \chi_U (t) &= \det (U - t I) \\
    &= t^2 - 2t \cos \theta + 1 \\
    \implies \lambda &= e^{\pm i \theta} \\
    \implies \underline{v} &= \theta \begin{pmatrix}1 \\ \mp i\end{pmatrix} \ (\alpha \neq 0 )
\end{align*} 
:::

### Deductions involving $\chi_A(t)$

For $A$ $n \times n$, characteristic polynomial has degree $n$
\begin{align*}
    \chi_A(t) &= \det \begin{pmatrix}
    A_{11} - t & A_{12} & \ldots & A_{1n} \\
    A_{21} & A_{22} - t & \ldots & A_{2n} \\
    \vdots & \vdots &  & \vdots \\
    A_{n1} & A_{n2} & \ldots & A_{nn} - t 
    \end{pmatrix} \\
    &= \sum_{j=0}^{n} c_j t^j, \text{ for some } c_j \in \mathbb{C} \\
    &= (-1)^n (t - \lambda_1) \ldots (t - \lambda_n)
\end{align*} 

i. $\exists$ at least one eigenvalue (one root of $\chi_A$); in fact $\exists$ $n$ roots counted with multiplicity (FTA).

ii. $\operatorname{tr}(A) = A_{ii} = \sum_i \lambda_i$, sum of eigenvalues, by comparing terms of order $n - 1$ in $t$. (???)

iii. $\det A = \chi_A(0) = \Pi_i \lambda_i$, product of eigenvalues.

iv. If $A$ is diagonal:
\begin{align*}
    A = \begin{pmatrix}
    \lambda_1 & & \\
    & \ddots & \\
    & & \lambda_n
    \end{pmatrix}
\end{align*} with the diagonal entries being the eigenvalues; (ii) and (iii) are then immediate.

v. If $A$ is real, then coefficients $c_j$ are real and $\chi_A(\lambda) = 0 \iff \chi_A(\overline{\lambda}) = 0$: non-real roots occur in conjugate pairs.

## Eigenspaces and Multiplicities

### Definitions

::: {.definition name="Eigenspace"}
For an eigenvalue $\lambda$ of matrix $A$, define the *eigenspace*
\begin{align*}
    E_\lambda = \{ \underline{v} : A \underline{v} = \lambda \underline{v}\} = \ker (A - \lambda I),
\end{align*} this is the subspace consisting of the eigenvectors and $\underline{0}$.
:::

::: {.definition name="Geometric multiplicity"}
The *geometric multiplicity*
\begin{align*}
    m_\lambda = \dim E_\lambda = \operatorname{null}(A - \lambda I),
\end{align*} the number of linearly independent eigenvectors with eigenvalue $\lambda$.
:::

::: {.definition name="Algebraic multiplicity"}
The *algebraic multiplicity* $M_\Lambda$ is the multiplicity of $\lambda$ as a root of $\chi_A$, i.e. $\chi_A(t) = (t-\lambda)^{M_\lambda} f(t)$ (with $f(\lambda) \neq 0$).
:::

::: {.proposition}
\begin{align*}
    M_\lambda \geq m_\lambda
\end{align*} 
Further discussion in 6.3.
:::

### Examples

:::{.example}

\

i. \begin{align*}
    A &= \begin{pmatrix}
    -2 & 2 & -3 \\
    2 & 1 & -6 \\
    -1 & -2 & 0
    \end{pmatrix} \\
    \chi_A(t) &= \det (A - tI) \\
    &= (5 - t) (t + 3)^2 \\
    \text{roots } \lambda &= 5, - 3 \\
    M_5 &= 1, M_{-3} = 2. \\ \\
    \underline{\lambda = 5:} \\
    (A - 5I)\underline{x} &= \begin{pmatrix}
    -7 & 2 & -3 \\
    2 & -4 & -6 \\
    -1 & -2 & -5
    \end{pmatrix}\begin{pmatrix}x_1 \\x_2 \\x_3\end{pmatrix} = \underline{0} \\
    \implies E_5 &= \left\{ \alpha \begin{pmatrix}1 \\2 \\-1\end{pmatrix} \right\} \\ \\
    \underline{\lambda = -3:} \\
    (A + 3I)\underline{x} &= \begin{pmatrix}
    1 & 2 & -3 \\
    2 & 4 & -6 \\
    -1 & -2 & 3
    \end{pmatrix}\begin{pmatrix}x_1 \\x_2 \\x_3\end{pmatrix} = \underline{0} \\
    \text{Solve to find} \\
    \underline{x} &= \begin{pmatrix}-2 x_1 + 3 x_3 \\x_2 \\x_3\end{pmatrix} \\
    \implies E_{-3} &= \left\{ \alpha \begin{pmatrix}-2 \\1 \\0\end{pmatrix} + \beta \begin{pmatrix}3 \\0 \\1\end{pmatrix} \right\} \\ \\
    m_5 &= \dim E_5 = 1 = M_5 \\
    m_{-3} &= \dim E_{-3} = 2 = M_{-3}
\end{align*} 

ii. \begin{align*}
    A &= \begin{pmatrix}
    -3 & -1 & 1 \\
    -1 & -3 & 1 \\
    -2 & -2 & 0
    \end{pmatrix} \\
    \chi_A(t) &= \det (A - tI) \\
    &= - (t + 2)^3 \\
    \text{root } \lambda &= -2 \\
    M_{-2} &= 3 \\
    \text{To find eigenvectors:} \\
    (A + 2I)\underline{x} &= \begin{pmatrix}
    -1 & -1 & 1 \\
    -1 & -1 & 1 \\
    -2 & -2 & 2
    \end{pmatrix} \begin{pmatrix}x_1 \\x_2 \\x_3\end{pmatrix} = \underline{0} \\
    \implies \underline{x} &= \begin{pmatrix}- x_2 + x_3 \\ x_2\\ x_3\end{pmatrix} \\
    \implies E_{-2} &= \left\{ \alpha \begin{pmatrix}-1 \\1 \\0\end{pmatrix} + \beta \begin{pmatrix}1 \\0 \\1\end{pmatrix} \right\} \\
    m_{-2} &= \dim E_{-2} = 2 \\
    \text{but } M_{-2} &= 3
\end{align*} 
:::

### Linear Independence of Eigenvectors

::: {.proposition #sone}

\

i. Let $\underline{v}_1, \ldots, \underline{v}_r$ be eigenvectors of matrix $A$ ($n \times n$) with eigenvalues $\lambda_1, \ldots, \lambda_r$.
If the eigenvalues are distinct, $\lambda_i \neq \lambda_j$ for $i \neq j$, then the eigenvectors are linearly independent.

ii. With conditions as in (i), let $\mathcal{B}_{\lambda_i}$ be a basis for $E_{\lambda_i}$, then 
\begin{align*}
    \mathcal{B}_{\lambda_1} \cup \mathcal{B}_{\lambda_2} \cup \ldots \cup \mathcal{B}_{\lambda_r}
\end{align*} is linearly independent.
:::

::: {.proof}

\

i. 
\begin{align*}
    \text{Note } \underline{w} &= \sum_{j=1}^{r} \alpha_j \underline{v}_j \\
    \implies (A - \lambda I) \underline{w} &= \sum_{j=1}^{r} \alpha_j (\lambda_j - \lambda) \underline{v}_j, \text{ as } A \underline{v}_j = \lambda_j \underline{v}_j \\
\end{align*} 

First, suppose the eigenvectors are linear dependent, so $\exists$ linear relations $\underline{w} = 0$ with a number of non-zero coefficients $p \geq 2$.
Pick a $\underline{w}$ for which $p$ is least and assume (wlog) $\alpha_1 \neq 0$.
Then $(A - \lambda_1 I) \underline{w} = \sum_{j>1} \alpha_j (\lambda_j - \lambda_1) \underline{v}_j = \underline{0}$ is a linear relation with $p - 1$ non-zero coefficients â¨³.

Or secondly, 
\begin{align*}
    \underline{w} &= \underline{0} \implies \Pi_{j \neq k} (A - \lambda_j I) \underline{w} \text{ for some chosen } k \\
    &= \alpha_k (\Pi_{j \neq k} (\lambda_k - \lambda_j)) \underline{v}_k = \underline{0},
    \implies \alpha_k &= 0
\end{align*} hence eigenvectors are linearly independent.

ii. It suffices to show that if $\underline{w} = \underline{w}_1 + \underline{w}_2 + \ldots + \underline{w}_r = \underline{0}$ with $\underline{w}_i \in E_{\lambda_i}$
\begin{align*}
    \implies \underline{w}_i = \underline{0}.
\end{align*} 
This follows by same arguments as in (i).
:::

## Diagonalisability and Similarity

### Introduction

::: {.proposition}
For a $n \times n$ matrix $A$ acting on $V = \mathbb{R}^n$ or $\mathbb{C}^n$, the following conditions are equivalent:

i. There exists a basis of eigenvectors for $V$, $\underline{v}_1, \underline{v}_2, \ldots, \underline{v}_n$ with $A \underline{v}_i = \lambda_i \underline{v}_i$ (not $\sum_i$ and $\lambda_i$ need not be distinct).

ii. There exists an $n \times n$ invertible matrix $P$ with 
\begin{align*}
    P^{-1} A P = D = \begin{pmatrix}
    \lambda_1 &  &  \\
     & \ddots &  \\
     &  & \lambda_n
    \end{pmatrix}
\end{align*} 

If either of these conditions holds, $A$ is *diagonalisable*.
:::

::: {.proof}
Note that for any matrix $P$, $AP$ has columns $A \underline{C}_i(P)$ and $PD$ has columns $\lambda_i \underline{C}_i(P)$ for each $i$.
Then (i) and (ii) are related by 
\begin{align*}
    \underline{v}_i &= \underline{C}_i(P): \\
    P^{-1} A P &= D \iff AP = PD \iff A \underline{v}_i = \lambda_i \underline{v}_i.
\end{align*} 
:::

::: {.example}
Refer to \@ref(exm:sone):
Eigenvalues $e^{\pm i \theta}$ and eigenvectors $\begin{pmatrix}1 \\ \mp i\end{pmatrix}$, the eigenvectors are linearly independent over $\mathbb{C}$.
\begin{align*}
    P &= \begin{pmatrix}
    1 & 1 \\
    -i & i
    \end{pmatrix} \implies P^{-1} = \frac{1}{2} \begin{pmatrix}
    1 & i \\
    1 & -i
    \end{pmatrix} \\
    P^{-1} U P &= \begin{pmatrix}
    e^{i\theta} & 0 \\
    0 & e^{-i \theta}
    \end{pmatrix}
\end{align*} 
$U$ is diagonalisable over $\mathbb{C}$ but not over $\mathbb{R}$.
:::

### Criteria for Diagonalisability 

::: {.theorem}
Let $A$ be a $n \times n$ matrix and $\lambda_1, \ldots, \lambda_r$ all its distinct eigenvalues.

i. A necessary and sufficient condition:
$A$ is diagonalisable iff 
\begin{align*}
    M_{\lambda_i} = m_{\lambda_i} \text{ for } i = 1, \ldots, r
\end{align*} 

ii. A sufficient condition:
$A$ is diagonalisable if there are $n$ distinct eigenvalues, i.e. $r = n$.
:::

::: {.proof}
Use Proposition \@ref(prp:sone)

ii. If $r = n$ we have $n$ distinct eigenvalues and hence $n$ linearly independent eigenvectors, which form a basis (for $\mathbb{R}^n$ or $\mathbb{C}^n$).

i. Choosing bases $\mathcal{B}_{\lambda_i}$ for each eigenspace,
\begin{align*}
    \mathcal{B}_{\lambda_1} \cup \mathcal{B}_{\lambda_2} \cup \ldots \cup \mathcal{B}_{\lambda_r}
\end{align*} is a linearly independent set of $m_{\lambda_1} + m_{\lambda_2} + \ldots + m_{\lambda_r}$ vectors.
It is a basis (for $\mathbb{R}^n$ or $\mathbb{C}^n$) iff we have $n$ vectors.
But
\begin{align*}
    m_{\lambda_i} &\leq M_{\lambda_i} \\
    \text{and } M_{\lambda_1} + M_{\lambda_2} + \ldots + M_{\lambda_r} &= n. \text{ (as there are n roots)}
\end{align*}
Hence we have a basis iff
\begin{align*}
    M_{\lambda_i} = m_{\lambda_i} \text{ for each } i.
\end{align*} 
:::

::: {.examples}
Refer to [Examples]
i. \begin{align*}
    A &= \begin{pmatrix}
    -2 & 2 & -3 \\
    2 & 1 & -6 \\
    -1 & -2 & 0
    \end{pmatrix} \\
    \lambda &= 5, - 3 \\
    m_5 &= 1 = M_5 \\
    m_{-3} &= 2 = M_{-3} \\
    \implies A &\text{ is diagonalisable} \\
    P &= \begin{pmatrix}
    1 & -2 & 3 \\
    2 & 1 & 0 \\
    -1 & 0 & 1
    \end{pmatrix}, P^{-1} = \frac{1}{8} \begin{pmatrix}
    1 & 2 & -3 \\
    -2 & 4 & 6 \\
    1 & 2 & 5
    \end{pmatrix} \\
    P^{-1} A P &= \begin{pmatrix}
    5 & 0 & 0 \\
    0 & -3 & 0 \\
    0 & 0 & -3
    \end{pmatrix} \text{ as expected}.
\end{align*} 

ii. \begin{align*}
    A &= \begin{pmatrix}
    -3 & -1 & 1 \\
    -1 & -3 & 1 \\
    -2 & -2 & 0
    \end{pmatrix} \\
    \lambda &= -2 \\
    M_{-2} &= 3 > m_{-2} = 2 \\
    \implies A &\text{ is not diagonalisable}
    \text{Check: if it was } P^{-1} A P &= - 2 I \\
    \implies A &= P (-2 I) P^{-1} = - 2I â¨³.
\end{align*} 
:::

### Similarity

Matrices $A$ and $B$ ($n \times n$) are *similar* if 
\begin{align*}
    B = P^{-1} A P
\end{align*} for some invertible $P$ ($n \times n$).
This is an equivalence relation.

::: {.proposition}
If $A$ and $B$ are similar, then

i. $B^r = P^{-1} A^r P$ for $r \geq 0$

ii. $B^{-1} = P^{-1} A^{-1} P$ (if either $A$ or $B$ is invertible, so is the other)

iii. $\operatorname{tr}(B) = \operatorname{tr}(A)$

iv. $\det B = \det A$

v. $\chi_B(t) = \chi_A(t)$
:::

::: {.proof} 

\

(i) is immediate.

ii. is immediate.

(iii) \begin{align*}
    \operatorname{tr}(B) &= \operatorname{tr}(P^{-1} A P) \\
    &= \operatorname{tr}(APP^{-1}) \\
    &= \operatorname{tr}(A).
\end{align*} 

(iv) \begin{align*}
    \det(B) &= \det(P^{-1} A P) \\
    &= \det(P^{-1}) \det(A) \det(P) \\
    &= \det A.
\end{align*} 

(v) \begin{align*}
    \det(B - tI) &= \det(P^{-1}AP - tI) \\
    &= \det(P^{-1} (A - tI) P) \\
    &= \det(A - tI) \text{ as in (iv)}.
\end{align*} 
:::
 
## Hermitian and Symmetric Matrices

### Real Eigenvalues and Orthogonal Eigenvectors

Recall: matrix $A$ ($n \times n$) is hermitian if
\begin{align*}
    A^\dagger = \overline{A}^T = A \text{ or } A_{ij} + \overline{A}_{ji}
\end{align*} 
special case: $A$ is real and symmetric
\begin{align*}
    \overline{A} = A \text{ and } A^T = A \text{ or } \begin{cases}
        A_{ij} &= \overline{A}_{ij} \\
        A_{ij} &= A_{ji}
    \end{cases}. 
\end{align*} 
Recall: complex inner-product for $\underline{v}, \underline{w} \in \mathbb{C}^n$ is
\begin{align*}
    \underline{v}^\dagger \underline{w} &= \sum_i \overline{v}_i w_i
\end{align*} and for $\underline{v}, \underline{w} \in \mathbb{R}^n$ this reduces to
\begin{align*}
    \underline{v}^T \underline{w} = \underline{v} \cdot \underline{w} = \sum_i v_i w_i
\end{align*} 
*Observation*: If $A$ is hermitian then
\begin{align*}
    (A \underline{v})^\dagger \underline{w} &= \underline{v}^\dagger (A \underline{w}) \ \forall \; \underline{v}, \underline{w} \in \mathbb{C}^n \\
\end{align*}

::: {.proof}
\begin{align*}
    (A \underline{v})^\dagger \underline{w} &= (\underline{v}^\dagger A^\dagger) \underline{w} \\
    &= \underline{v}^\dagger A \underline{w} \\
    &= \underline{v}^\dagger (A \underline{w})
\end{align*} 
:::

::: {.theorem}
For a matrix $A$ ($n \times n$) that is hermitian

i. Every eigenvalue $\lambda$ is real

ii. Eigenvectors $\underline{v}, \underline{w}$ with distinct eigenvalues $\lambda, \mu$ respectively ($\lambda \neq \mu$) are orthogonal
\begin{align*}
    \underline{v}^\dagger \underline{w} = 0
\end{align*}

iii. If $A$ is real and symmetric then for each $\lambda$ in (i) we can choose a real eigenvector $\underline{v}$ and (ii) becomes
\begin{align*}
    \underline{v}^T \underline{w} = \underline{v} \cdot \underline{w} = 0
\end{align*} 
:::

::: {.proof}

\

i. \begin{align*}
    \underline{v}^\dagger (A \underline{v}) &= (A \underline{v}^\dagger) \underline{v} \\
    \implies \underline{v}^\dagger (\lambda \underline{v}) &= (\lambda \underline{v})^\dagger \underline{v} \\
    \implies \lambda \underline{v}^\dagger \underline{v} &= \overline{\lambda} \underline{v}^\dagger \underline{v} \\
\end{align*} 
for $\underline{v}$ an eigenvector with eigenvalue $\lambda$.
But $\underline{v} \neq 0$ so $\underline{v}^\dagger \underline{v} \neq 0$ and so $\lambda = \overline{\lambda}$.

ii. \begin{align*}
    \underline{v}^\dagger (A \underline{w}) &= (A \underline{v})^\dagger \underline{w} \\
    \implies \underline{v}^\dagger (\mu \underline{w}) &= (\lambda \underline{v})^\dagger \underline{w} \\
    \implies \mu \underline{v}^\dagger \underline{w} &= \overline{\lambda} \underline{v}^\dagger \underline{w} \\
    &= \lambda \underline{v}^\dagger \underline{w} \text{ (from (i))}
\end{align*}  
But $\mu \neq \lambda$ so $\underline{v}^\dagger \underline{w} = 0$.

iii. Given $A \underline{v} = \lambda \underline{v}$ with $\underline{v} \in \mathbb{C}^n$ and $A, \lambda$ real, let $\underline{v} = \underline{u} + i \underline{u'}$ with $\underline{u}, \underline{u'} \in \mathbb{R}^n$.
Then $A \underline{u} = \lambda \underline{u}$ and $A \underline{u'} = \lambda \underline{u'}$ but $\underline{v} \neq \underline{0} \implies$ one of $\underline{u}, \underline{u'} \neq 0$ so there is at least one real eigenvector.
\

:::
