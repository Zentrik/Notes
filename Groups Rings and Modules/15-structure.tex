\section{The structure theorem and applications}
We will assume that $R$ is a Euclidean domain in this section, and let $\varphi$ be a Euclidean function for $R$.
\subsection{Row and column operations}

We will consider an $m \times n$ matrix with entries in $R$.

\begin{definition}[Elementary Row Operations]
	The \vocab{elementary row operations} on a matrix are
	\begin{enumerate}
		\item (ER1) Add $\lambda \in R$ times the $j$th row to the $i$th row, where $i \neq j$;
		\item (ER2) Swap the $i$th row and the $j$th row;
		\item (ER3) Multiply the $i$th row by $u \in R^\times$.
	\end{enumerate}
\end{definition}

\begin{remark}
	Each of these operations can be realised by left-multiplication by some $m \times m$ matrix.
	These operations are all invertible, so their matrices are all invertible.

	We can define elementary column operations in an analogous way (EC1-3), using right-multiplication by an $n \times n$ matrix instead.
\end{remark} 

\begin{definition}[Equivalent]
	Two $m \times n$ matrices $A, B$ are \vocab{equivalent} if there exists a sequence of elementary row and column operations that transforms one matrix into the other.
	If they are equivalent, then there exist invertible matrices $P, Q$ such that $B = QAP$.
\end{definition}

\begin{definition}[Minor]
	A $k \times k$ \vocab{minor} of an $m \times n$ matrix $A$ is the determinant of a $k \times k$ submatrix of $A$, which is a matrix of $A$ produced by removing $m-k$ rows and $n-k$ columns.
\end{definition}

\begin{definition}[Fitting Ideal]
	The $k$th \vocab{Fitting ideal} $\mathrm{Fit}_k(A) \triangleleft R$ is the ideal generated by the $k \times k$ minors of $A$.
\end{definition} 

\begin{lemma} \label{eq:16.2}
	The $k$th fitting ideal of a matrix is invariant under elementary row and column operations, i.e. for equivalent $A, B$ $\mathrm{Fit}_k(A) = \mathrm{Fit}_k(B) \; \forall \; k$.
\end{lemma}

\begin{proof}
	It suffices by symmetry to show that the elementary row operations do not change the Fitting ideal.

	For the first elementary row operation, ER1, on a matrix $A$, suppose we add $\lambda \in R$ multiplied by the $j$th row to the $i$th row, yielding a matrix $A'$.
	In particular, $a_{ik} \mapsto a_{ik} + \lambda a_{jk}$ for all $k$.
	Let $C$ be a $k \times k$ submatrix of $A$ and $C'$ the corresponding submatrix of $A'$.

	If row $i$ was not chosen in $C$, then $C$ and $C'$ are the same matrix.
	Hence $\det C = \det C'$.

	If row $i$ and row $j$ were both chosen in $C$, we have that $C, C'$ differ by a row operation.
	Since the determinant is invariant under this elementary row operations thus $\det C = \det C'$.

	If row $i$ was chosen but row $j$ was not chosen, by expanding the determinant along the $i$th row, we find
	\begin{align*}
		\det C' = \det C \pm \lambda \det D\footnote{Expanding the det along the $i$th row is $(a_{i 1} + \lambda a_{j 1}) \det E - (a_{i 2} + \lambda a_{j 2}) \det F + \dots \pm (a_{i n} + \lambda a_{j n}) \det Z = (a_{i 1} \det E - a_{i 2} \det F + \dots \pm a_{i n} \det Z) + \lambda (a_{j 1} \det E - a_{j 2} \det F + \dots \pm a_{j n} \det Z) = \det C \pm \lambda \det D$.}
	\end{align*}
	where we can show that $D$ is a $k \times k$ submatrix of $A$ that includes row $j$ but not row $i$.
	By definition, $\det D \in \mathrm{Fit}_k(A)$ and $\det C \in \mathrm{Fit}_k(A)$, so certainly $\det C' \in \mathrm{Fit}_k(A)$.
	Hence $\mathrm{Fit}_k(A') \subseteq \mathrm{Fit}_k(A)$.
	By the invertibility of the elementary row operations, $\mathrm{Fit}_k(A') \supseteq \mathrm{Fit}_k(A)$.

	The proofs for the other elementary row operations are left as an exercise.
\end{proof}

\subsection{Smith normal form}
\begin{theorem}[Smith Normal Form]
	An $m \times n$ matrix $A = (a_{ij})$ over a Euclidean domain $R$ is equivalent to a matrix of the form, with $d_i \neq 0$
	\begin{align*}
		\begin{pmatrix}
			d_1                            \\
			 & \ddots                      \\
			 &        & d_t                \\
			 &        &     & 0            \\
			 &        &     &   & \ddots   \\
			 &        &     &   &        &
		\end{pmatrix};\quad d_1 \mid d_2 \mid \dots \mid d_t
	\end{align*}
	The $d_i$ are known as \textit{invariant factors}, and they are unique up to associates.
\end{theorem}

\begin{proof}
	If $A = 0$, the matrix is already in Smith normal form.
	Otherwise, we can swap columns and rows such that $a_{11} \neq 0$.
	We will reduce $\varphi(a_{11})$ as much as possible until it divides every other element in the matrix, using the following algorithm.

	\underline{STEP 1}: If $a_{11} \nmid a_{1j}$ for some $j \geq 2$, then $a_{1j} = q a_{11} + r$ where $q, r \in R$ and $\varphi(r) < \varphi(a_{11})$.
	We can subtract $q$ multiplied by column 1 from column $j$.
	Swapping these columns leaves $a_{11} = r$.

	\underline{STEP 2}: If $a_{11} \nmid a_{i1}$ for some $i \geq 2$, then repeat the above process using row operations.

	STEP 1 and 2 decrease $\phi(a_{11})$, so we repeat until $a_{11} \mid a_{1j}, a_{i1}$ for all $j \geq 2, i \geq 2$.
	We only need to repeat finitely many times as the Euclidean function takes values in $\mathbb Z_{\geq 0}$ and $\varphi(a_{11})$ strictly decreases in each iteration.
	Now, $a_{11} \mid a_{ij}$ for all $i,j$.

	Now, we can subtract multiples of the first row to clear out the first column and multiples of the first column to clear out the first row to give
	\begin{align*}
		A = \begin{pmatrix}
			a_{11} & 0 & \cdots & 0 \\
			0                       \\
			\vdots &   & A'         \\
			0
		\end{pmatrix}
	\end{align*}
	\underline{STEP 3}: If $a_{11} \nmid a_{ij}$ for $i,j \geq 2$, then add the $i$th row to the first row.
	There is now an element in the first row, $a_{ij}$, that $a_{11}$ does not divide.
	We can then perform column operations as in step 1 to decrease $\varphi(a_{11})$.

	We will then restart the algorithm.
	After finitely many steps, this algorithm will terminate and $a_{11}$ will divide all elements $a_{ij}$ of the matrix.
	\begin{align*}
		A = \begin{pmatrix}
			a_{11} & 0 & \cdots & 0 \\
			0                       \\
			\vdots &   & A'         \\
			0
		\end{pmatrix};\quad a_{11} \equiv d_1 \mid a_{ij}
	\end{align*}
	We can now apply the algorithm to $A'$, since column and row operations not including the first row or column do not change whether $a_{11} \mid a_{ij}$.

	We now demonstrate uniqueness of the invariant factors.
	Suppose $A$ has Smith normal form with invariant factors $d_i$ where $d_1 \mid \dots \mid d_t$.
	Then, for all $k$, $\mathrm{Fit}_k(A)$ can be evaluated in Smith normal form by invariance of the Fitting ideal under row and column operations.
	Hence $\mathrm{Fit}_k(A) = (d_1 d_2 \cdots d_k) \triangleleft R$.
	Thus, the product $d_1 \cdots d_k$ depends only on $A$, and is unique up to associates.
	Cancelling, we can see that each $d_i$ depends only on $A$, up to associates.
\end{proof}

\begin{example}
	Consider the matrix over $\mathbb Z$ given by
	\begin{align*}
		A = \begin{pmatrix}
			2 & -1 \\
			1 & 2
		\end{pmatrix}
	\end{align*}
	Using elementary row and column operations,
	\begin{align*}
		\begin{pmatrix}
			2 & -1 \\
			1 & 2
		\end{pmatrix} \xrightarrow{c_1 \mapsto c_1 + c_2} \begin{pmatrix}
			1 & -1 \\
			3 & 2
		\end{pmatrix} \xrightarrow{c_2 \mapsto c_1 + c_2} \begin{pmatrix}
			1 & 0 \\
			3 & 5
		\end{pmatrix} \xrightarrow{r_2 \mapsto -3r_1 + r_2} \begin{pmatrix}
			1 & 0 \\
			0 & 5
		\end{pmatrix}
	\end{align*}
	This is in Smith normal form as $1 \mid 5$.

	Alternatively, $(d_1)$ is all the $1 \times 1$ minors, i.e. $(2, -1, 1, 2) = (1)$.
	So $d_1 = \pm 1$.
	Further, $(d_1 d_2) = (\det A) = (5)$.
	So $d_1 d_2 = \pm 5$ and hence $d_2 = \pm 5$.
\end{example}

\subsection{The structure theorem}
\begin{lemma} \label{lem:16.3}
	Let $R$ be a Euclidean domain with Euclidean function $\varphi$ (or, indeed, a principal ideal domain).
	Any submodule of the free module $R^m$ is generated by at most $m$ elements.
\end{lemma}

\begin{proof}
	The $m=1$ case was \label{lem:14.4}

	Let $N \leq R^m$.
	Consider
	\begin{align*}
		I = \qty{r \in R : \exists \; r_2, \dots, r_m \in R,\, (r,r_2, \dots, r_m) \in N}
	\end{align*}
	Since $N$ is a submodule, this is an ideal.
	Since $R$ is a principal ideal domain, $I = (a)$ for some $a \in R$.
	Let $n = (a, a_2, \dots, a_m) \in N$.
	For $(r_1, \dots, r_m) \in N$, we have $r_1 = ra$ for some $r$.
	Hence $(r_1, \dots, r_m) - rn = (0,r_2 - ra_2, \dots, r_m - ra_m)$, which lies in $N' = N \cap \qty(\qty{0} \times R^{m-1}) \leq R^{m-1}$, hence $N = Rn + N'$.
	By induction, $N'$ is generated by $n_2, \dots, n_m$, hence $\{n, n_2, \dots, n_m\}$ generate $N$.
\end{proof}

\begin{theorem} \label{thm:16.4}
	Let $R$ be a Euclidean domain, and $N \leq R^m$.
	Then there is a free basis $x_1, \dots, x_m$ for $R^m$ such that $N$ is generated by $d_1 x_1, \dots, d_t x_t$ for some $d_i \in R$ and $t \leq m$, and such that $d_1 \mid \dots \mid d_t$.
\end{theorem}

\begin{proof}
	By \Cref{lem:16.3}, we have $N = R y_1 + \dots + R y_n$ for some $y_i \in R^m$ for some $n \leq m$.
	Each $y_i$ belongs to $R^m$ so we can form the $m \times n$ matrix $A$ which has columns $y_i$.
	$A$ is equivalent to a matrix $A'$ in Smith normal form with invariant factors $d_1 \mid \dots \mid d_t$.

	$A'$ is obtained from $A$ by elementary row and column operations.
	Switching row $i$ and row $j$ in $A$ corresponds to reassigning the standard basis elements $e_i$ and $e_j$ to each other.
	Adding a multiple of row $i$ to row $j$ corresponds to replacing $e_1, \dots, e_m$ with a linear combination of these basis elements which is a free basis.
	In general, each row operation simply changes the choice of free basis used for $R^m$.
	Analogously, each column operation changes the set of generators $y_i$ for $N$.

	Hence, after applying these row and column operations, the free basis $e_i$ of $R^m$ is converted into $x_1, \dots, x_m$, and $N$ is generated by $d_1 x_1, \dots, d_t x_t$.
\end{proof}

\begin{theorem}[Structure Theorem for finitely generated modules over Euclidean domains]
	Let $R$ be a Euclidean domain, and $M$ a finitely generated module over $R$.
	Then
	\begin{align*}
		M \cong \faktor{R}{(d_1)} \oplus \dots \oplus \faktor{R}{(d_t)} \oplus \underbrace{R \oplus \dots \oplus R}_{k \text{ copies}} \cong \faktor{R}{(d_1)} \oplus \dots \oplus \faktor{R}{(d_t)} \oplus R^k
	\end{align*}
	for some $0 \neq d_i \in R$ and $d_1 \mid \dots \mid d_t$, and where $k \geq 0$.
	The $d_i$ are called invariant factors.
\end{theorem}

\begin{proof}
	Since $M$ is finitely generated, there exists a surjective $R$-module homomorphism $\varphi : R^m \to M$ for some $m$ by \Cref{lem:14.1}.
	By the first isomorphism theorem, $M \cong \faktor{R^m}{\ker \varphi}$.
	By \Cref{thm:16.4}, there exists a free basis $x_1, \dots, x_m$ for $R^m$ such that $\ker \varphi \leq R^m$ is generated by $d_1 x_1, \dots, d_t x_t$ where $d_1 \mid \dots \mid d_t$.
	Then,
	\begin{align*}
		M &\cong \frac{\underbrace{R \oplus \dots R}_{m \text{ copies}}}{d_1 R \oplus \dots \oplus d_t R \oplus \underbrace{0 \oplus \dots \oplus 0}_{m-t \text{ copies}}} \\
		&\cong \faktor{R}{(d_1)} \oplus \dots \oplus \faktor{R}{(d_t)} \oplus \underbrace{R \oplus \dots \oplus R}_{m-t \text{ copies}} \text{ by \Cref{lem:15.1}}
	\end{align*}
\end{proof}

\begin{remark}
	After deleting those $d_i$ which are units, the invariant factors of $M$ are unique up to associates.
	The proof is omitted.
\end{remark}

\begin{corollary} \label{cor:16.5}
	Let $R$ be a Euclidean domain.
	Then any finitely generated torsion-free module is free.
\end{corollary}

\begin{proof}
	Since $M$ is torsion-free, there are no submodules of the form $\faktor{R}{(d)}$ with $d$ nonzero, since then multiplying an element of $M$ by $d$ would give zero.
	Hence, by the structure theorem, $M \cong R^m$ for some $m$.
\end{proof}

\begin{example}
	Consider $R = \mathbb Z$, and the abelian group $G = \genset{a,b}$ subject to the relations $2a + b = 0$ and $-a + 2b = 0$, so $G \cong \faktor{\mathbb Z^2}{N}$ where $N$ is the $\mathbb Z$-submodule of $\mathbb Z^2$ generated by $(2,1)$ and $(-1,2)$.
	Consider
	\begin{align*}
		A = \begin{pmatrix}
			2 & -1 \\
			1 & 2
		\end{pmatrix}
	\end{align*}
	which has Smith normal form $d_1 = 1$ and $d_2 = 5$.
	Hence, by changing basis for $\mathbb Z^2$, we can let $N$ be generated by $(1,0)$ and $(0,5)$.
	Hence,
	\begin{align*}
		G \cong \faktor{\mathbb Z \oplus \mathbb Z}{\mathbb Z \oplus 5 \mathbb Z} \cong \faktor{\mathbb Z}{5\mathbb Z}
	\end{align*}
\end{example}

\subsection{Primary decomposition theorem}
More generally, applying the structure theorem to $\mathbb Z$-modules, we obtain the structure theorem for finitely generated abelian groups:
\begin{theorem}[Structure Theorem for finitely generated abelian groups]
	Let $G$ be a finitely generated abelian group.
	Then
	\begin{align*}
		G \cong C_{d_1} \times \dots \times C_{d_t} \times \mathbb Z^r
	\end{align*}
	where $d_1 \mid \dots \mid d_t$ in $\mathbb Z$, and $r \geq 0$.
\end{theorem}

\begin{proof}
	Take $R = \mathbb{Z}$ in structure theorem for modules. 
	We have replaced the submodule notation $\faktor{\mathbb Z}{n\mathbb Z}$ and $\oplus$ with the group notation $C_n$ and $\times$.
\end{proof} 

\begin{remark}
	The special case of $G$ finite means $r = 0$ and was quoted as \Cref{thm:6.4}.
\end{remark} 

We have also seen that any finite abelian group can be written as a product of cyclic groups of prime power order.
This also has a generalisation for modules.
The previous result relied on the lemma $C_{mn} \cong C_m \times C_n$ where $m$ and $n$ are coprime.
There is an analogous result for principal ideal domains.

\begin{lemma} \label{lem:16.6}
	Let $R$ be a principal ideal domain, and $a, b \in R$ with $\operatorname{gcd} = 1$.
	Then, treating these quotients as $R$-modules,
	\begin{align*}
		\faktor{R}{(ab)} \cong \faktor{R}{(a)} \oplus \faktor{R}{(b)}
	\end{align*}
\end{lemma}

\begin{note}
	The case of $R = \mathbb{Z}$ was \Cref{lem:6.2}.
\end{note} 

\begin{proof}
	Since $R$ is a principal ideal domain, $(a,b) = (d)$ for some $d \in R$.
	The greatest common divisor of $a, b$ is a unit, so $d$ is a unit, giving $(a,b) = R$.
	Hence, there exist $r,s \in R$ such that $ra + sb = 1$.
	This is a generalisation of B\'ezout's theorem.

	Now, we define an $R$-module homomorphism $\psi : R \to \faktor{R}{(a)} \oplus \faktor{R}{(b)}$ by $\psi(x) = (x+(a), x+(b))$.
	Then $\psi(sb) = (sb+(a), sb+(b)) = (1-ra+(a),sb+(b)) = (1+(a), (b))$, and similarly $\psi(ra) = ((a),1+(b))$.
	Hence, $\psi(sbx + rby) = (x+(a),y+(b))$ so $\psi$ is surjective.

	Clearly we have $(ab) \subset \ker \psi$, so it suffices to show the converse.
	If $x \in \ker \psi$, then $x \in (a)$ and $x \in (b)$, so $x \in (a) \cap (b)$.
	Since $x = x(ra+sb) = r(ax) + s(bx)$, we must have that $s(bx) \in (a)$ and $r(ax) \in (b)$, so $x \in (ab)$.
	Hence $\ker \psi = (ab)$, and the result follows from the first isomorphism theorem for modules.
\end{proof}

\begin{lemma}[Primary Decomposition Theorem]
	Let $R$ be a Euclidean domain and $M$ a finitely generated $R$-module.
	Then
	\begin{align*}
		M \cong \faktor{R}{\qty(p_1^{n_1})} \oplus \dots \oplus \faktor{R}{\qty(p_k^{n_k})} \oplus R^m
	\end{align*}
	where the quotients are considered as $R$-modules, where $p_i$ are primes in $R$, which are not necessarily distinct, and where $m \geq 0$.
\end{lemma}

\begin{proof}
	By the structure theorem,
	\begin{align*}
		M \cong \faktor{R}{(d_1)} \oplus \dots \oplus \faktor{R}{(d_t)} \oplus \underbrace{R \oplus \dots \oplus R}_{m \text{ copies}} \cong \faktor{R}{(d_1)} \oplus \dots \oplus \faktor{R}{(d_t)} \oplus R^m
	\end{align*}
	where $d_1 \mid \dots \mid d_t$.
	So it suffices to show that each $\faktor{R}{(d_i)}$ can be written as a product of factors of the form $\faktor{R}{(p_j^{n_j})}$.
	Since $R$ is a unique factorisation domain and a principal ideal domain, $d_i$ can be written as a product $u p_1^{\alpha_1} \cdots p_r^{\alpha_r}$ where $u$ is a unit and the $p_j$ are pairwise non-associate primes.
	By \Cref{lem:16.6},
	\begin{align*}
		\faktor{R}{(d_i)} \cong \faktor{R}{(p_1^{\alpha_1})} \oplus \dots \faktor{R}{(p_r^{\alpha_r})}
	\end{align*}
\end{proof}

\subsection{Rational canonical form}
Let $V$ be a vector space over a field $F$, and $\alpha : V \to V$ be a linear map.
Let $V_\alpha$ denote the $F[X]$-module $V$ where scalar multiplication, $F[X] \times V \to V$, is defined by $(f(X), v) \mapsto f(X) \cdot v = f(\alpha)(v)$.

\begin{lemma} \label{lem:16.7}
	If $V$ is finite-dimensional as a vector space, then $V_\alpha$ is finitely generated as an $F[X]$-module.
\end{lemma}

\begin{proof}
	Consider a basis $v_1, \dots, v_n$ of $V$, so $v_1, \dots, v_n$ generate $V$ as an $F$-vector space.
	Then, these vectors generate $V_\alpha$ as an $F[X]$-module, since $F \leq F[X]$.
\end{proof}

\begin{example}
	Suppose $V_\alpha \cong \faktor{F[X]}{(X^n)}$ as an $F[X]$-module.
	Then, $1, X, X^2, \dots, X^{n-1}$ is a basis for $\faktor{F[X]}{(X^n)}$ as an $F$-vector space.
	With respect to this basis, $\alpha$ has the matrix form
	\begin{equation}
		\begin{pmatrix}
			0      & 0      & 0      & \cdots & 0      & 0      \\
			1      & 0      & 0      & \cdots & 0      & 0      \\
			0      & 1      & 0      & \cdots & 0      & 0      \\
			0      & 0      & 1      & \cdots & 0      & 0      \\
			\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
			0      & 0      & 0      & \cdots & 1      & 0
		\end{pmatrix}
		\tag{\(\ast\)}
	\end{equation}
\end{example}

\begin{example}
	Suppose $V_\alpha \cong \faktor{F[X]}{(X-\lambda)^n}$ as an $F[X]$-module.
	Consider the basis $1, X-\lambda, (X-\lambda)^2, \dots, (X-\lambda)^{n-1}$ for $\faktor{F[X]}{(X-\lambda)^n}$ as an $F$-vector space.
	Here, $\alpha - \lambda \id$ has matrix (\(\ast\)) from the previous example.
	Hence, $\alpha$ has matrix $(\ast) + \lambda I$.
\end{example}

\begin{example}
	Suppose $V_\alpha \cong \faktor{F[X]}{(f)}$ where $f \in F[X]$ as an $F[X]$-module, such that $f$ is monic.
	Let
	\begin{align*}
		f(X) = X^n + a_{n-1} X^{n-1} + \dots + a_0
	\end{align*}
	With respect to basis $1, X, \dots, X^{n-1}$, $\alpha$ has matrix
	\begin{align*}
		C(f) =
		\begin{pmatrix}
			0      & 0      & 0      & \cdots & 0      & -a_0     \\
			1      & 0      & 0      & \cdots & 0      & -a_1     \\
			0      & 1      & 0      & \cdots & 0      & -a_2     \\
			0      & 0      & 1      & \cdots & 0      & -a_3     \\
			\vdots & \vdots & \vdots & \ddots & \vdots & \vdots   \\
			0      & 0      & 0      & \cdots & 1      & -a_{n-1}
		\end{pmatrix}
	\end{align*}
	since $f$ is monic and the last column represents $X^n$.
	The above matrix is known as the \textit{companion matrix} of the monic polynomial.
\end{example}

\begin{theorem}[Rational canonical form]
	Let $F$ be a field, $V$ be a finite-dimensional $F$-vector space, and $\alpha : V \to V$ be a linear map.
	Then the $F[X]$-module $V_\alpha$ decomposes as
	\begin{align*}
		V_\alpha \cong \faktor{F[X]}{(f_1)} \oplus \dots \oplus \faktor{F[X]}{(f_t)}
	\end{align*}
	for some monic polynomials $f_i \in F[X]$, and $f_1 \mid \dots \mid f_t$.
	Moreover, with respect to a suitable basis, $\alpha$ has matrix
	\begin{equation}
		\begin{pmatrix}
			C(f_1)                      \\
			 & C(f_2)                   \\
			 &        & \ddots          \\
			 &        &        & C(f_t)
		\end{pmatrix}
		\tag{\(\ast\ast\)}
	\end{equation}
\end{theorem}

\begin{proof}
	We know that $V_\alpha$ is finitely generated as an $F[X]$-module, since $V$ is finite-dimensional by \Cref{lem:16.7}.
	Since $F[X]$ is a Euclidean domain, the structure theorem applies, and
	\begin{align*}
		V_\alpha \cong \faktor{F[X]}{(f_1)} \oplus \dots \oplus \faktor{F[X]}{(f_t)} \oplus F[X]^m
	\end{align*}
	for some $m$, where $f_1 \mid \dots \mid f_t$.
	Since $V$ is finite-dimensional as an $F$ vector space, $m = 0$ as $F[X]$ is infinite-dimensional.
	As $F$ is a field, wlog we may multiply each $f_i$ by a unit to ensure that they are monic.

	Then, using the previous example, we can construct the companion matrices for each polynomial and obtain the matrix as required.
\end{proof}

\begin{remark}
	\begin{enumerate}
		\item If $\alpha$ is represented by an $n \times n$ matrix $A$, there exists a change of basis matrix $P$ such that $PAP\inv$ has form ($\ast\ast$) as stated in the theorem.
		Any square matrix over a field is similar to ($\ast\ast$).
		\item Note further that (\(\ast\ast\)) can be used to find the minimal and characteristic polynomials of $\alpha$; the minimal polynomial is $f_t$ as if $f_i \mid f_j$ then $f_j = 0 \implies f_i = 0$.
		So $f_t = 0 \implies f_1 = f_2 = \dots = f_{t - 1} = 0$.
		\item The characteristic polynomial is $f_1 \cdots f_t$.
		\item In particular, the minimal polynomial divides the characteristic polynomial, and this implies the Cayley-Hamilton theorem.
	\end{enumerate} 
\end{remark}

\begin{example}
	Consider $\dim V = 2$.
	Then, $\sum \deg f_i = 2$, so there are two cases: one polynomial of degree two, or two polynomials of degree one.
	Consider $V_\alpha \cong \faktor{F[X]}{(X-\lambda)} \oplus \faktor{F[X]}{(X-\mu)}$.
	Since one of the $f_i$ must divide the other, we have $\lambda = \mu$.
	If we have one polynomial of degree two, we have $V_\alpha \cong \faktor{F[X]}{(f)}$, where $f$ is the characteristic polynomial of $\alpha$.
\end{example}

\begin{corollary} \label{cor:16.9}
	Let $A, B$ be invertible $2 \times 2$ non-scalar matrices over a field $F$.
	Then $A, B$ are similar iff their characteristic polynomials are equal.
\end{corollary}

\begin{proof}
	$(\implies)$: If $A, B$ are similar they have the same characteristic polynomial, which is proven in Part IB Linear Algebra.

	$(\Longleftarrow)$: If the matrices are non-scalar, the modules $V_\alpha, V_\beta$ are of the form $\faktor{F[X]}{(f)}$ by the previous example, so they are both similar to the companion matrix of $f$, where $f$ is the characteristic polynomial of $A$ or $B$.
\end{proof}

\begin{definition}[Annihilator]
	The \vocab{annihilator} of an $R$-module $M$ is
	\begin{align*}
		\mathrm{Ann}_R(M) = \qty{r \in R : \forall m \in M,\, rm = 0} \triangleleft R
	\end{align*}
\end{definition}

\begin{example}
	Let $I \triangleleft R$.
	Then the annihilator of $\faktor{R}{I}$ is $\mathrm{Ann}_R\qty(\faktor{R}{I}) = I$.
\end{example} 

\begin{example}
	Let $A$ be a finite abelian group.
	Then, considering $A$ as a $\mathbb Z$-module, $\mathrm{Ann}_{\mathbb Z}(A) = (e)$ where $e$ is the \textit{exponent} of the group, which is the lowest common multiple of the orders of elements in the group.
\end{example}

\begin{example}
	Let $V_\alpha$ be as above.
	Then $\mathrm{Ann}_{F[X]}(V_\alpha) = (f)$ where $f$ is the minimal polynomial of $\alpha$.
\end{example} 

\subsection{Jordan normal form}
Jordan normal form concerns matrix similarity in $\mathbb C$.
The following results are therefore restricted to this particular field.

\begin{lemma} \label{lem:16.10}
	The primes (or equivalently, irreducibles) in $\mathbb C[X]$ are the polynomials $X - \lambda$ for $\lambda \in \mathbb C$, up to associates.
\end{lemma}

\begin{proof}
	By the fundamental theorem of algebra, any non-constant polynomial with complex coefficients has a complex root.
	By the Euclidean algorithm, we can show that having a root $\lambda$ is equivalent to having a linear factor $X - \lambda$.
	Hence the irreducibles have degree one, and thus are $X - \lambda$ exactly, up to associates.
\end{proof}

\begin{theorem}[Jordan Normal Form] \label{thm:jnf}
	Let $\alpha : V \to V$ be an endomorphism of a finite-dimensional $\mathbb C$-vector space $V$.
	Let $V_\alpha$ be the set $V$ as a $\mathbb C[X]$-module, where scalar multiplication is defined by $f\cdot v = f(\alpha)(v)$.
	Then, there exists an isomorphism of $\mathbb C[X]$-modules
	\begin{align*}
		V_\alpha \cong \faktor{\mathbb C[X]}{\qty((X-\lambda_1)^{n_1})} \oplus \dots \oplus \faktor{\mathbb C[X]}{\qty((X-\lambda_t)^{n_t})}
	\end{align*}
	where $\lambda_i \in \mathbb C$ are not necessarily distinct.
	In particular, there exists a basis for this vector space such that $\alpha$ has matrix in block diagonal form
	\begin{align*}
		\begin{pmatrix}
			J_{n_1}(\lambda_1)                                  \\
			 & J_{n_2}(\lambda_2)                               \\
			 &                    & \ddots                      \\
			 &                    &        & J_{n_t}(\lambda_t)
		\end{pmatrix}
	\end{align*}
	where each \textit{Jordan block} $J_{n_i}(\lambda_i)$ is an $n_i \times n_i$ matrix of the form
	\begin{align*}
		J_{n_i}(\lambda_i) = \begin{pmatrix}
			\lambda_i & 0         & 0         & \cdots & 0         \\
			1         & \lambda_i & 0         & \cdots & 0         \\
			0         & 1         & \lambda_i & \cdots & 0         \\
			\vdots    & \vdots    & \vdots    & \ddots & \vdots    \\
			0         & 0         & 0         & \cdots & \lambda_i
		\end{pmatrix}
	\end{align*}
\end{theorem}

\begin{proof}
	Note $\mathbb C[X]$ is a Euclidean domain using the degree function, and $V_\alpha$ is finitely generated as a $\mathbb C[X]$-module by \Cref{lem:16.7}.
	These are the assumptions of the primary decomposition theorem so we can apply it, finding the module decomposition as required, noting that the primes in $\mathbb C[X]$ are the linear polynomials as in \Cref{lem:16.10}.
	Note that the free factor $\mathbb C[X]$ cannot appear in the decomposition since $V$ is finite-dimensional.

	We have already seen that for a module $W_\alpha \cong \faktor{F[X]}{\qty((X-\lambda)^n)}$, multiplication by $X$ is represented by the matrix $J_n(\lambda)$ with respect to the basis $1, (X-\lambda), \dots, (X-\lambda)^{n-1}$.
	Hence the result follows by considering the union of these bases.
\end{proof}

\begin{remark}
	\begin{enumerate}
		\item If $\alpha$ is represented by a matrix $A$, then $A$ is similar to a matrix in Jordan normal form.
		This is the form of the result often used in linear algebra.
		\item The Jordan blocks are uniquely determined up to reordering.
		This can be proven by considering the dimensions of the \textit{generalised eigenspaces}, which are $\ker\qty((\alpha - \lambda \id)^m)$ for some $m \in \mathbb N$.
		\item The minimal polynomial of $\alpha$ is $\prod_{\lambda} (X-\lambda)^{c_\lambda}$ where $c_\lambda$ is the size of the largest $\lambda$-block.
		\item The characteristic polynomial of $\alpha$ is $\prod_{\lambda} (X-\lambda)^{a_\lambda}$ where $a_\lambda$ is the sum of the sizes of the $\lambda$-blocks.
		\item The number of $\lambda$-blocks is the dimension of the eigenspace of $\lambda$.
	\end{enumerate} 
\end{remark}

\subsection{Modules over principal ideal domains (non-examinable)}
The structure theorem above was proven for Euclidean domains.
This also holds for principal ideal domains.
Some of the ideas relevant to this proof are illustrated in this subsection.

\begin{theorem} \label{thm:17.1}
	Let $R$ be a principal ideal domain.
	Then any finitely generated torsion-free $R$-module is free.
\end{theorem}

If $R$ is a Euclidean domain, this was proven as a corollary to the structure theorem, \Cref{cor:16.5}.

\begin{lemma} \label{lem:17.2}
	Let $R$ be a principal ideal domain and $M$ be an $R$-module.
	Let $r_1, r_2 \in R$ be not both zero, and let $d$ be their greatest common divisor.
	Then,
	\begin{enumerate}
		\item there exists $A \in SL_2(R)$ such that
		      \begin{align*}
			      A \begin{pmatrix}
				      r_1 \\
				      r_2
			      \end{pmatrix} = \begin{pmatrix}
				      d \\
				      0
			      \end{pmatrix}
		      \end{align*}
		\item if $x_1, x_2 \in M$, then there exist $x_1', x_2' \in M$ such that $Rx_1 + Rx_2 = Rx_1' + Rx_2'$, and $r_1 x_1 + r_2 x_2  = d x_1' + 0 \cdot x_2'$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Since $R$ is a principal ideal domain, $(r_1, r_2) = (d)$.
	Hence, by definition, $d = \alpha r_1 + \beta r_2$ for some $\alpha, \beta \in R$.
	Let $r_1 = s_1 d$ and $r_2 = s_2 d$.
	Then $\alpha s_1 + \beta s_2 = 1$.
	Now, let
	\begin{align*}
		A = \begin{pmatrix}
			\alpha & \beta \\
			-s_2   & s_1
		\end{pmatrix} \implies \det A = 1;\quad A \begin{pmatrix}
			r_1 \\
			r_2
		\end{pmatrix} = \begin{pmatrix}
			d \\
			0
		\end{pmatrix}
	\end{align*}
	as required.

	For the second part, let $x_1' = s_1 x_1 + s_2 x_2$ and $x_2' = -\beta x_1 + \alpha x_2$.
	Then $Rx_1' + Rx_2' \subseteq Rx_1 + Rx_2$.
	The matrix defining $x_1', x_2'$ in terms of $x_1, x_2$ is invertible since its determinant is a unit; we can solve for $x_1, x_2$ in terms of $x_1', x_2'$.
	So $Rx_1' + Rx_2' = Rx_1 + Rx_2$.
	Then by direct computation we can see that $r_1 x_2 + r_2 x_2 = d x_1' + 0 \cdot x_2'$.
\end{proof}

The structure theorem for principal ideal domains follows the same method; it is deduced for Smith normal form.
That theorem also holds for principal ideal domains.
The above lemma allows one to prove Smith normal form for principal ideal domains.
In a Euclidean domain, we used the Euclidean function for a notion of size in order to perform induction; in a principal ideal domain we can count the irreducibles in a factorisation.

\begin{proof}[Proof of theorem]
	Let $M = Rx_1 + \dots + Rx_n$ where $n$ is minimal.
	If $x_1, \dots, x_n$ are independent, then $M$ is free as required.
	Suppose that the $x_i$ are not independent, so there exists $r_i$ such that $\sum r_i x_i = 0$ but not all of the $r_i$ are zero.
	By reordering, we can suppose that $r_1 \neq 0$.
	By using part (ii) of the previous lemma, after replacing $x_1$ and $x_2$ by suitable $x_1', x_2'$, we may assume that $r_1 \neq 0$ and $r_2 = 0$.
	By repeating this process with $x_1$ and $x_i$ for all $i \geq 2$, we obtain $r_1 \neq 0$ and $r_2 = \dots = r_n = 0$, so $r_1 x_1'' = 0$ for some nonzero $x_1'' \in M$.
	But $M$ is torsion-free, so $r_1$ must be zero, and this is a contradiction.
\end{proof}