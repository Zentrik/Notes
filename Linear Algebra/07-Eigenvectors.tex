\section{Eigenvectors and Eigenvalues}

\subsection{Eigenvalues}
Let $V$ be an $F$-vector space.
Let $\dim_F V = n < \infty$, and let $\alpha$ be an endomorphism of $V$.
\begin{question}
	Can we find a basis $B$ of $V$ such that, in this basis, $[\alpha]_B \equiv [\alpha]_{B,B}$ has a simple (e.g.\ diagonal, triangular) form?
\end{question} 
\textcolor{blue}{Recall} that if $B'$ is another basis and $P$ is the change of basis matrix, $[\alpha]_{B'} = P^{-1} [\alpha]_B P$.
\textcolor{blue}{Equivalently}, given a square matrix $A \in M_n(F)$ we want to conjugate it by a matrix $P$ such that the result is \textcolor{blue}{`simpler'}.

\begin{definition}[Diagonalisable]
	Let $\alpha \in L(V)$ be an endomorphism.
	We say that $\alpha$ is \vocab{diagonalisable} if there exists a basis $B$ of $V$ such that the matrix $[\alpha]_B$ is diagonal.
\end{definition}

\begin{definition}[Triangulable]
	We say that $\alpha$ is \vocab{triangulable} if there exists a basis $B$ of $V$ such that $[\alpha]_B$ is triangular.
\end{definition} 

\begin{remark}
	We can express this equivalently in terms of conjugation of matrices.
\end{remark}

\begin{definition}[Eigenvalue, Eigenvector and Eigenspace]
	A scalar $\lambda \in F$ is an \vocab{eigenvalue} of an endomorphism $\alpha$ if and only if there exists a vector $v \in V \setminus \qty{0}$ such that $\alpha(v) = \lambda v$.
	Such a vector is an \vocab{eigenvector} with eigenvalue $\lambda$. \\
	$V_\lambda = \qty{ v \in V \colon \alpha(v) = \lambda v } \leq V$ is the \vocab{eigenspace} associated to $\lambda$.
\end{definition}

\begin{lemma}
	Let $\alpha \in L(V)$ and $\lambda \in F$. \\
	$\lambda$ is an eigenvalue iff $\det(\alpha - \lambda I) = 0$.
\end{lemma}

\begin{proof}
	If $\lambda$ is an eigenvalue, there exists a nonzero vector $v$ such that $\alpha(v) = \lambda v$, so $(\alpha - \lambda I)(v) = 0$.
	So the kernel is non-trivial.
	So $\alpha - \lambda I$ is not injective, so it is not surjective by the rank-nullity theorem.
	Hence this matrix is not invertible, so it has zero determinant.
\end{proof}
\begin{remark}
	If $\alpha(v_j) = \lambda_j v_j$ ($v_j \neq 0$) for $j \in \qty{1, \dots, m}$, we can complete the family $v_j$ into a basis $(v_1, \dots, v_n)$ of $V$.
	Then in this basis, the first $m$ columns of the matrix $\alpha$ has diagonal entries $\lambda_j$.
\end{remark}

\subsection{Elementary facts about polynomials}
Recall the following facts about polynomials on a field $F$, for instance
\begin{align*}
	f(t) = a_n t^n + \dots + a_1 t + a_0, \quad a_i \in F
\end{align*}
We say that the degree of $f$, written $\deg f$ is $n$.
The degree of $f + g$ is at most the maximum degree of $f$ and $g$.
$\deg (fg) = \deg f + \deg g$. \\
Let $F[t]$ be the vector space of polynomials with coefficients in $F$. \\
$\lambda$ is a root of $f(t)$ $\iff f(\lambda = 0)$.

\begin{lemma}
	If $\lambda$ is a root of $f$ then $(t-\lambda)$ divides $F$.
	I.e. $f(t) = (t - \lambda) g(t)$ where $g(t) \in F[t]$.
\end{lemma} 

\begin{proof}
	\begin{align*}
		f(t) = a_n t^n + \dots + a_1 t + a_0
	\end{align*}
	Hence,
	\begin{align*}
		f(\lambda) = a_n \lambda^n + \dots + a_1 \lambda + a_0 = 0
	\end{align*}
	which implies that
	\begin{align*}
		f(t) = f(t) - f(\lambda) = a_n(t^n - \lambda^n) + \dots + a_1(t - \lambda)
	\end{align*}
	But note that, for all $n$,
	\begin{align*}
		t^n - \lambda^n = (t - \lambda)(t^{n-1} + \lambda t^{n-2} + \dots + \lambda^{n-2} t + \lambda^{n-1})
	\end{align*}
\end{proof}

\begin{remark}
	We say that $\lambda$ is a root of \vocab{multiplicity} $k$ if $(t-\lambda)^k$ divides $f$ but $(t-\lambda)^{k+1}$ does not.
\end{remark}

\begin{corollary}
	A \textcolor{blue}{nonzero} polynomial of degree $n$ has \textcolor{red}{at most} $n$ roots, counted with multiplicity.
\end{corollary}

\begin{proof}
	Induction on the degree.
	Left as an exercise.
\end{proof} 

\begin{corollary}
	If $f_1, f_2$ are two polynomials of degree less than $n$ such that $f_1(t_i) = f_2(t_i)$ for $i \in \qty{1, \dots, n}$ and $t_i$ distinct, then $f_1 \equiv f_2$.
\end{corollary}

\begin{proof}
	$f_1 - f_2$ has degree less than $n$, but has $n$ roots.
	Hence it is zero.
\end{proof}

\begin{theorem}
	Any polynomial $f \in \mathbb C[t]$ of positive degree has a complex root.
	When counted with multiplicity, $f$ has a number of roots equal to its degree.
\end{theorem}

\begin{corollary}
	Any polynomial $f \in \mathbb C[t]$ can be factorised into an amount of linear factors equal to its degree.
	$f(t) = c \prod_{i = 1}^r (t - \lambda_i)^{\alpha_i}$, with $c \in \mathbb{C}$, $\lambda_i \in \mathbb{C}$, $\alpha_i \in \mathbb{N}$.
\end{corollary}

Proved in Complex Analysis.

\subsection{Characteristic polynomials}
\begin{definition}[Characteristic polynomials]
	Let $\alpha$ be an endomorphism.
	The \vocab{characteristic polynomial} of $\alpha$ is
	\begin{align*}
		\chi_\alpha(t) = \det(A\footnote{$A = [\alpha]_B$ for any basis $B$, we will see it's well defined below.} - t I)
	\end{align*}
\end{definition}

\begin{remark}
	\begin{enumerate}
		\item $\chi_\alpha$ is a polynomial because the determinant is defined as a polynomial in the terms of the matrix.
		\item Note further that conjugate matrices have the same characteristic polynomial, so the above definition is well defined in any basis.
		Indeed, $\det(P^{-1}A P - \lambda I) = \det(P^{-1}(A - \lambda I)P) = \det(A - \lambda I)$.
	\end{enumerate} 
\end{remark}

\begin{theorem}
	Let $\alpha \in L(V)$.
	$\alpha$ is triangulable iff $\chi_\alpha$ can be written as a product of linear factors over $F$.
	I.e. $\chi_\alpha(t) = c \prod_{i = 1}^n (t - \lambda_i)$\footnote{$\lambda_i$ need not be distinct.}
\end{theorem}

\begin{corollary}
	In particular, all complex matrices are triangulable.
\end{corollary} 

\begin{proof}
	($\implies$):
	Suppose $\alpha$ is triangulable.
	Then for a basis $B$, $[\alpha]_B$ is triangulable with diagonal entries $a_i$.
	Then
	\begin{align*}
		\chi_\alpha(t) = (a_1 - t)(a_2 - t) \cdots (a_n - t)
	\end{align*}

	($\Longleftarrow$): We argue by induction on $n = \dim V$.
	True for $n = 1$. \\
	By assumption, let $\chi_\alpha(t)$ be the characteristic polynomial of $\alpha$ with a root $\lambda$.
	Then, $\chi_\alpha(\lambda) = 0$ implies $\lambda$ is an eigenvalue.
	Let $V_\lambda$ be the corresponding eigenspace.
	Let $(v_1, \dots, v_k)$ be the basis of this eigenspace, completed to a basis $(v_1, \dots, v_n)$ of $V$.
	Let $W = \vecspan\qty{v_{k+1}, \dots, v_n}$, and then $V = V_\lambda \oplus W$.
	Then
	\begin{align*}
		[\alpha]_B = \begin{pmatrix}
			\lambda I & \star \\
			0         & C
		\end{pmatrix}
	\end{align*}
	where $\star$ is arbitrary, and $C$ is a block of size $(n-k) \times (n-k)$. \\
	Then $\alpha$ induces an endomorphism $\overline \alpha \colon V/V_\lambda \to V/V_\lambda$ with $C = [\overline \alpha]_{\overline B}$ and $\overline B = (v_{k + 1} + V_\lambda, \dots, v_n + V_\lambda)$.

	Then (block product)
	\begin{align*}
		\det([\alpha]_B - tI) &= \det \begin{pmatrix}
		(\lambda - t) I & \star \\
		0 & C - tI
		\end{pmatrix} \\
		&= (\lambda - t)^k \det(C - tI) \\
		\text{We know } \det([\alpha]_B - tI) &= c \prod_{i = 1}^n (t - a_i) \\
		\implies \det(C - tI)\footnote{As $\det(C - tI)$ is a polynomial} &= c \prod_{k + 1}^n (t - \tilde{a_i})
	\end{align*} 

	By induction on the dimension, we can find a basis $(w_{k+1}, \dots, w_n)$ of $W$ for which $[C]_W$ has a triangular form.
	Then the basis $(v_1, \dots, v_k, w_{k+1}, \dots, w_n)$ is a basis for which $\alpha$ is triangular.
\end{proof}

\begin{lemma}
	Let $n = \dim V$, and $V$ be a vector space over $\mathbb R$ or $\mathbb C$.
	Let $\alpha$ be an endomorphism on $V$.
	Then
	\begin{align*}
		\chi_\alpha(t) = (-1)^n t^n + c_{n-1} t^{n-1} + \dots + c_0
	\end{align*}
	with
	\begin{align*}
		c_0 = \det A;\quad c_{n-1} = (-1)^{n-1} \tr A
	\end{align*}
\end{lemma}
\begin{proof}
	\begin{align*}
		\chi_\alpha(t) = \det(\alpha - t I) \implies \chi_\alpha(0) = \det(\alpha) = c_0.
	\end{align*}
	Further, for $\mathbb R, \mathbb C$\footnote{For $\mathbb{R}$ we can think of $A$ as having complex entries as well.} we know that $\alpha$ is triangulable over $\mathbb C$.
	Hence $\chi_\alpha(t)$ is the determinant of a triangular matrix;
	\begin{align*}
		\chi_\alpha(t) &= \prod_{i=1}^n (a_i - t) \\
		&= (-1)^n t^n + c_{n-1} t^{n-1} + \dots + c_0
	\end{align*}
	Hence
	\begin{align*}
		c_{n-1} = (-1)^{n-1} \underbracket{\sum_i^n  a_i}_{\tr A}
	\end{align*}
	Since the trace is invariant under a change of basis, this is exactly the trace as required.
\end{proof}

\subsection{Polynomials for matrices and endomorphisms}
Let $p(t)$ be a polynomial over $F$.
We will write
\begin{align*}
	p(t) = a_n t^n + \dots + a_0,\quad a_i \in F
\end{align*}
For a matrix $A \in M_n(F)$ ($\forall \; k \ A^k \in M_n(f)$), we define
\begin{align*}
	p(A) = a_n A^n + \dots + a_0 \in M_n(F)
\end{align*}
For an endomorphism $\alpha \in L(V)$,
\begin{align*}
	p(\alpha) = a_n \alpha^n + \dots + a_0 I \in L(V);\quad \alpha^k \equiv \underbrace{\alpha \circ \dots \circ \alpha}_{k \text{ times}}
\end{align*}

\subsection{Sharp criterion of diagonalisability}
\begin{theorem}
	Let $V$ be a vector space over $F$ of finite dimension $n$.
	Let $\alpha$ be an endomorphism of $V$. \\
	Then $\alpha$ is diagonalisable if and only if there exists a polynomial $p$ which is a product of \textit{distinct linear factors}, such that $p(\alpha) = 0$.
	In other words, there exist \textit{distinct} $\lambda_1, \dots, \lambda_k$ such that
	\begin{align*}
		p(t) = \prod_{i=1}^n (t - \lambda_i) \implies p(\alpha) = 0
	\end{align*}
\end{theorem}
\begin{proof}
	($\implies$)
	Suppose $\alpha$ is diagonalisable.
	Let $\lambda_1, \dots, \lambda_k$ be the $k \leq n$ \textit{distinct} eigenvalues.
	Let
	\begin{align*}
		p(t) = \prod_{i=1}^k (t-\lambda_i)
	\end{align*}
	Let $B$ be a basis of $V$ made of the eigenvectors of $\alpha$ (it is precisely the basis in which $[\alpha]_B$ is diagonal). \\
	Let $v \in B$.
	Then $\alpha(v) = \lambda_i v$ for some $i$.
	Then, since the terms in the following product commute,
	\begin{align*}
		(\alpha - \lambda_i I)(v) = 0 \implies p(\alpha)(v) = \qty[\prod_{j=1}^k (\alpha - \lambda_j I)](v)\footnote{One of the $j$s is $i$, so as they commute product is 0.} = 0
	\end{align*}
	So for all basis vectors, $p(\alpha)(v) = 0$.
	As $B$ a basis, by linearity, $p(\alpha)(v) = 0 \ \forall \; v \in V$ so $p(\alpha) = 0$.

	($\Longleftarrow$) (Kernel lemma, Bezout's theorem for prime polynomials) \\
	Conversely, suppose that $p(\alpha) = 0$ for some polynomial $p(t) = \prod_{i=1}^k (t-\lambda_i)$ with distinct $\lambda_i$.
	Let $V_{\lambda_i} = \ker(\alpha - \lambda_i I)$.
	We claim that
	\begin{align*}
		V = \bigoplus_{i=1}^k V_{\lambda_i}
	\end{align*}
	Consider the polynomials
	\begin{align*}
		q_j(t) = \prod_{i=1,i \neq j}^k \frac{t-\lambda_i}{\lambda_j - \lambda_i}
	\end{align*}
	These polynomials evaluate to one at $\lambda_j$ and zero at $\lambda_i$ for $i \neq j$.
	Hence $q_j(\lambda_i) = \delta_{ij}$.
	We now define the polynomial
	\begin{align*}
		q = q_1 + \dots + q_k
	\end{align*}
	We know $\deg q_j \leq k-1$ so $\deg q \leq k -1$.
	Note, $q(\lambda_i) = 1$ for all $i \in \qty{1, \dots, k}$.
	The only polynomial that evaluates to one at $k$ points with degree at most $(k-1)$ is exactly given by $q(t) = 1$. \\
	Consider the endomorphism
	\begin{align*}
		\pi_j = q_j(\alpha) \in L(V)
	\end{align*}
	These are called the `projection operators'.
	By construction,
	\begin{align*}
		\sum_{j=1}^k \pi_j = \sum_{j=1}^k q_j(\alpha) = I
	\end{align*}
	So the sum of the $\pi_j$ is the identity.
	Hence, for all $v \in V$,
	\begin{align*}
		I(v) = v = \sum_{j=1}^k \pi_j(v) = \sum_{j=1}^k q_j(\alpha)(v)
	\end{align*}
	So we can decompose any vector as a sum of its projections $\pi_j(v)$.
	\underline{Observe} by definition of $q_j$ and $p$,
	\begin{align*}
		(\alpha - \lambda_j I) q_j(\alpha)(v) & = \frac{1}{\prod_{i \neq j} (\lambda_j - \lambda_i)} (\alpha - \lambda_j I) \qty[\prod_{i \neq j} (t - \lambda_i)] (\alpha) \\
		& = \frac{1}{\prod_{i \neq j} (\lambda_j - \lambda_i)} \prod_{i=1}^k (\alpha - \lambda_i I)(v) \\
		& = \frac{1}{\prod_{i \neq j} (\lambda_j - \lambda_i)} p(\alpha)(v)
	\end{align*}
	By assumption, this is zero.
	For all $v$, we have
	\begin{align*}
		(\alpha - \lambda_j I) \pi_j(v) = 0 \implies \pi_j(v) \in \ker(\alpha - \lambda_j I) = V_{\lambda_j}
	\end{align*}
	($\pi_j$ is a projector on $V_{\lambda_j}$).
	We have then proven that, for all $v \in V$,
	\begin{align*}
		v = q(v) = \sum_{j=1}^k \underbrace{\pi_j(v)}_{\in V_{\lambda_j}}
	\end{align*}
	Hence,
	\begin{align*}
		V = \sum_{j=1}^k V_{\lambda_j}
	\end{align*}

	It remains to show that the sum is direct.
	Indeed, let
	\begin{align*}
		v \in V_{\lambda_j} \cap \qty(\sum_{i \neq j} V_{\lambda_i})
	\end{align*}
	We must show $v = 0$.
	$v \in V_{\lambda_j}$ so applying $\pi_j$,
	\begin{align*}
		\pi_j(v) = q_j(\alpha)(v) = \prod_{i \neq j} \frac{(\alpha - \lambda_i I)(v)}{\lambda_j - \lambda_i}
	\end{align*}
	Since $\alpha(v) = \lambda_j v$,
	\begin{align*}
		\pi_j(v) = \prod_{i \neq j} \frac{(\lambda_j - \lambda_i)v}{\lambda_j - \lambda_i} = v
	\end{align*}
	So $\pi_j \mid_{V_{\lambda_j}} = Id$.
	% Hence $\pi_j$ really projects onto $V_{\lambda_j}$.
	However, we also know $v \in \sum_{i \neq j} V_{\lambda_i}$.
	So we can write $v = \sum_{i \neq j} w_i$ for $w \in V_{\lambda_i}$.
	Thus,
	\begin{align*}
		\pi_j(w_i) = \prod_{m \neq j} \frac{(\alpha - \lambda_m I)(w_i)}{\lambda_m - \lambda_j}
	\end{align*}
	Since $\alpha(w_i) = \lambda_i w_i$, one of the factors will vanish, hence
	\begin{align*}
		\pi_j(w_i) = 0
	\end{align*}
	So $\pi_j \mid_{V_{\lambda_i}} = 0$ for $i \neq j$ and
	\begin{align*}
		v = \sum_{i \neq j} w_i \implies \pi_j(v) = \sum_{i \neq j} \pi_j(w_i) = 0
	\end{align*}
	But $v = \pi_j(v)$ hence $v = 0$.

	So the sum is direct.
	Hence, $B = (B_1, \dots, B_k)$ is a basis of $V$, where the $B_i$ are bases of $V_{\lambda_i}$.
	Then $[\alpha]_B$ is diagonal.

	Also, we know $\pi_j \mid_{V_{\lambda_j}} = Id$ and $\pi_j \mid_{V_{\lambda_i}} = 0$ for $i \neq j$ so $\pi_j$ is the projector onto $V_{\lambda_j}$.
\end{proof}
\begin{remark}
	We have shown further that if $\lambda_1, \dots, \lambda_k$ are \underline{distinct} eigenvalues of $\alpha$, then
	\begin{align*}
		\sum_{i=1}^k V_{\lambda_i} = \bigoplus_{i=1}^k V_{\lambda_i}
	\end{align*}
	(and we know the projectors).
	Therefore, the only way that diagonalisation fails is when this sum is not direct, so
	\begin{align*}
		\sum_{i=1}^k V_{\lambda_i} < V
	\end{align*}
\end{remark}
\begin{example}
	Let $F = \mathbb C$.
	Let $A \in M_n(F)$ such that $A$ has finite order; there exists $m \in \mathbb N$ such that $A^m = I$.
	Then $A$ is diagonalisable.
	This is because
	\begin{align*}
		t^m - 1 = p(t) = \prod_{j=1}^m (t - \xi_m^j);\quad \xi_m = e^{2 \pi i/m}
	\end{align*}
	and $p(A) = 0$.
\end{example}

\subsection{Simultaneous diagonalisation}
\begin{theorem}
	Let $V$ be a finite dimensional vector space.
	Let $\alpha, \beta$ be endomorphisms of $V$ which are diagonalisable. \\
	Then $\alpha, \beta$ are \vocab{simultaneously diagonalisable} (there exists a basis $B$ of $V$ such that $[\alpha]_B, [\beta]_B$ are diagonal) if and only if $\alpha$ and $\beta$ commute.
\end{theorem}

\begin{proof}
	($\implies$) $\exists \; B$ basis of $V$ s.t. $[\alpha]_B$, $[\beta]_B$ are diagonal.
	Two diagonal matrices commute, i.e. $[\alpha]_B [\beta]_B = [\beta]_B [\alpha]_B$.
	If such a basis exists, $\alpha \beta = \beta \alpha$ in this basis.
	So this holds in any basis.

	($\Longleftarrow$)
	Conversely, suppose $\alpha, \beta$ are diagonalisable and $\alpha \beta = \beta \alpha$.
	We have
	\begin{align*}
		V = \bigoplus_{i=1}^k V_{\lambda_i}
	\end{align*}
	where $\lambda_i, \dots, \lambda_k$ are the $k$ distinct eigenvalues of $\alpha$.
	\begin{claim}
		$V_{\lambda_i}$ stable by $\beta$, i.e. $\beta \qty(V_{\lambda_j}) \leq V_{\lambda_j}$.
	\end{claim} 
	\begin{proof}
		Indeed, for $v \in V_{\lambda_j}$,
		\begin{align*}
			\alpha \beta(v) = \beta \alpha(v) = \beta(\lambda_j v) = \lambda_j \beta(v) \implies \alpha(\beta(v)) = \lambda_j \beta(v)
		\end{align*}
		Hence, $\beta(v) \in V_{\lambda_j}$.
	\end{proof} 
	By assumption, $\beta$ is diagonalisable.
	Hence, there exists a polynomial $p$ with distinct linear factors such that $p(\beta) = 0$.
	Now, $\beta\qty(V_{\lambda_j}) \leq V_{\lambda_j}$ so we can consider $\eval{\beta}_{V_{\lambda_j}}$.
	This is an endomorphism of $V_{\lambda_j}$.
	We can see that
	\begin{align*}
		p\qty(\eval{\beta}_{V_{\lambda_j}}) = 0
	\end{align*}
	Hence, $\eval{\beta}_{V_{\lambda_j}}$ is diagonalisable.
	Let $B_i$ be the basis of $V_{\lambda_i}$ in which $\eval{\beta}_{V_{\lambda_j}}$ is diagonal.
	Since $V = \bigoplus V_{\lambda_i}$, $B = (B_1, \dots, B_k)$ is a basis of $V$.
	Then the matrices of $\alpha$ and $\beta$ in $V$ are diagonal.
\end{proof}

\subsection{Minimal polynomials of an endomorphism}
Recall from IB GRM the Euclidean algorithm for dividing polynomials.
Given $a, b$ polynomials over $F$ with $b$ nonzero, there exist polynomials $q, r$ over $F$ with $\deg r < \deg b$ and $a = qb + r$.
\begin{definition}[Minimal polynomial]
	Let $V$ be a finite dimensional $F$-vector space.
	Let $\alpha$ be an endomorphism on $V$. \\
	The \vocab{minimal polynomial} $m_\alpha$ of $\alpha$ is the (unique up to a constant) nonzero polynomial with \textit{smallest degree} such that $m_\alpha(\alpha) = 0$.
\end{definition}
\begin{remark}
	If $\dim V = n < \infty$, then $\dim L(V) = n^2$.
	In particular, the family $\qty{I, \alpha, \dots, \alpha^{n^2}}$ cannot be free since it has $n^2+1$ entries.
	So $\exists \; (a_{n^2}, \dots, a_1, a_0) \neq 0$ s.t. $a_{n^2} \alpha^{n^2} + \dots = a_1 \alpha + a_0 = 0$.
	So $\exists \; p \in F[t]$ s.t. $p \neq 0$ and $p(\alpha) = 0$.
	Hence, a minimal polynomial always exists.
\end{remark}
\begin{lemma}
	Let $\alpha \in L(V)$ and $p \in F[t]$ be a polynomial. \\
	Then $p(\alpha) = 0$ if and only if $m_\alpha$ is a factor of $p$.
	In particular, $m_\alpha$ is well-defined and unique up to a constant multiple.
\end{lemma}
\begin{proof}
	Let $p \in F[t]$ such that $p(\alpha) = 0$.
	If $m_\alpha(\alpha) = 0$ and $\deg m_\alpha < \deg p$, we can perform the division $p = m_\alpha q + r$ for $\deg r < \deg m_\alpha$.
	Then $p(\alpha) = m_\alpha(\alpha) q(\alpha) + r(\alpha)$.
	But $m_\alpha(\alpha) = 0$ so $r(\alpha) = 0$. \\
	But $\deg r < \deg m_\alpha$ and $m_\alpha$ is the smallest degree polynomial which evaluates to zero for $\alpha$, so $r \equiv 0$ so $p = m_\alpha q$.
	d
	In particular, if $m_1, m_2$ are both minimal polynomials that evaluate to zero for $\alpha$, we have $m_1$ divides $m_2$ and $m_2$ divides $m_1$.
	Hence they are equivalent up to a constant.
\end{proof}
\begin{example}
	Let $V = F^2$ and
	\begin{align*}
		A= \begin{pmatrix}
			1 & 0 \\
			0 & 1
		\end{pmatrix};\quad B = \begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}
	\end{align*}
	We can check $p(t) = (t-1)^2$ gives $p(A) = p(B) = 0$.
	So the minimal polynomial of $A$ or $B$ must be either $(t-1)$ or $(t-1)^2$ (as the min poly divides any poly s.t. $p(\alpha) = 0$).
	For $A$, we can find the minimal polynomial is $(t-1)$, and for $B$ we require $(t-1)^2$. \\
	A is diagonalisable as it is a product of distinct linear factors.
	So $B$ is not diagonalisable, since its minimal polynomial is not a product of distinct linear factors.
\end{example}

\subsection{Cayley-Hamilton theorem}
\begin{theorem}[Cayley-Hamitlon]
	Let $V$ be a finite dimensional $F$-vector space.
	Let $\alpha \in L(V)$ with characteristic polynomial $\chi_\alpha(t) = \det(\alpha - t I)$.
	Then $\chi_\alpha(\alpha) = 0$.
\end{theorem}

\begin{corollary}
	$m_\alpha \mid \chi_\alpha$.
\end{corollary} 

Two proofs will provided; one more physical and based on $F = \mathbb C$ and one more algebraic.
\begin{proof}
	Let $F = \mathbb{C}$.
	Let $B = \qty{v_1, \dots, v_n}$ be a basis of $V$ such that $[\alpha]_B$ is triangular.
	Note, if the diagonal entries in this basis are $a_i$,
	\begin{align*}
		\chi_\alpha(t) = \prod_{i=1}^n (a_i - t) \implies \chi_\alpha(\alpha) = (\alpha - a_1 I) \dots (\alpha - a_n I)
	\end{align*}
	We want to show that this expansion evaluates to zero.
	Let $U_j = \vecspan \qty{v_1, \dots, v_j}$.
	Let $v \in V = U_n$.
	We want to compute $\chi_\alpha(\alpha)(v)$.
	Note, by construction of the triangular matrix.
	\begin{align*}
		\chi_\alpha(\alpha)(v) & = (\alpha - a_1 I) \dots \underbrace{(\alpha - a_n I)(v)}_{\in U_{n-1}} \\
		&= (\alpha - a_1 I) \dots \underbrace{(\alpha - a_{n-1} I)(\alpha - a_n I)(v)}_{\in U_{n-2}} \\
		&= (\alpha - a_1 I) \underbrace{\dots (\alpha - a_n I)(v)}_{\in U_1} \\
		&= 0
	\end{align*}
	Hence $\chi_\alpha(\alpha) = 0$.
\end{proof}
\noindent The following proof works for any field where we can equate coefficients, but is much less intuitive.
\begin{proof}
	We will write
	\begin{align*}
		\det(t I - \alpha) = (-1)^n \chi_\alpha(t) = t^n + a_{n-1}t^{n-1} + \dots + a_0
	\end{align*}
	For any matrix $B$, we have proven $B \adj B = (\det B) I$.
	We apply this relation to the matrix $B = tI - A$.
	We can check that
	\begin{align*}
		\adj B = \adj(tI - A) = B_{n-1} t^{n-1} + \dots + B_1 t + B_0
	\end{align*}
	since adjugate matrices are degree $(n-1)$ polynomials for each element.
	Then, by applying $B \adj B = (\det B) I$,
	\begin{align*}
		(tI - A) [ B_{n-1} t^{n-1} + \dots + B_1 t + B_0 ] = (\det B) I = (t^n + \dots + a_0) I
	\end{align*}
	Since this is true for all $t$, we can equate coefficients.
	This gives
	\begin{align*}
		t^n     & :      & I         & = B_{n-1}            \\
		t^{n-1} & :      & a_{n-1} I & = B_{n-2} - AB_{n-1} \\
		        & \vdots &           & \vdots               \\
		t^0     & :      & a_0 I     & = -A B_1
	\end{align*}
	Then, substituting $A$ for $t$ in each relation will give, for example, $A^n I = A^n B_{n-1}$.
	Computing the sum of all of these identities, we recover the original polynomial in terms of $A$ instead of in terms of $t$.
	Many terms will cancel since the sum telescopes, yielding
	\begin{align*}
		A^n + a_{n-1} A^{n-1} + \dots + a_0 I = 0
	\end{align*}
\end{proof}

\subsection{Algebraic and geometric multiplicity}
\begin{definition}[Algebraic/ geometric multiplicity.]
	Let $V$ be a finite dimensional $F$-vector space.
	Let $\alpha \in L(V)$ and let $\lambda$ be an eigenvalue of $\alpha$. \\
	Then
	\begin{align*}
		\chi_\alpha(t) = (t-\lambda)^{a_\lambda} q(t)
	\end{align*}
	where $q(t)$ is a non zero polynomial over $F$ such that $(t-\lambda)$ does not divide $q$.
	$a_\lambda$ is known as the \vocab{algebraic multiplicity} of the eigenvalue $\lambda$.
	We define the \vocab{geometric multiplicity} $g_\lambda$ of $\lambda$ to be the dimension of the eigenspace associated with $\lambda$, so $g_\lambda = \dim \ker (\alpha - \lambda I)$.
\end{definition}

\begin{remark}
	$\lambda$ an eigenvalue iff $\alpha - \lambda I$ singular iff $\det(\alpha - \lambda I) = \chi_\alpha(\lambda) = 0$.
\end{remark} 

\begin{lemma}
	If $\lambda$ is an eigenvalue of $\alpha \in L(V)$, then $1 \leq g_\lambda \leq a_\lambda$.
\end{lemma}

\begin{proof}
	We have $g_\lambda = \dim \ker (\alpha - \lambda I)$.
	There exists a nontrivial vector $v \in V$ such that $v \in \ker(\alpha - \lambda I)$ since $\lambda$ is an eigenvalue.
	Hence $g_\lambda \geq 1$. \\
	We will show that $g_\lambda \leq a_\lambda$.
	Indeed, let $v_1, \dots, v_{g_\lambda}$ be a basis of $V_\lambda \equiv \ker (\alpha - \lambda I)$.
	We complete this into a basis $B \equiv \qty(v_1, \dots, v_{g_\lambda}, v_{g_\lambda + 1}, \dots, v_n)$ of $V$.
	Then note that
	\begin{align*}
		[\alpha]_B = \begin{pmatrix}
			\lambda I_{g_\lambda} & \star \\
			0                     & A_1
		\end{pmatrix}
	\end{align*}
	for some matrix $A_1$.
	Now,
	\begin{align*}
		\det (\alpha - tI) = \det \begin{pmatrix}
			(\lambda - t) I_{g_\lambda} & \star     \\
			0                           & A_1 - t I
		\end{pmatrix}
	\end{align*}
	By the formula for determinants of block matrices with a zero block on the off diagonal,
	\begin{align*}
		\det (\alpha - tI) = (\lambda-t)^{g_\lambda} \det(A_1 - t I)
	\end{align*}
	Hence $g_\lambda \leq a_\lambda$ since the determinant is a polynomial that could have more factors of the same form.
\end{proof}
\begin{lemma}
	Let $V$ be a finite dimensional $F$-vector space.
	Let $\alpha \in L(V)$ and let $\lambda$ be an eigenvalue of $\alpha$.
	Let $c_\lambda$ be the multiplicity of $\lambda$ as a root of the minimal polynomial of $\alpha$.
	Then $1 \leq c_\lambda \leq a_\lambda$.
\end{lemma}
\begin{proof}
	By the Cayley-Hamilton theorem, $\chi_\alpha(\alpha) = 0$.
	Since $m_\alpha$ is linear, $m_\alpha$ divides $\chi_\alpha$.
	Hence $c_\lambda \leq a_\lambda$. \\
	Now we show $c_\lambda \geq 1$.
	Indeed, $\lambda$ is an eigenvalue hence there exists a nonzero $v \in V$ such that $\alpha(v) = \lambda v$.
	For such an eigenvector, $\alpha^P(v) = \lambda^P v$ for $P \in \mathbb N$.
	Hence for $p \in F[t]$, $p(\alpha)(v) = [p(\lambda)]v$.
	Hence $m_\alpha(\alpha)(v) = [m_\alpha(\lambda)](v)$.
	Since the left hand side is zero, $m_\alpha(\lambda) = 0$.
	So $c_\lambda \geq 1$.
\end{proof}
\begin{example}
	Let
	\begin{align*}
		A = \begin{pmatrix}
			1 & 0 & -2 \\
			0 & 1 & 1  \\
			0 & 0 & 2
		\end{pmatrix}
	\end{align*}
	The minimal polynomial can be computed by considering the characteristic polynomial
	\begin{align*}
		\chi_A(t) = (t-1)^2(t-2)
	\end{align*}
	So the minimal polynomial is either $(t-1)^2(t-2)$ or $(t-1)(t-2)$.
	We check $(t-1)(t-2)$.
	$(A - I)(A - 2I)$ can be found to be zero.
	So $m_A(t) = (t-1)(t-2)$.
	Since this is a product of distinct linear factors, $A$ is diagonalisable.
\end{example}
\begin{example}
	Let $A$ be a Jordan block of size $n \geq 2$.
	Then $g_\lambda = 1$, $a_\lambda = n$, and $c_\lambda = n$.
\end{example}

\subsection{Characterisation of diagonalisable complex endomorphisms}
\begin{lemma}[Characterisation of diagonalisable endomorphisms over $F = \mathbb{C}$]
	Let $F = \mathbb C$.
	Let $V$ be a finite-dimensional $\mathbb C$-vector space.
	Let $\alpha$ be an endomorphism of $V$.
	Then the following are equivalent.
	\begin{enumerate}
		\item $\alpha$ is diagonalisable;
		\item for all $\lambda$ eigenvalues of $\alpha$, we have $a_\lambda = g_\lambda$;
		\item for all $\lambda$ eigenvalues of $\alpha$, $c_\lambda = 1$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	First, the fact that (i) is true if and only if (iii) is true has already been proven.
	Now let us show that (i) is equivalent to (ii).
	
	Let $\lambda_1, \dots, \lambda_k$ be the distinct eigenvalues of $\alpha$.
	We have already found that $\alpha$ is diagonalisable if and only if $V = \bigoplus V_{\lambda_i}$.
	The sum was found to be always direct, regardless of diagonalisability.
	We will compute the dimension of $V$ in two ways;
	\begin{align*}
		n = \dim V = \deg \chi_\alpha;\quad n = \dim V = \sum_{i=1}^k a_{\lambda_i}
	\end{align*}
	since $\chi_\alpha$ is a product of $(t-\lambda_i)$ factors as $F = \mathbb C$.
	Since the sum is direct,
	\begin{align*}
		\dim \qty(\bigoplus_{i=1}^k V_{\lambda_i}) = \sum_{i=1}^k g_{\lambda_i}
	\end{align*}
	$\alpha$ is diagonalisable if and only if the dimensions are equal, so
	\begin{align*}
		\sum_{i=1}^k g_{\lambda_i} = \sum_{i=1}^k a_{\lambda_i}
	\end{align*}
	Conversely, we have proven that for all eigenvalues $\lambda_i$, we have $g_{\lambda_i} \leq a_{\lambda_i}$.
	Hence, $\sum_{i=1}^k g_{\lambda_i} = \sum_{i=1}^k a_{\lambda_i}$ holds if and only if $g_{\lambda_i} = a_{\lambda_i}$ for all $i$.
\end{proof}