\section{Jordan Normal Form}

For this section, let $F = \mathbb C$.

\subsection{Definition}
\begin{definition}
	Let $A \in M_n(\mathbb C)$.
	We say that $A$ is in \textit{Jordan normal form} if it is a block diagonal matrix, where each block is of the form
	\begin{align*}
		J_{n_i}(\lambda) =
		\begin{pmatrix}
			\lambda & 1       & 0       & \cdots & 0       \\
			0       & \lambda & 1       & \cdots & 0       \\
			0       & 0       & \lambda & \cdots & 0       \\
			\vdots  & \vdots  & \vdots  & \ddots & \vdots  \\
			0       & 0       & 0       & \cdots & \lambda
		\end{pmatrix}
	\end{align*}
	We say that $J_{n_i}(\lambda) \in M_{n_i}(\mathbb C)$ are \textit{Jordan blocks}.
	The $\lambda_i \in \mathbb C$ need not be distinct.
\end{definition}
\begin{remark}
	In three dimensions,
	\begin{align*}
		A =
		\begin{pmatrix}
			\lambda & 0       & 0       \\
			0       & \lambda & 0       \\
			0       & 0       & \lambda
		\end{pmatrix}
	\end{align*}
	is in Jordan normal form, with three one-dimensional Jordan blocks with the same $\lambda$ value.
\end{remark}

\subsection{Similarity to Jordan normal form}
\begin{theorem}
	Any complex matrix $A \in M_n(\mathbb C)$ is similar to a matrix in Jordan normal form, which is unique up to reordering the Jordan blocks.
\end{theorem}
\noindent The proof is non-examinable.
This follows from IB Groups, Rings and Modules.
\begin{example}
	Let $\dim V = 2$.
	Then any matrix is similar to one of
	\begin{align*}
		\begin{pmatrix}
			\lambda_1 & 0         \\
			0         & \lambda_2
		\end{pmatrix};\quad
		\begin{pmatrix}
			\lambda & 0       \\
			0       & \lambda
		\end{pmatrix};\quad
		\begin{pmatrix}
			\lambda & 1       \\
			0       & \lambda
		\end{pmatrix}
	\end{align*}
	The minimal polynomials are
	\begin{align*}
		(t-\lambda_1)(t-\lambda_2);\quad (t-\lambda);\quad (t-\lambda)^2
	\end{align*}
\end{example}

\subsection{Direct sum of eigenspaces}
\begin{theorem}
	Let $V$ be a $\mathbb C$-vector space.
	Let $\dim V = n < \infty$.
	Then, the minimal polynomial $m_\alpha(t)$ of an endomorphism $\alpha \in L(V)$ satisfies
	\begin{align*}
		V = \bigoplus_{j=1}^k V_j
	\end{align*}
	where $V_j = \ker[(\alpha - \lambda_j I)^{c_j}]$, and where
	\begin{align*}
		m_\alpha(t) = \prod_{i=1}^k (t - \lambda_i)^{c_i}
	\end{align*}
	$V_j$ is called a \textit{generalised eigenspace} associated with $\lambda_j$.
\end{theorem}
\begin{remark}
	Note that $V_j$ is stable by $\alpha$, that is, $\alpha(V_j) = V_j$.
	Note further that $\eval{(\alpha - \lambda_j I)}_{V_j} = \mu_j$ gives that $\mu_j$ is a nilpotent endomorphism; $\mu_j^{c_j} = 0$.
	So the Jordan normal form theorem is a statement about nilpotent matrices.

	Note, when $\alpha$ is diagonalisable, $c_j = 1$ and hence we recover $V_j = \ker(\alpha - \lambda_j I)$ and $V = \bigoplus V_j$.
\end{remark}
\begin{proof}
	The key to this proof is that the projectors onto $V_j$ are `explicit'.
	First, recall
	\begin{align*}
		m_\alpha(t) = \prod_{j=1}^k (t-\lambda_j)^{c_j}
	\end{align*}
	Then, let
	\begin{align*}
		p_j(t) = \prod_{i \neq j} (t - \lambda_i)^{c_i}
	\end{align*}
	Then $p_j$ have by definition no common factor.
	So by Euclid's algorithm, we can find polynomials $q_i$ such that
	\begin{align*}
		\sum_{i=1}^k q_i p_i = 1
	\end{align*}
	We define the projector $\pi_j = q_j p_j(\alpha)$, which is an endomorphism.
	By construction, for all $v \in V$, we have
	\begin{align*}
		\sum_{j=1}^k \pi_j(v) = \sum_{j=1}^k a_j p_j(\alpha(v)) = I(v) = v
	\end{align*}
	Hence,
	\begin{align*}
		v = \sum_{i=1}^k \pi_i(v)
	\end{align*}
	Observe further that $\pi_j(v) \in V_j$.
	Indeed,
	\begin{align*}
		(\alpha - \lambda_j I)^{c_j} \pi_j(v) = (\alpha - \lambda_j I)^{c_j} q_j p_j(\alpha(v)) = q_j m_\alpha (\alpha(v)) = 0
	\end{align*}
	Hence $\pi_j(v) \in V_j$.
	In particular, $V = \sum_{j=1}^k V_j$.
	We need to show that this sum is direct.
	Note, for $i \neq j$, $\pi_i \pi_j = 0$ from the definition of $\pi$.
	Hence, observe that
	\begin{align*}
		\pi_i = \pi_i \qty(\sum_{j=1}^k \pi_j) \implies \pi_i = \pi_i \pi_i
	\end{align*}
	Thus, $\pi$ is a projector.
	In particular, this implies that $\eval{\pi_i}_{V_j}$ is the identity if $i = j$ and zero if $i \neq j$.
	This immediately implies that th sum is direct;
	\begin{align*}
		V = \bigoplus_{j=1}^k V_j
	\end{align*}
	Indeed, suppose
	\begin{align*}
		\sum_{j=1}^k \alpha_j v_j = 0;\quad v_j \in V_j;\quad \alpha_1 = 0
	\end{align*}
	Then
	\begin{align*}
		v_1 = -\frac{1}{\alpha_1} \sum_{j=2}^k \alpha_j v_j
	\end{align*}
	Applying $\pi_1$,
	\begin{align*}
		v_1 = -\frac{1}{\alpha_1} \sum_{j=2}^k \alpha_j \pi_1(v_j) = 0
	\end{align*}
	Iterating, we find $v = 0$.
\end{proof}
\begin{remark}
	We can compute the quantities $a_\lambda, g_\lambda, c_\lambda$ on the Jordan normal form of a matrix.
	Indeed, let $m \geq 2$ and consider a Jordan block $J_m(\lambda)$.
	Then $J_m(\lambda) - \lambda I$ is the zero matrix with ones on the off-diagonal.
	$(J_m(\lambda) - \lambda I)^k$ pushes the ones onto the next line iteratively, so
	\begin{align*}
		(J_m(\lambda) - \lambda I)^k = \begin{pmatrix}
			0 & I_{m-k} \\
			0 & 0
		\end{pmatrix}
	\end{align*}
	Hence $J$ is nilpotent of order exactly $m$.
	In Jordan normal form,
	\begin{enumerate}
		\item $a_\lambda$ is the sum of sizes of blocks with eigenvalue $\lambda$.
		      This is the amount of times $\lambda$ is seen on the diagonal.
		\item $g_\lambda$ is the amount of blocks with eigenvalue $\lambda$, since each block represents one eigenvector.
		\item $c_\lambda$ is the size of the largest block with eigenvalue $\lambda$.
	\end{enumerate}
\end{remark}
\begin{example}
	Let
	\begin{align*}
		A = \begin{pmatrix}
			0 & -1 \\
			1 & 2
		\end{pmatrix}
	\end{align*}
	We wish to convert this matrix into Jordan normal form; so we seek a basis for which this matrix becomes Jordan normal form.
	\begin{align*}
		\chi_A(t) = (t-1)^2
	\end{align*}
	Hence there exists only one eigenvalue, $\lambda = 1$.
	$A - I \neq 0$ hence $m_\alpha(t) = (t-1)^2$.
	Thus, the Jordan normal form of $A$ is of the form
	\begin{align*}
		B = \begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}
	\end{align*}
	Now,
	\begin{align*}
		\ker(A - I) = \genset{v_1};\quad v_1 = \begin{pmatrix}
			1 \\ -1
		\end{pmatrix}
	\end{align*}
	Further, we seek a $v_2$ such that
	\begin{align*}
		(A - I)v_2 = v_1 \implies v_2 = \begin{pmatrix}
			-1 \\ 0
		\end{pmatrix}
	\end{align*}
	Such a $v_2$ is not unique.
	Now,
	\begin{align*}
		A = \begin{pmatrix}
			1  & -1 \\
			-1 & 0
		\end{pmatrix}
		\begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}
		\begin{pmatrix}
			1  & -1 \\
			-1 & 0
		\end{pmatrix}^{-1}
	\end{align*}
\end{example}