\section{Determinant and Traces}

\subsection{Trace}
\begin{definition}[Trace]
	The \vocab{trace} of a square matrix $A \in M_{n,n}(F) \equiv M_n(F)$ is defined by
	\begin{align*}
		\tr A = \sum_{i=1}^n A_{ii}
	\end{align*}
\end{definition}

\begin{remark}
    \begin{align*}
        M_n(F) &\to F \\
        A &\mapsto \tr A
    \end{align*} 
    The trace is a linear form.
\end{remark} 

\begin{lemma}
	$\tr (AB) = \tr (BA)$ for any matrices $A, B \in M_n(F)$.
\end{lemma}

\begin{proof}
	We have
	\begin{align*}
		\tr (AB) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} b_{ji} = \sum_{j=1}^n \sum_{i=1}^n b_{ji} a_{ij} = \tr (BA)
	\end{align*}
\end{proof}
\begin{corollary}
	Similar matrices have the same trace.
\end{corollary}
\begin{proof}
	\begin{align*}
		\tr(P^{-1}AP) = \tr (A P^{-1} P) = \tr A
	\end{align*}
\end{proof}
\begin{definition}[Trace of a linear]
	If $\alpha \colon V \to V$ is linear, we can define the \vocab{trace} of $\alpha$ as
	\begin{align*}
		\tr \alpha = \tr [\alpha]_B
	\end{align*}
	for any basis $B$.
	This is well-defined by the corollary above.
\end{definition}
\begin{lemma}
	If $\alpha \colon V \to V$ is linear, $\alpha^\star \colon V^\star \to V^\star$ satisfies
	\begin{align*}
		\tr \alpha = \tr \alpha^\star
	\end{align*}
\end{lemma}

\begin{proof}
	\begin{align*}
		\tr \alpha = \tr [\alpha]_B = \tr [\alpha]_B^\transpose\footnote{Check $\tr [\alpha]_B = \tr [\alpha]_B^\transpose$} = \tr [\alpha^\star]_{B^\star} = \tr \alpha^\star
	\end{align*}
\end{proof}

\subsection{Permutations and transpositions}
Recall the following facts about permutations and transpositions.
$S_n$ is the group of permutations of the set $\qty{1, \dots, n}$; the group of bijections $\sigma \colon \qty{1, \dots, n} \to \qty{1, \dots, n}$.
A transposition $\tau_{k \ell} = (k, \ell)$ is defined by $k \mapsto \ell, \ell \mapsto k, x \mapsto x$ for $x \neq k, \ell$.
Any permutation $\sigma$ can be decomposed as a product of transpositions.
This decomposition is not necessarily unique, but the parity of the number of transpositions is well-defined.
We say that the signature of a permutation, denoted $\varepsilon \colon S_n \to \qty{-1, 1}$, is $1$ if the decomposition has even parity and $-1$ if it has odd parity.
We can then show that $\varepsilon$ is a homomorphism.

\subsection{Determinant}
\begin{definition}[Determinant]
	Let $A \in M_n(F)$.
	We define
	\begin{align*}
		\det A = \sum_{\sigma \in S_n} \varepsilon(\sigma) A_{\sigma(1) 1} \dots A_{\sigma(n) n}
	\end{align*}
\end{definition}
\begin{example}
	Let $n = 2$.
	Then,
	\begin{align*}
		A = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \implies \det A = a_{11} a_{22} - a_{12} a_{21}
	\end{align*}
\end{example}
\begin{lemma}
	If $A = (a_{ij})$ is an upper (or lower) triangular matrix (with zeroes on the diagonal), then $\det A = 0$.
\end{lemma}
\begin{proof}
	Let $(a_{ij}) = 0$ for $i > j$.
	Then
	\begin{align*}
		\det A = \sum_{\sigma \in S_n} \varepsilon(\sigma) a_{\sigma(1) 1} \dots a_{\sigma(n) n}
	\end{align*}
	For the summand to be nonzero, $\sigma(j) \leq j$ for all $j$.
	Thus,
	\begin{align*}
		\det A = a_{1 1} \dots a_{n n} = 0
	\end{align*}
\end{proof}

\begin{exercise}
	Show similarly $\det \begin{pmatrix}\lambda_1 &  & * \\ & \ddots & \\ 0 &  & \lambda_n\end{pmatrix} = \prod_{i = 1}^n \lambda_i$.
\end{exercise} 

\begin{lemma}
	Let $A \in M_n(F)$.
	Then, $\det A = \det A^\transpose$.
\end{lemma}
\begin{proof}
	\begin{align*}
		\det A & = \sum_{\sigma \in S_n} \varepsilon(\sigma) a_{\sigma(1) 1} \dots a_{\sigma(n) n} \\
		&= \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_i a_{i \sigma\inv(i)} \quad \text{as $\sigma$ a bijection}\footnote{See V\&M notes for better explanation.} \\
		&= \sum_{\sigma\inv \in S_n} \varepsilon(\sigma\inv) \prod_i a_{i \sigma\inv(i)} \\
		& = \sum_{\sigma \in S_n} \varepsilon(\sigma)\prod_i a_{i \sigma(i)} \quad \text{as $\sigma$ a bijection}\\
		& = \det A^\transpose
	\end{align*}
\end{proof}

\subsection{Volume forms}
Why do we use this formula for $\det A$?

\begin{definition}[Volume Form]
	A \vocab{volume form} $d$ on $F^n$ is a function $d \colon \underbrace{F^n \times \dots \times F^n}_{n \text{ times}} \to F$ satisfying
	\begin{enumerate}
		\item $d$ is multilinear: for all $i \in \qty{1, \dots, n}$ and for all $v_1, \dots, v_{i-1}, v_{i+1}, \dots, v_n \in F^n$, the map from $F^n$ to $F$ defined by
		\begin{align*}
			v \mapsto (v_1, \dots, v_{i-1}, v, v_{i+1}, \dots, v_n)
		\end{align*}
		is linear.
		In other words, this map is an element of $(F^n)^\star$.\footnote{Linear with respect to all $n$ coordinates.}
		\item $d$ is alternating: if $v_i = v_j$ for some $i \neq j$, $d = 0$.
	\end{enumerate}
	So an alternating multilinear form is a volume form.
\end{definition}

\underline{Aim}: We want to show that there is in fact only ONE (up to a multiplicative constant) volume form on $F^n \times \dots \times F^n$ which is given by the determinant.

\begin{lemma}
	The map $(F^n)^n \to F$ defined by $(A^{(1)}, \dots, A^{(n)}) \mapsto \det A$ is a volume form.
	This map is the determinant of $A$, but thought of as acting on the column vectors of $A$.
\end{lemma}
\begin{proof}
	We first show that this map is multilinear.
	Fix $\sigma \in S_n$, and consider $\prod_{i=1}^n a_{\sigma(i) i}$.
	This product contains exactly one term in each column of $A$.
	Thus, the map $(A^{(1)}, \dots, A^{(n)}) \mapsto \prod_{i=1}^n a_{\sigma(i) i}$ is multilinear.
	This then clearly implies that the determinant, a sum of such multilinear maps, is itself multilinear.

	Now, we show that the determinant is alternating.
	Let $k \neq \ell$, and $A^{(k)} = A^{(\ell)}$.
	I want to show $\det A = 0$. \\
	Let $\tau = ( k \; \ell )$ be the transposition exchanging $k$ and $\ell$.
	Then, for all $i, j \in \qty{1, \dots, n}$, $a_{ij} = a_{i \tau(j)}$.
	We can decompose permutations into two disjoint sets: $S_n = A_n \cup \tau A_n$\footnote{As $\tau$ bijective and $\varepsilon(\tau) = -1$}, where $A_n$ is the alternating group of order $n$.
	\begin{align*}
		\det A &= \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_{i = 1}^n a_{i \sigma(i)} \\
		&= \sum_{\sigma\in A_n} \varepsilon(\sigma) \prod_{i = 1}^n a_{i \sigma(i)} + \sum_{\sigma \in \tau A_n} \varepsilon(\sigma) \prod_{i = 1}^n a_{i \sigma(i)} \\
		&= \sum_{\sigma\in A_n} \prod_{i = 1}^n a_{i \sigma(i)} - \sum_{\sigma\in A_n} \prod_{i = 1}^n a_{i \tau\sigma(i)} \\
		&= \sum_{\sigma\in A_n} \prod_{i = 1}^n a_{i \sigma(i)} - \sum_{\sigma\in A_n} \prod_{i = 1}^n a_{i \mathcolor{red}{\sigma(i)}} \quad \text{as $a_{ij} = a_{i \tau(j)}$} \\
		&= 0
	\end{align*} 
	So the determinant is alternating, and hence a volume form.
\end{proof}

\begin{lemma}
	Let $d$ be a volume form.
	Then, swapping two entries changes the sign.
\end{lemma}
\begin{proof}
	Take the sum of these two results:
	\begin{align*}
		d(v_1, \dots, v_i, \dots, v_j, \dots, v_n) & + d(v_1, \dots, v_j, \dots, v_i, \dots, v_n) \\
		& = d(v_1, \dots, v_i, \dots, v_j, \dots, v_n) \\
		& + d(v_1, \dots, v_j, \dots, v_i, \dots, v_n) \\
		& + \underbracket{d(v_1, \dots, v_i, \dots, v_i, \dots, v_n)}_0 \\
		& + \underbracket{d(v_1, \dots, v_j, \dots, v_j, v_n)}_0 \\
		& = 2 d(v_1, \dots, v_i + v_j, \dots, v_i + v_j, \dots, v_n) \\
		& = 0
	\end{align*}
	as required.
\end{proof}
\begin{corollary}
	If $\sigma \in S_n$ and $d$ is a volume form, $d(v_{\sigma(1)}, \dots, v_{\sigma(n)}) = \varepsilon(\sigma) d(v_1, \dots, v_n)$.
\end{corollary}
\begin{proof}
	We can decompose $\sigma$ as a product of transpositions $\prod_{i=1}^{n_\sigma} e_i$.
\end{proof}
\begin{theorem}
	Let $d$ be a volume form on $F^n$.
	Let $A$ be a matrix whose columns are $A^{(i)}$.
	Then
	\begin{align*}
		d(A^{(1)}, \dots, A^{(n)}) = \det A \cdot d(e_1, \dots, e_n)
	\end{align*}
	So there is a unique volume form up to a constant multiple.
\end{theorem}
\begin{proof}
	\begin{align*}
		d(A^{(1)}, \dots, A^{(n)}) = d\qty(\sum_{i=1}^n a_{i1} e_i, A^{(2)}, \dots, A^{(n)})
	\end{align*}
	Since $d$ is multilinear,
	\begin{align*}
		d(A^{(1)}, \dots, A^{(n)}) = \sum_{i=1}^n a_{i1} d\qty(e_i, A^{(2)}, \dots, A^{(n)})
	\end{align*}
	Inductively on all columns,
	\begin{align*}
		d(A^{(1)}, \dots, A^{(n)}) &= \sum_{i=1}^n \sum_{j=1}^n a_{i1} a_{j2} d\qty(e_i, e_j, A^{(3)}, \dots, A^{(n)})\\
		&\vdots \\
		&= \sum_{\substack{1 \leq i_1 \leq n \\ \vdots \\ 1 \leq i_n \leq n}} \prod_{k=1}^n a_{i_k k} d(e_{i_1}, \dots e_{i_n})
	\end{align*}
	Since $d$ is alternating, we know that for $d(e_{i_1}, \dots, e_{i_n})$ to be nonzero, the $i_k$ must be different, so this corresponds to a permutation $\sigma \in S_n$.
	\begin{align*}
		d(A^{(1)}, \dots, A^{(n)}) = \sum_{\sigma \in S_n} \prod_{k=1}^n a_{\sigma(k) k} \varepsilon(\sigma) d(e_1, \dots, e_n)
	\end{align*}
	which is exactly the determinant up to a constant multiple.
\end{proof}

\begin{corollary}
	We can then see that $\det A$ is the only volume form such that $d(e_1, \dots, e_n) = 1$.
\end{corollary} 

\subsection{Multiplicative property of determinant}
\begin{lemma}
	Let $A, B \in M_n(F)$.
	Then $\det(AB) = \det(A) \det(B)$.
\end{lemma}
\begin{proof}
	Given $A$, we define the volume form $d_A \colon (F^n)^n \to F$ by
	\begin{align*}
		d_A(v_1, \dots, v_n) \mapsto \det(A v_1, \dots, A v_n)
	\end{align*}
	$v_i \mapsto A v_i$ is linear, and the determinant is multilinear, so $d_A$ is multilinear.
	If $i \neq j$ and $v_i = v_j$, then $\det(\dots, A v_i, \dots, A v_j, \dots) = 0$ so $d_A$ is alternating.
	Hence $d_A$ is a volume form.

	Hence there exists a constant $C_A$ such that $d_A(v_1, \dots, v_n) = C_A \det(v_1, \dots, v_n)$.
	We can compute $C_A$ by considering the basis vectors; $A e_i = A_i$ where $A_i$ is the $i$th column vector of $A$.
	Then,
	\begin{align*}
		C_A = d_A(e_1, \dots, e_n) = \det(Ae_1, \dots, Ae_n) = \det A
	\end{align*}
	Hence,
	\begin{align*}
		\det(AB) = d_A(B_1, \dots, B_n) = \det A \det B
	\end{align*}
\end{proof}

\subsection{Singular and non-singular matrices}
\begin{definition}[Singular]
	Let $A \in M_n(F)$.
	We say that
	\begin{enumerate}
		\item $A$ is \vocab{singular} if $\det A = 0$;
		\item $A$ is \vocab{non-singular} if $\det A \neq 0$.
	\end{enumerate}
\end{definition}

\begin{lemma}
	If $A$ is invertible, it is non-singular.
\end{lemma}

\begin{proof}
	If $A$ is invertible, there exists $A^{-1}$.
	\begin{align*}
		\det(A A^{-1}) = \det I = 1
	\end{align*}
	Thus $\det A \det A^{-1} = 1$ and hence neither of these determinants can be zero.
\end{proof}

\begin{remark}
	We have proved that $\det A\inv = \frac{1}{\det A}$
\end{remark} 

\begin{theorem}
	Let $A \in M_n(F)$.
	The following are equivalent.
	\begin{enumerate}
		\item $A$ is invertible;
		\item $A$ is non-singular;
		\item $r(A) = n$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	We have just shown that (i) implies (ii).
	We have also shown that (i) and (iii) are equivalent by the rank-nullity theorem.
	So it suffices to show that (ii) implies (iii).

	Suppose $r(A) < n$.
	Then we will show $A$ is singular.
	We have $\dim \vecspan(A_1, \dots, A_n) < n$.
	Therefore, since there are $n$ vectors, $(A_1, \dots, A_n)$ is not free.
	So there exist scalars $\lambda_i$ not all zero such that $\sum_i \lambda_i A_i = 0$.
	Choose $j$ such that $\lambda_j \neq 0$.
	Then,
	\begin{align*}
		A_j = -\frac{1}{\lambda_j} \sum_{i \neq j} \lambda_i A_i
	\end{align*}
	So we can compute the determinant of $A$ by
	\begin{align*}
		\det A = \det(A_1, \dots, -\frac{1}{\lambda_j} \sum_{i \neq j} \lambda_i A_i, \dots, A_n)
	\end{align*}
	Since the determinant is alternating and linear in the $j$th entry, its value is zero.
	So $A$ is singular as required.
\end{proof}
\begin{remark}
	The above theorem gives necessary and sufficient conditions for invertibility of a set of $n$ linear equations with $n$ unknowns.
	There exists a unique solution $X \in F^n$ to $AX = Y$ iff $A$ is invertible.
\end{remark}

\subsection{Determinants of linear maps}
\begin{lemma}
	Similar matrices have the same determinant.
\end{lemma}
\begin{proof}
	\begin{align*}
		\det (P^{-1} A P) = \det(P^{-1}) \det A \det P = \det A \det (P^{-1} P) = \det A\footnote{$P$ invertible.}
	\end{align*}
\end{proof}
\begin{definition}
	If $\alpha$ is an endomorphism, then we define
	\begin{align*}
		\det \alpha = \det [\alpha]_{B, B}
	\end{align*}
	where $B$ is any basis of the vector space.
	This is well-defined, since this value does not depend on the choice of basis.
\end{definition}
\begin{theorem}
	$\det \colon L(V,V) \to F$ satisfies the following properties.
	\begin{enumerate}
		\item $\det I = 1$;
		\item $\det (\alpha\beta) = \det\alpha \det\beta$;
		\item $\det \alpha \neq 0$ if and only if $\alpha$ is invertible, and in this case, $\det(\alpha^{-1}) \det \alpha = 1$.
	\end{enumerate}
	This is simply a reformulation of the previous theorem for matrices.
\end{theorem}

\begin{proof}
	The proof is simple, and relies on the invariance of the determinant under a change of basis.
	Simply pick a basis, and re-express in terms of $[\alpha]_B, [\beta]_B$.
\end{proof} 

\subsection{Determinant of block-triangular matrices}
\begin{lemma}
	Let $A \in M_k(F)$, $B \in M_\ell(F)$, $C \in M_{k, \ell}(F)$.
	Consider the matrix
	\begin{align*}
		M = \begin{pmatrix}
			A & C \\
			0 & B
		\end{pmatrix}
	\end{align*}
	Then $\det M = \det A \det B$.
\end{lemma}
\begin{proof}
	Let $n = k + \ell$, so $M \in M_n(F)$.
	Let $M = (m_{ij})$.
	We must compute
	\begin{align*}
		\det M = \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_{i=1}^n m_{\sigma(i) i}
	\end{align*}
	Observe that $m_{\sigma(i) i} = 0$ if $i \leq k$ and $\sigma(i) > k$.
	Then, we need only sum over $\sigma \in S_n$ such that for all $j \leq k$, we have $\sigma(j) \leq k$.
	Thus, for all $j \in \qty{k+1, \dots, n}$, we have $\sigma(j) \in \qty{k+1, \dots, n}$.
	We can then uniquely decompose $\sigma$ into two permutations $\sigma = \sigma_1 \sigma_2$, where $\sigma_1$ is restricted to $\qty{1, \dots, k}$ and $\sigma_2$ is restricted to $\qty{k+1, \dots, n}$.
	Hence,
	\begin{align*}
		\det M & = \sum_{\sigma_1 \in S_k} \sum_{\sigma_2 \in S_{n-k}} \varepsilon(\sigma) \prod_{i=1}^n m_{\sigma(i) i} \\
		& = \sum_{\sigma_1 \in S_k} \sum_{\sigma_2 \in S_{n-k}} \varepsilon(\sigma_1) \varepsilon(\sigma_2) \prod_{i=1}^k m_{\sigma_1(i) i} \prod_{i=k+1}^n m_{\sigma_2(i) i} \\
		&= \sum_{\sigma_1 \in S_k} \sum_{\sigma_2 \in S_{n-k}} \varepsilon(\sigma_1) \varepsilon(\sigma_2) \prod_{i=1}^k A_{\sigma_1(i) i}\footnote{$i, \sigma_1(i) \in [1, k]$ so $m_{\sigma_1(i) i} = A_{\sigma_1(i) i}$.} \prod_{i=k+1}^n B_{\sigma_2(i) i} \\
		& = \qty(\sum_{\sigma_1 \in S_k} \varepsilon(\sigma_1) \prod_{i=1}^k A_{\sigma(i) i}) \qty(\sum_{\sigma_2 \in S_{n-k}} \varepsilon(\sigma_2) \prod_{i=k+1}^n B_{\sigma(i) i}) \\
		& = \det A \det B
	\end{align*}
\end{proof}
\begin{corollary}
	We need not restrict ourselves to just two blocks, since we can apply the above lemma inductively.
	In particular, this implies that an upper-triangular matrix with diagonal elements $\lambda_i$ has determinant $\prod_i \lambda_i$.
\end{corollary}