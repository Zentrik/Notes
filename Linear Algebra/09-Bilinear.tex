\section{Properties of bilinear forms}

\subsection{Changing basis}
Let $\phi \colon V \times V \to \mathbb F$ be a bilinear form.
Let $V$ be a finite-dimensional $F$-vector space.
Let $B$ be a basis of $V$ and let $[\phi]_B = [\phi]_{BB}$ be the matrix with entries $\phi(e_i, e_j)$.
\begin{lemma}
	Let $\phi$ be a bilinear form $V \times V \to F$.
	Then if $B, B'$ are bases for $V$, and $P = [I]_{B', B}$ we have
	\begin{align*}
		[\phi]_{B'} = P^\transpose [\phi]_B P
	\end{align*}
\end{lemma}
\begin{proof}
	This is a special case of the general change of basis formula.
\end{proof}
\begin{definition}
	Let $A, B \in M_n(F)$ be square matrices.
	We say that $A, B$ are \textit{congruent} if there exists $P \in M_n(F)$ such that $A = P^\transpose B P$.
\end{definition}
\begin{remark}
	Congruence is an equivalence relation.
\end{remark}
\begin{definition}
	A bilinear form $\phi$ on $V$ is \textit{symmetric} if, for all $u, v \in V$, we have
	\begin{align*}
		\phi(u,v) = \phi(v,u)
	\end{align*}
\end{definition}
\begin{remark}
	If $A$ is a square matrix, we say $A$ is symmetric if $A = A^\transpose$.
	Equivalently, $A_{ij} = A_{ji}$ for all $i, j$.
	So $\phi$ is symmetric if and only if $[\phi]_B$ is symmetric for any basis $B$.
	Note further that to represent $\phi$ by a diagonal matrix in some basis $B$, it must necessarily be symmetric, since
	\begin{align*}
		P^\transpose A P = D \implies D = D^\transpose = \qty(P^\transpose A P)^\transpose = P^\transpose A^\transpose P \implies A = A^\transpose
	\end{align*}
\end{remark}

\subsection{Quadratic forms}
\begin{definition}
	A map $Q \colon V \to F$ is a \textit{quadratic form} if there exists a bilinear form $\phi \colon V \times V \to F$ such that, for all $u \in V$,
	\begin{align*}
		Q(u) = \phi(u,u)
	\end{align*}
	So a quadratic form is the restriction of a bilinear form to the diagonal.
\end{definition}
\begin{remark}
	Let $B = (e_i)$ be a basis of $V$.
	Let $A = [\phi]_B = (\phi(e_i, e_j)) = (a_{ij})$.
	Then, for $u = \sum_i x_i e_i \in V$,
	\begin{align*}
		Q(u) = \phi(u,u) = \phi\qty(\sum_i x_i e_i, \sum_j x_j e_j) = \sum_i \sum_j x_i x_j \phi(e_i, e_j) = \sum_i \sum_j x_i x_j a_{ij}
	\end{align*}
	We can check that this is equal to
	\begin{align*}
		Q(u) = x^\transpose A x
	\end{align*}
	where $[u]_B = x$.
	Note further that
	\begin{align*}
		x^\transpose A x = \sum_i \sum_j a_{ij} x_i x_j = \sum_i \sum_j a_{ji} x_i x_j = \sum_i \sum_j \frac{a_{ij} + a_{ji}}{2} x_i x_j = x^\transpose \qty(\underbrace{\frac{A + A^\transpose}{2}}_{\mathclap{\text{symmetric}}}) x
	\end{align*}
	So we can always express the quadratic form as a symmetric matrix in any basis.
\end{remark}
\begin{proposition}
	If $Q \colon V \to F$ is a quadratic form, then there exists a unique symmetric bilinear form $\phi \colon V \times V \to F$ such that $Q(u) = \phi(u,u)$.
\end{proposition}
\begin{proof}
	Let $\psi$ be a bilinear form on $V$ such that for all $u \in V$, we have $Q(u) = \psi(u,u)$.
	Then, let
	\begin{align*}
		\phi(u,v) = \frac{1}{2}\qty[\psi(u,v) + \psi(v,u)]
	\end{align*}
	Certainly $\phi$ is a bilinear form and symmetric.
	Further, $\phi(u,u) = \psi(u,u) = Q(u)$.
	So there exists a symmetric bilinear form $\phi$ such that $Q(u) = \phi(u,u)$, so it suffices to prove uniqueness.
	Let $\phi$ be a symmetric bilinear form such that for all $u \in V$ we have $Q(u) = \phi(u,u)$.
	Then, we can find
	\begin{align*}
		Q(u + v) = \phi(u + v, u + v) = \phi(u,u) + \phi(v,v) + 2\phi(u,v)
	\end{align*}
	Thus $\phi(u,v)$ is defined uniquely by $Q$, since
	\begin{align*}
		2 \phi(u,v) = Q(u+v) - Q(u) - Q(v)
	\end{align*}
	So $\phi$ is unique (when $2$ is invertible in $F$).
	This identity for $\phi(u,v)$ is known as the polarisation identity.
\end{proof}

\subsection{Diagonalisation of symmetric bilinear forms}
\begin{theorem}
	Let $\phi \colon V \times V \to F$ be a symmetric bilinear form, where $V$ is finite-dimensional.
	Then there exists a basis $B$ of $V$ such that $[\phi]_B$ is diagonal.
\end{theorem}
\begin{proof}
	By induction on the dimension, suppose the theorem holds for all dimensions less than $n$ for $n \geq 2$.
	If $\phi(u,u) = 0$ for all $u \in V$, then $\phi = 0$ by the polarisation identity, which is diagonal.
	Otherwise $\phi(e_1, e_1) \neq 0$ for some $e_1 \in V$.
	Let
	\begin{align*}
		U = \qty(\genset{e_1})^\perp = \qty{v \in V \colon \phi(e_1, v) = 0}
	\end{align*}
	This is a vector subspace of $V$, which is in particular
	\begin{align*}
		\ker \qty{ \phi(e_1, {}\cdot{}) \colon V \to F }
	\end{align*}
	By the rank-nullity theorem, $\dim U = n - 1$.
	We now claim that $U + \genset{e_1}$ is a direct sum.
	Indeed, for $v = \genset{e_1} \cap U$, we have $v = \lambda e_1$ and $\phi(e_1, v) = 0$.
	Hence $\lambda = 0$, since by assumption $\phi(e_1, e_1) \neq 0$.
	So we find a basis $B' = (e_2, \dots, e_n)$ of $U$, which we extend by $e_1$ to $B = (e_1, e_2, \dots, e_n)$.
	Since $U \oplus \genset{e_1}$ has dimension $n$, this is a basis of $V$.
	Under this basis, we find
	\begin{align*}
		[\phi]_B = \begin{pmatrix}
			\phi(e_1, e_1) & 0                          \\
			0              & \qty[\eval{\phi}_{U}]_{B'}
		\end{pmatrix}
	\end{align*}
	because
	\begin{align*}
		\phi(e_1, e_j) = \phi(e_j, e_1) = 0
	\end{align*}
	for all $j \geq 2$.
	By the inductive hypothesis we can take a basis $B'$ such that the restricted $\phi$ to be diagonal, so $[\phi]_B$ is diagonal in this basis.
\end{proof}
\begin{example}
	Let $V = \mathbb R^3$ and choose the canonical basis $(e_i)$.
	Let
	\begin{align*}
		Q(x_1, x_2, x_3) = x_1^2 + x_2^2 + 2x_3^2 + 2x_1 x_2 + 2x_1 x_3 - 2x_2 x_3
	\end{align*}
	Then, if $Q(x_1, x_2, x_3) = x^\transpose A x$, we have
	\begin{align*}
		A = \begin{pmatrix}
			1 & 1  & 1  \\
			1 & 1  & -1 \\
			1 & -1 & 2
		\end{pmatrix}
	\end{align*}
	Note that the off-diagonal terms are halved from their coefficients since in the expansion of $x^\transpose A x$ they are included twice.
	Then, we can find a basis in which $A$ is diagonal.
	We could use the above algorithm to find a basis, or complete the square in each component.
	We can write
	\begin{align*}
		Q(x_1, x_2, x_3) = (x_1 + x_2 + x_3)^2 + x_3^2 - 4 x_2 x_3 = (x_1 + x_2 + x_3)^2 + (x_3 - 2x_2)^2 - (2x_2)^2
	\end{align*}
	This yields a new coordinate basis $x_1', x_2', x_3'$.
	Then $P^{-1} A P$ is diagonal.
	$P$ is given by
	\begin{align*}
		\begin{pmatrix} x_1' \\ x_2' \\ x_3' \end{pmatrix} = \underbrace{\begin{pmatrix} 1 & 1 & 1 \\ 0 & -2 & 1 \\ 0 & -2 & 0 \end{pmatrix}}_{P^{-1}} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix}
	\end{align*}
\end{example}

\subsection{Sylvester's law}
\begin{corollary}
	If $F = \mathbb C$, for any symmetric bilinear form $\phi$ there exists a basis of $V$ such that $[\phi]_B$ is
	\begin{align*}
		\begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}
	\end{align*}
\end{corollary}
\begin{proof}
	Since any symmetric bilinear form $\phi$ in a finite-dimensional $F$-vector space $V$ can be diagonalised, let $E = (e_1, \dots, e_n)$ such that $[\phi]_E$ is diagonal with diagonal entries $a_i$.
	Order the $a_i$ such that $a_i$ is nonzero for $1 \leq i \leq r$, and the remaining values (if any) are zero.
	For $i \leq r$, let $\sqrt{a_i}$ be a choice of a complex root for $a_i$.
	Then $v_i = \frac{e_i}{\sqrt{a_i}}$ for $i \leq r$ and $v_i = e_i$ for $i > r$ gives the basis $B$ as required.
\end{proof}
\begin{corollary}
	Every symmetric matrix of $M_n(\mathbb C)$ is congruent to a unique matrix of the form
	\begin{align*}
		\begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}
	\end{align*}
	where $r$ is the rank of the matrix.
\end{corollary}
\begin{corollary}
	Let $F = \mathbb R$, and let $V$ be a finite-dimensional $\mathbb R$-vector space.
	Let $\phi$ be a symmetric bilinear form on $V$.
	Then there exists a basis $B = (v_1, \dots, v_n)$ of $V$ such that
	\begin{align*}
		[\phi]_B = \begin{pmatrix}
			I_p & 0    & 0 \\
			0   & -I_q & 0 \\
			0   & 0    & 0
		\end{pmatrix}
	\end{align*}
	for some integers $p, q$.
\end{corollary}
\begin{proof}
	Since square roots do not necessarily exist in $\mathbb R$, we cannot use the form above.
	We first diagonalise the bilinear form in some basis $E$.
	Then, reorder and group the $a_i$ into a positive group of size $p$, a negative group of size $q$, and a zero group.
	Then,
	\begin{align*}
		v_i = \begin{cases}
			\frac{e_i}{\sqrt{a_i}}  & i \in \qty{1, \dots, p}     \\
			\frac{e_i}{\sqrt{-a_i}} & i \in \qty{p+1, \dots, p+q} \\
			e_i                     & i \in \qty{p+q+1, \dots, n}
		\end{cases}
	\end{align*}
	This gives a new basis as required.
\end{proof}
\begin{definition}
	Let $F = \mathbb R$.
	The \textit{signature} of a bilinear form $\phi$ is
	\begin{align*}
		s(\phi) = p - q
	\end{align*}
	where $p$ and $q$ are defined as in the corollary above.
\end{definition}
\begin{theorem}
	Let $F = \mathbb R$.
	Let $V$ be a finite-dimensional $\mathbb R$-vector space.
	If a real symmetric bilinear form is represented by some matrix
	\begin{align*}
		\begin{pmatrix}
			I_p & 0    & 0 \\
			0   & -I_q & 0 \\
			0   & 0    & 0
		\end{pmatrix}
	\end{align*}
	in some basis $B$, and some other matrix
	\begin{align*}
		\begin{pmatrix}
			I_{p'} & 0       & 0 \\
			0      & -I_{q'} & 0 \\
			0      & 0       & 0
		\end{pmatrix}
	\end{align*}
	in another basis $B'$, then $p = p'$ and $q = q'$.
	Thus, the signature of the matrix is well defined.
\end{theorem}
\begin{definition}
	Let $\phi$ be a symmetric bilinear form on a real vector space $V$.
	We say that
	\begin{enumerate}
		\item $\phi$ is \textit{positive definite} if $\phi(u,u) > 0$ for all nonzero $u \in V$;
		\item $\phi$ is \textit{positive semidefinite} if $\phi(u,u) \geq 0$ for all $u \in V$;
		\item $\phi$ is \textit{negative definite} or \textit{negative semidefinite} if $\phi(u,u) < 0$ or $\phi(u,u) \leq 0$ respectively for all nonzero $u \in V$.
	\end{enumerate}
\end{definition}
\begin{example}
	The matrix
	\begin{align*}
		\begin{pmatrix}
			I_r & 0 \\
			0   & 0
		\end{pmatrix}
	\end{align*}
	is positive definite for $r = n$, and positive semidefinite for $r < n$.
\end{example}
\noindent We now prove Sylvester's law.
\begin{proof}
	In order to prove uniqueness of $p$, we will characterise the matrix in a way that does not depend on the basis.
	In particular, we will show that $p$ is the largest dimension of a vector subspace of $V$ such that the restriction of $\phi$ on this subspace is positive definite.
	Suppose we have $B = (v_1, \dots, v_n)$ and
	\begin{align*}
		[\phi]_B = \begin{pmatrix}
			I_p & 0    & 0 \\
			0   & -I_q & 0 \\
			0   & 0    & 0
		\end{pmatrix}
	\end{align*}
	We consider
	\begin{align*}
		X = \genset{v_1, \dots, v_p}
	\end{align*}
	Then we can easily compute that $\eval{\phi}_X$ is positive definite.
	Let
	\begin{align*}
		Y = \genset{v_{p+1}, \dots, v_n}
	\end{align*}
	Then, as above, $\eval{\phi}_Y$ is negative semidefinite.
	Suppose that $\phi$ is positive definite on another subspace $X'$.
	In this case, $Y \cap X' = \qty{0}$, since if $y \in Y \cap X'$ we must have $Q(y) \leq 0$, but since $y \in X'$ we have $y = 0$.
	Thus, $Y + X' = Y \oplus X'$, so $n = \dim V \geq \dim Y + \dim X'$.
	But $\dim Y = n - p$, so $\dim X' \leq p$.
	The same argument can be executed for $q$, hence both $p$ and $q$ are independent of basis.
\end{proof}

\subsection{Kernels of bilinear forms}
\begin{definition}
	Let $K = \qty{ v \in V \colon \forall u \in V, \phi(u,v) = 0 }$.
	This is the \textit{kernel} of the bilinear form.
\end{definition}
\begin{remark}
	By the rank-nullity theorem,
	\begin{align*}
		\dim K + \rank \phi = n
	\end{align*}
	Using the above notation, we can show that there exists a subspace $T$ of dimension $n - (p+q) + \min\qty{p,q}$ such that $\eval{\phi}_T = 0$.
	Indeed, let $B = (v_1, \dots, v_n)$ such that
	\begin{align*}
		[\phi]_B = \begin{pmatrix}
			I_p & 0    & 0 \\
			0   & -I_q & 0 \\
			0   & 0    & 0
		\end{pmatrix}
	\end{align*}
	The quadratic form has a zero subspace of dimension $n - (p+q)$ in the bottom right.
	But by setting
	\begin{align*}
		T = \qty{v_1 + v_{p+1}, \dots, v_q + v_{p+q}, v_{p+q+1}, \dots, v_n}
	\end{align*}
	we can combine the positive and negative blocks (assuming here that $p \geq q$) to produce more linearly independent elements of the kernel.
	In particular, $\dim T$ is the largest possible dimension of a subspace $T'$ of $V$ such that $\eval{\phi}_{T'} = 0$.
\end{remark}

\subsection{Sesquilinear forms}
Let $F = \mathbb C$.
The standard inner product on $\mathbb C^n$ is defined to be
\begin{align*}
	\inner{\begin{pmatrix} x_1 \\ \vdots \\ v_n \end{pmatrix}, \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}} = \sum_{i=1}^n x_i \overline y_i
\end{align*}
This is not a bilinear form on $\mathbb C$ due to the complex conjugate, it is linear in the first entry.
\begin{definition}
	Let $V, W$ be $\mathbb C$-vector spaces.
	A form $\phi \colon V \times W \to \mathbb C$ is called \textit{sesquilinear} if it is linear in the first entry, and
	\begin{align*}
		\phi(v, \lambda_1 w_1 + \lambda_2 w_2) = \overline \lambda_1 \phi(v,w_1) + \overline \lambda_2 \phi(v,w_2)
	\end{align*}
	so it is antilinear with respect to the second entry.
\end{definition}
\begin{lemma}
	Let $B = (v_1, \dots, v_m)$ be a basis of $V$ and $C = (w_1, \dots, w_n)$ be a basis of $W$.
	Let $[\phi]_{B,C} = \qty(\phi(v_i, w_j))$.
	Then,
	\begin{align*}
		\phi(v,w) = [v]_B^\transpose [\phi]_{B,C} \overline{[w]_C}
	\end{align*}
\end{lemma}
\begin{proof}
	Let $B, B'$ be bases of $V$ and $C, C'$ be bases of $W$.
	Let $P = [I]_{B', B}$ and $Q = [I]_{C', C}$.
	Then
	\begin{align*}
		[\phi]_{B', C'} = P^\transpose [\phi]_{B,C} \overline Q
	\end{align*}
\end{proof}

\subsection{Hermitian forms}
\begin{definition}
	Let $V$ be a finite-dimensional $\mathbb C$-vector space.
	Let $\phi$ be a sesquilinear form on $V$.
	Then $\phi$ is \textit{Hermitian} if, for all $u, v \in V$,
	\begin{align*}
		\phi(u, v) = \overline{\phi(v,u)}
	\end{align*}
\end{definition}
\begin{remark}
	If $\phi$ is Hermitian, then $\phi(u,u) = \overline{\phi(u,u)} \in \mathbb R$.
	Further, $\phi(\lambda u, \lambda u) = \abs{\lambda}^2 \phi(u,u)$.
	This allows us to define positive and negative definite Hermitian forms.
\end{remark}
\begin{lemma}
	A sesquilinear form $\phi \colon V \times V \to \mathbb C$ is Hermitian if and only if, for any basis $B$ of $V$,
	\begin{align*}
		[\phi]_B = [\phi]_B^\dagger
	\end{align*}
\end{lemma}
\begin{proof}
	Let $A = [\phi]_B = (a_{ij})$.
	Then $a_{ij} = \phi(e_i, e_j)$, and $a_{ji} = \phi(e_j, e_i) = \overline{\phi(e_i, e_j)} = \overline{a_{ij}}$.
	So $\overline A^\transpose = A$.
	Conversely suppose that $[\phi]_B = A = \overline A^\transpose$.
	Now let
	\begin{align*}
		u = \sum_{i=1}^n \lambda_i e_i;\quad v = \sum_{i=1}^n \mu_i e_i
	\end{align*}
	Then,
	\begin{align*}
		\phi(u,v) = \phi\qty(\sum_{i=1}^n \lambda_i e_i, \sum_{i=1}^n \mu_i e_i) = \sum_{i=1}^n \sum_{j=1}^n \lambda_i \overline{\mu_j} a_{ij}
	\end{align*}
	Further,
	\begin{align*}
		\overline{\phi(v,u)} = \overline{\phi\qty(\sum_{i=1}^n \mu_i e_i, \sum_{i=1}^n \lambda_i e_i)} = \sum_{i=1}^n \sum_{j=1}^n \overline{\mu_j \overline{\lambda_i}} \overline{a_{ij}}
	\end{align*}
	which is equivalent.
	Hence $\phi$ is Hermitian.
\end{proof}

\subsection{Polarisation identity}
A Hermitian form $\phi$ on a complex vector space $V$ is entirely determined by a quadratic form $Q \colon V \to \mathbb R$ such that $v \mapsto \phi(v,v)$ by the formula
\begin{align*}
	\phi(u,v) = \frac{1}{4} \qty[ Q(u+v) - Q(u-v) + iQ(u+iv) - iQ(u-iv) ]
\end{align*}

\subsection{Hermitian formulation of Sylvester's law}
\begin{theorem}
	Let $V$ be a finite-dimensional $\mathbb C$-vector space.
	Let $\phi \colon V \times V \to \mathbb C$ be a Hermitian form on $V$.
	Then there exists a basis $B = (v_1, \dots, v_n)$ of $V$ such that
	\begin{align*}
		[\phi]_B = \begin{pmatrix}
			I_p & 0    & 0 \\
			0   & -I_q & 0 \\
			0   & 0    & 0
		\end{pmatrix}
	\end{align*}
	where $p, q$ depend only on $\phi$ and not $B$.
\end{theorem}
\begin{proof}
	The following is a sketch proof; it is nearly identical to the case of real symmetric bilinear forms.
	If $\phi = 0$, existence is trivial.
	Otherwise, using the polarisation identity there exists $e_1 \neq 0$ such that $\phi(e_1, e_1) \neq 0$.
	Let
	\begin{align*}
		v_1 = \frac{e_1}{\sqrt{\abs{\phi(e_1, e_1)}}} \implies \phi(v_1, v_1) = \pm 1
	\end{align*}
	Consider the orthogonal space $W = \qty{w \in V \colon \phi(v_1, w) = 0}$.
	We can check, arguing analogously to the real case, that $V = \genset{v_1} \oplus W$.
	Hence, we can inductively diagonalise $\phi$.

	$p, q$ are unique.
	Indeed, we can prove that $p$ is the maximal dimension of a subspace on which $\phi$ is positive definite (which is well-defined since $\phi(u,u) \in \mathbb R$).
	The geometric interpretation of $q$ is similar.
\end{proof}

\subsection{Skew-symmetric forms}
\begin{definition}
	Let $V$ be a finite-dimensional $\mathbb R$-vector space.
	Let $\phi$ be a bilinear form on $V$.
	Then $\phi$ is \textit{skew-symmetric} if, for all $u,v \in V$,
	\begin{align*}
		\phi(u,v) = -\phi(v,u)
	\end{align*}
\end{definition}
\begin{remark}
	$\phi(u,u) = -\phi(u,u) = 0$.
	Also, in any basis $B$ of $V$, we have $[\phi]_B = -[\phi]_B^\transpose$.
	Any real matrix can be decomposed as the sum
	\begin{align*}
		A = \frac{1}{2}\qty(A + A^\transpose) + \frac{1}{2}\qty(A - A^\transpose)
	\end{align*}
	where the first summand is symmetric and the second is skew-symmetric.
\end{remark}

\subsection{Skew-symmetric formulation of Sylvester's law}
\begin{theorem}
	Let $V$ be a finite-dimensional $\mathbb R$-vector space.
	Let $\phi \colon V \times V \to \mathbb R$ be a skew-symmetric form on $V$.
	Then there exists a basis
	\begin{align*}
		B = (v_1, w_1, v_2, w_2, \dots, v_m, w_m, v_{2m+1}, v_{2m+2}, \dots, v_n)
	\end{align*}
	of $V$ such that
	\begin{align*}
		[\phi]_B = \begin{pmatrix}
			0  & 1                       \\
			-1 & 0                       \\
			   &   & 0  & 1              \\
			   &   & -1 & 0              \\
			   &   &    &   & \ddots     \\
			   &   &    &   &        & 0
		\end{pmatrix}
	\end{align*}
\end{theorem}
\begin{corollary}
	Skew-symmetric matrices have an even rank.
\end{corollary}
\begin{proof}
	This is again very similar to the previous case.
	We will perform an inductive step on the dimension of $V$.
	If $\phi \neq 0$, there exist $v_1, w_1$ such that $\phi_1(v_1, w_1) \neq 0$.
	After scaling one of the vectors, we can assume $\phi(v_1, w_1) = 1$.
	Since $\phi$ is skew-symmetric, $\phi(w_1, v_1) = -1$.
	Then $v_1, w_1$ are linearly independent; if they were linearly dependent we would have $\phi(v_1, w_1) = \phi(v_1, \lambda v_1) = 0$.
	Let $U = \genset{v_1, w_1}$ and let $W = \qty{v \in V \colon \phi(v_1, v) = \phi(w_1, v) = 0}$ and we can show $V = U \oplus W$.
	Then induction gives the required result.
\end{proof}